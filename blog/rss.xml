<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Isaac Chung Blog</title>
        <link>https://isaac-chung.github.io/blog</link>
        <description>Isaac Chung Blog</description>
        <lastBuildDate>Sun, 14 Jan 2024 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <copyright>Copyright © 2024 Isaac Chung</copyright>
        <item>
            <title><![CDATA[When Quantized Models Still Don't Fit]]></title>
            <link>https://isaac-chung.github.io/blog/quantized-models-dont-fit</link>
            <guid>https://isaac-chung.github.io/blog/quantized-models-dont-fit</guid>
            <pubDate>Sun, 14 Jan 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That's what happened to me when I was trying to run Mixtral-8x7B with Ollama (check out this previous blog post on what Ollama is). The model requires 26GB of RAM while my laptop only has 16GB. I'll try to walk through the workaround a bit at a time (pun intended).]]></description>
            <content:encoded><![CDATA[<p>A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That's what happened to me when I was trying to run <a href="https://ollama.ai/library/mixtral" target="_blank" rel="noopener noreferrer">Mixtral-8x7B with Ollama</a> (check out this <a href="https://isaac-chung.github.io/blog/what-is-ollama">previous blog post on what Ollama is</a>). The model requires 26GB of RAM while my laptop only has 16GB. I'll try to walk through the workaround a <em>bit</em> at a time (pun intended).</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I'll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>What is quantization?</li>
<li>What is offloading?</li>
<li>How to run Mixtral-8x7B for free?</li>
</ul></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-quantization">What is quantization?<a href="https://isaac-chung.github.io/blog/quantized-models-dont-fit#what-is-quantization" class="hash-link" aria-label="Direct link to What is quantization?" title="Direct link to What is quantization?">​</a></h2>
<p>Quantization generally is a process that converts continuous values to a discrete set of values. A tangible analogy would be how we tell time: time is continuous, and we use hours, minutes, and seconds to "quantize" time. Sometimes "around 10am" is good enough, and sometimes we want to be precise to the millisecond.
In the context of deep learning, it is a technique to reduce the computational and memory costs of running a model by using lower-precision numerical types to repesent its weights and activations. In simpler terms, we are trying to be less precise with the numbers that makes up the model weights so that it takes up less memory and can perform operations faster. <a href="https://stackoverflow.blog/2023/08/23/fitting-ai-models-in-your-pocket-with-quantization/" target="_blank" rel="noopener noreferrer">This StackOverflow blog</a> has great visualizations of how quantization works. Without repeating the main content, the key takeaway for me was this image: The fewer bits you use per pixel, the less memory needed, but the image quality also may decrease.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right"><img src="https://cdn.stackoverflow.co/images/jo7n4k8s/production/5ee6f4e98bf05001b3699344f784adad0177ebe0-688x444.gif?auto=format" alt="Representing images with varying number of bits." style="max-width:100%;height:auto"><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em">Image from StackOverflow.</figcaption></figure>
<p>Weights normally use 32-bit floating points, and are often quantized to float16 or <a href="https://github.com/TimDettmers/bitsandbytes" target="_blank" rel="noopener noreferrer">int8</a>. Ollama uses 4-bit quantization, which means instead of using 32 "101100..."s for one value, only 4 are used. That means theoretically, you get 8x savings in memory usage. Quantization can be broadly grouped into 2 main methods:</p>
<ol>
<li>Post Training Quantization (PTQ): Done after a model is trained. A "calibration dataset" is used to capture the distribution of activations to calculate quant parameters (scale, zero point) for all inputs. No re-training is needed.</li>
<li>Quantization Aware Training (QAT): Models are quantized during re-training/finetuning where low precision behaviour is simulated in the forward pass (backward pass remains the same). QAT is often able to better preserve accuracy when compared to PTQ, but incurs a high cost from re-training, which may not be suitable for LLMs.</li>
</ol>
<p>When it comes to quantizing LLM weights, methods like <a href="https://arxiv.org/abs/2305.14314" target="_blank" rel="noopener noreferrer">NF4</a>, <a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="noopener noreferrer">GPTQ</a>, <a href="https://arxiv.org/abs/2306.00978" target="_blank" rel="noopener noreferrer">AWQ</a>, and <a href="https://github.com/rustformers/llm/blob/main/crates/ggml/README.md" target="_blank" rel="noopener noreferrer">GGML/GGUF</a> are among the most popular. Key observation is that not all weights are equally important (as pointed out in the AWQ paper). Keeping higher precision for more critical layers / weights proved to be key in the balance of accuracy and resource usage. In particular, the Mixtral model from Ollama seems to be using GGML, which groups blocks of values and rounds them to a lower precision (as opposed to using a global parameter).</p>
<p><a href="https://arxiv.org/pdf/2312.17238.pdf" target="_blank" rel="noopener noreferrer">A recent paper</a> (3 weeks old!) that focuses on running Mixture-of-Experts type models on consumer hardware seems to have the answer to my misfortunes. In addition to quantization, they use one more trick in the book - offloading.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-offloading">What is offloading?<a href="https://isaac-chung.github.io/blog/quantized-models-dont-fit#what-is-offloading" class="hash-link" aria-label="Direct link to What is offloading?" title="Direct link to What is offloading?">​</a></h2>
<p>Offloading is putting some parameters in a separate, cheaper memory, such as system RAM, and only load them "just-in-time" when they are needed for computation. It proves to be very suitable for inferencing and training LLMs with limited GPU memory. In the context of using Mixtral, the MoE architecture contain multiple “experts” (layers) and a “gating function” that selects which experts are used on a given input. That way the MoE block only uses a small portion of all “experts” for any single forward pass. Each expert is offloaded separately and only brought pack to GPU when needed.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right"><img src="https://isaac-chung.github.io/assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg" alt="8 big apples barely fitting into a crate." style="max-width:100%;height:auto"><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em">Image by OpenAI DALL-E 3.</figcaption></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-run-mixtral-8x7b-for-free">How to run Mixtral-8x7B for free?<a href="https://isaac-chung.github.io/blog/quantized-models-dont-fit#how-to-run-mixtral-8x7b-for-free" class="hash-link" aria-label="Direct link to How to run Mixtral-8x7B for free?" title="Direct link to How to run Mixtral-8x7B for free?">​</a></h2>
<p>So here we are. This is by no means to run Mixtral for production use. Here is the <a href="https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb" target="_blank" rel="noopener noreferrer">colab notebook</a> by the authors of the paper mentioned above. Even though they were targeting the hardware specs of the Google free-tier instances, I might just be able to run Mixtral on my laptop after all.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="further-reading">Further Reading<a href="https://isaac-chung.github.io/blog/quantized-models-dont-fit#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading">​</a></h2>
<ol>
<li><a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes" target="_blank" rel="noopener noreferrer">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a></li>
<li><a href="https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172" target="_blank" rel="noopener noreferrer">Quantize Llama models with GGUF and llama.cpp</a></li>
<li><a href="https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34" target="_blank" rel="noopener noreferrer">4-bit Quantization with GPTQ</a></li>
</ol>]]></content:encoded>
            <category>quantization</category>
            <category>mixed</category>
            <category>AI</category>
            <category>LLM</category>
            <category>ML</category>
            <category>chatbot</category>
            <category>mixtral</category>
        </item>
        <item>
            <title><![CDATA[What is Ollama? A shallow dive into running LLMs locally]]></title>
            <link>https://isaac-chung.github.io/blog/what-is-ollama</link>
            <guid>https://isaac-chung.github.io/blog/what-is-ollama</guid>
            <pubDate>Sun, 07 Jan 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Being able to run LLMs locally and easily is truly a game changer. I have heard about Ollama before and decided to take a look at it this past weekend.]]></description>
            <content:encoded><![CDATA[<p>Being able to run LLMs locally and <em>easily</em> is truly a game changer. I have heard about <a href="https://github.com/jmorganca/ollama" target="_blank" rel="noopener noreferrer">Ollama</a> before and decided to take a look at it this past weekend.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I'll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>Why is running LLMs locally becoming a hot thang</li>
<li>What is Ollama?</li>
<li>Should you use Ollama?</li>
</ul></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-is-running-llms-locally-becoming-a-hot-thang">Why is running LLMs locally becoming a hot thang?<a href="https://isaac-chung.github.io/blog/what-is-ollama#why-is-running-llms-locally-becoming-a-hot-thang" class="hash-link" aria-label="Direct link to Why is running LLMs locally becoming a hot thang?" title="Direct link to Why is running LLMs locally becoming a hot thang?">​</a></h2>
<p>There was a time when LLMs were only accessible via cloud APIs from the giant providers like OpenAI and Anthropic. Don't get me wrong, those cloud API providers still dominate the market, and they have nice UIs that makes it easy for many users to get started. However, the price users pay (other than a pro plan or API costs) is that the providers will have full access to your chat data. For those who want run LLMs securely on their own hardware, they either had to train their own LLM (which is super costly), or wait till the release of Llama2 - an open weights model family. After that, a flood of how-to cookbooks and self-deployment launch services came onto the scene to help user deploy their own Llama2 "instance".</p>
<p>At this point, LLMs are still running on the cloud (just happens to be your own cloud), where management of the instances and GPUs could take up quite a lot of resources. It was necessary at the time because of the model sizes. The smallest Llama2 model (16bit floating point precision version of Llama2-7b-chat) comes in at 13GB. This means that most of the LLMs that are bigger that 7B (i.e. in the 10s or even 100s of GB in size) cannot fit into a regular laptop GPU.</p>
<p>Then came the hail mary - quantization (to be covered in a separate blog later). By quantizing the model weights to 4-bits, the <a href="https://ollama.ai/library/llama2:7b" target="_blank" rel="noopener noreferrer">Llama2-7b-chat model</a> now only takes up 3.8GB, which means, it'll finally fit!</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right"><img src="https://isaac-chung.github.io/assets/images/tiny-llama-eab3b5a919157baa1f102158a2858f98.png" alt="A tiny llama alongside a regular sized llama" style="max-width:100%;height:auto"><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em">Image by OpenAI DALL-E 3.</figcaption></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-ollama">What is Ollama?<a href="https://isaac-chung.github.io/blog/what-is-ollama#what-is-ollama" class="hash-link" aria-label="Direct link to What is Ollama?" title="Direct link to What is Ollama?">​</a></h2>
<p><a href="https://ollama.ai/" target="_blank" rel="noopener noreferrer">Ollama</a> is an open-source app that lets you run, create, and share large language models locally with a command-line interface on MacOS and Linux.
Given the name, Ollama began by supporting Llama2, then expanded its <a href="https://ollama.ai/library" target="_blank" rel="noopener noreferrer">model library</a> to include models like Mistral and Phi-2. Ollama makes it easy to get started with running LLMs on your own hardware in very little setup time.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="should-you-use-ollama">Should you use Ollama?<a href="https://isaac-chung.github.io/blog/what-is-ollama#should-you-use-ollama" class="hash-link" aria-label="Direct link to Should you use Ollama?" title="Direct link to Should you use Ollama?">​</a></h2>
<p>Yes, if you want to be able to run LLMs on your laptop, keep your chat data away from 3rd party services, and can interact with them via command line in a simple way. There are also many community integrations such as UIs and plugins in chat platforms. It might not be for you if you do not want to deal with setting up at all.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-get-started-with-ollama">How to get started with Ollama<a href="https://isaac-chung.github.io/blog/what-is-ollama#how-to-get-started-with-ollama" class="hash-link" aria-label="Direct link to How to get started with Ollama" title="Direct link to How to get started with Ollama">​</a></h2>
<p>It seems super simple.</p>
<ol>
<li>On Mac, simply <a href="https://ollama.ai/download/Ollama-darwin.zip" target="_blank" rel="noopener noreferrer">download the application</a>.</li>
<li>Then run this to start chatting with Llama2:</li>
</ol>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ollama run llama2</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In addition to chatting with text prompts, Ollama also supports:</p>
<ul>
<li><a href="https://github.com/jmorganca/ollama?tab=readme-ov-file#multimodal-models" target="_blank" rel="noopener noreferrer">multimodal inputs</a>: e.g. asking questions about an iamge</li>
<li><a href="https://github.com/jmorganca/ollama?tab=readme-ov-file#pass-in-prompt-as-arguments" target="_blank" rel="noopener noreferrer">passing an argument within a prompt</a>: e.g. summarize a README page</li>
<li><a href="https://github.com/jmorganca/ollama?tab=readme-ov-file#rest-api" target="_blank" rel="noopener noreferrer">serving as a REST API</a>: e.g. chat with the model using python scripts</li>
<li><a href="https://ollama.ai/blog/ollama-is-now-available-as-an-official-docker-image" target="_blank" rel="noopener noreferrer">running as a docker image</a>: e.g. Deploy Ollama with Kubernetes</li>
</ul>
<p>The <a href="https://github.com/jmorganca/ollama" target="_blank" rel="noopener noreferrer">official Github repo README page</a> has more examples.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="some-notes">Some notes<a href="https://isaac-chung.github.io/blog/what-is-ollama#some-notes" class="hash-link" aria-label="Direct link to Some notes" title="Direct link to Some notes">​</a></h2>
<p>After using Ollama for a weekend, I have noticed the following that may not be obvious at first glance:</p>
<ol>
<li>By hitting the <code>run</code> command, you start a chat session. This session will live until you exit from it or when you terminate process. Interestingly, chat state was not managed at the beginning, which means that you'd run into issues where <a href="https://github.com/jmorganca/ollama/issues/8" target="_blank" rel="noopener noreferrer">the model immediately forgets about the context right after a response</a>. Now, you can keep chatting and the model remembers what you entered as long as it fits within its context window. Once the context window is exceeded, Ollama will truncate the input from the beginngin until it fits the context window again <a href="https://github.com/jmorganca/ollama/pull/306" target="_blank" rel="noopener noreferrer">while keeping the system instructions</a>.</li>
<li>Ollama is based on <a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer">llama.cpp</a>, an implementation of the Llama architecture in plain C/C++ without dependencies using only CPU and RAM.</li>
<li>Ollama is quite docker-like, and for me it feels intuitive. You pull models then run them. The <a href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md" target="_blank" rel="noopener noreferrer">Modelfile</a>, the "blueprint to create and share models with Ollama", is also quite dockerfile-like.</li>
</ol>
<p>Overall I find Ollama quite easy to use and would likely continue to use it for something quick. It would be pretty fun if <a href="https://github.com/jmorganca/ollama/issues/142" target="_blank" rel="noopener noreferrer">conversation history can be persisted</a>!</p>]]></content:encoded>
            <category>ollama</category>
            <category>llama</category>
            <category>chat</category>
            <category>AI</category>
            <category>LLM</category>
            <category>ML</category>
            <category>chatbot</category>
            <category>local</category>
        </item>
    </channel>
</rss>