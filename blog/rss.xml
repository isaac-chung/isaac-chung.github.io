<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Isaac Chung Blog</title>
        <link>https://isaac-chung.github.io/blog</link>
        <description>Isaac Chung Blog</description>
        <lastBuildDate>Sat, 13 Apr 2024 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <copyright>Copyright © 2024 Isaac Chung</copyright>
        <item>
            <title><![CDATA[Serving Concurrent Requests with Quantized LLMs]]></title>
            <link>https://isaac-chung.github.io/blog/concurrent-requests</link>
            <guid>https://isaac-chung.github.io/blog/concurrent-requests</guid>
            <pubDate>Sat, 13 Apr 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Being able to serve concurrent LLM generation requests are crucial to production LLM applications that have multiple users. I recently gave a talk at PyCon Lithuania on serving quantized LLMs with llama-cpp-python, an open source python library that helps serve quantized models in the GGUF format. At the end, a question came from the audience about supporting multiple users and concurrent requests. I decided to take a deeper look into why the library wasn't able to support that at the time.]]></description>
            <content:encoded><![CDATA[<p>Being able to serve concurrent LLM generation requests are crucial to production LLM applications that have multiple users. <a href="https://pycon.lt/2024/talks/DHBLXW" target="_blank" rel="noopener noreferrer">I recently gave a talk at PyCon Lithuania</a> on serving quantized LLMs with <a href="https://github.com/abetlen/llama-cpp-python" target="_blank" rel="noopener noreferrer">llama-cpp-python</a>, an open source python library that helps serve quantized models in the GGUF format. At the end, a question came from the audience about supporting multiple users and concurrent requests. I decided to take a deeper look into why the library wasn't able to support that at the time.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I'll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>What are the challenges of serving concurrent requests with LLMs?</li>
<li>How to serve concurrent requests with quantized LLMs?</li>
</ul></div></div>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="https://isaac-chung.github.io/assets/images/octopus-69454e9dcdc193dfb7a6f3ba4040312b.png" data-pswp-width="0" data-pswp-height="0"><img src="https://isaac-chung.github.io/assets/images/octopus-69454e9dcdc193dfb7a6f3ba4040312b.png" alt="New yorker style comic depicting a cute, friendly cartoon octopus with a baseball cap holding multiple tennis rackets. White background only." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em">Image by Dalle3.</figcaption></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-challenges-of-serving-concurrent-requests-with-llms">What are the challenges of serving concurrent requests with LLMs?<a href="https://isaac-chung.github.io/blog/concurrent-requests#what-are-the-challenges-of-serving-concurrent-requests-with-llms" class="hash-link" aria-label="Direct link to What are the challenges of serving concurrent requests with LLMs?" title="Direct link to What are the challenges of serving concurrent requests with LLMs?">​</a></h2>
<p>LLM inference involves generating tokens in an autoregressive manner. To avoid repeating calculations when generating future tokens, a KV cache is used.
The KV cache size grows quickly with the number of requests. The <a href="https://arxiv.org/abs/2309.06180" target="_blank" rel="noopener noreferrer">vLLM paper</a> uses the 13B OPT model as an example:</p>
<blockquote>
<p>the KV cache of a single token demands 800 KB of space, calculated as 2 (key and value vectors) × 5120 (hidden state size) × 40 (number of layers) × 2 (bytes per FP16). Since OPT can generate sequences up to 2048 tokens, the memory required to store the KV cache of one request can be as much as 1.6 GB</p>
</blockquote>
<p>It's clear that this is a memory-bound process. In the context of serving requests, we can batch multiple incoming requests to improve compute utilization. Batching isn't trivial either. The main challenges includes:</p>
<ol>
<li>The requests may arrive at different times, and</li>
<li>the requests may have very different input and output lengths.</li>
</ol>
<p>Batching requests naively may lead to huge delays from waiting for earlier requests to finish before starting the next batch, or waiting for the longest generation to finish. It would also lead to computation and memory wastage from padding inputs/outputs due to their difference in lengths.</p>
<p>Back to the question from the talk. At the time of writing, llama-cpp-python did not support batched requests. Moreover, concurrent requests would <a href="https://www.reddit.com/r/LocalLLaMA/comments/15kbbna/how_to_make_multiple_inference_requests_from_a/" target="_blank" rel="noopener noreferrer">lead to the server crashing</a>. This might be due to the locks being used to control access to shared resources, particularly the <code>llama_proxy</code> variable, which handles the model resources. This means that to serve parallel requests, multiple instances of the models may be needed, and this drastically increases the resources required for model serving.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-serve-concurrent-requests-with-quantized-llms">How to serve concurrent requests with quantized LLMs?<a href="https://isaac-chung.github.io/blog/concurrent-requests#how-to-serve-concurrent-requests-with-quantized-llms" class="hash-link" aria-label="Direct link to How to serve concurrent requests with quantized LLMs?" title="Direct link to How to serve concurrent requests with quantized LLMs?">​</a></h2>
<p>Improvements have been introduced to make serving concurrent requests more efficient. These include and are not limited to:</p>
<ol>
<li><strong>Continuous Batching</strong>: It allows new requests to join the current batch in the next decoder cycle instead of waiting for the end of the current batch to finish. This improves throughput and compute utilization. The upstream <a href="https://github.com/ggerganov/llama.cpp/tree/master/examples/server" target="_blank" rel="noopener noreferrer">llama.cpp repo</a> has the capability to serve parallel requests with continuous batching. This, however, does not entirely solve the memory issue. We still need to reserve memory using the longest sequence in the batch.</li>
<li><strong>Paged Attention</strong>: It divides up the KV cache into blocks that would contain keys and values for a fixed number of tokens. It eliminates external fragmentation (unusable gaps between allocated memory blocks in a GPU's memory) since all blocks have the same size. Also, it eases internal fragmentation by using relatively small blocks. An LLM serving engine that's built on top of Paged Attention is <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a>.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-vllm">What is vLLM?<a href="https://isaac-chung.github.io/blog/concurrent-requests#what-is-vllm" class="hash-link" aria-label="Direct link to What is vLLM?" title="Direct link to What is vLLM?">​</a></h2>
<p>vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. vLLM supports serving quantized LLMs with concurrent requests with AWQ (4-bit) models, which boasts a 2.78x speed up on a single GPU according to <a href="https://lightning.ai/lightning-ai/studios/optimized-llm-inference-api-for-mistral-7b-using-vllm" target="_blank" rel="noopener noreferrer">a benchmark done by lightning.ai</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="final-thoughts">Final Thoughts<a href="https://isaac-chung.github.io/blog/concurrent-requests#final-thoughts" class="hash-link" aria-label="Direct link to Final Thoughts" title="Direct link to Final Thoughts">​</a></h2>
<p>Tools like llama-cpp-python and ExLlama2 may not have been designed to be <a href="https://github.com/turboderp/exllamav2/issues/95#issuecomment-2019991153" target="_blank" rel="noopener noreferrer">an efficient backend for large deployments</a>, but they are certainly worth a look for anyone who wants to deploy way smaller models on a budget. vLLM on the other hand seem to have that in mind from the beginning. It can even be used with the <a href="https://github.com/triton-inference-server/vllm_backend" target="_blank" rel="noopener noreferrer">triton inference server as a supported backend</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="further-reading">Further Reading<a href="https://isaac-chung.github.io/blog/concurrent-requests#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading">​</a></h2>
<ul>
<li>Check out <a href="https://www.youtube.com/channel/UCtAcpQcYerN8xxZJYTfWBMw/videos" target="_blank" rel="noopener noreferrer">this YouTube channel</a> for in-depth explanations about popular transformer/LLM architectures. It helped me get through some dense materials clearly, so thank you, Umar!</li>
<li>This blog on <a href="https://www.run.ai/blog/serving-large-language-models" target="_blank" rel="noopener noreferrer">Serving LLMs by run.ai</a>.</li>
</ul>]]></content:encoded>
            <category>LLM</category>
            <category>concurrent</category>
            <category>requests</category>
            <category>parallel</category>
            <category>multi-user</category>
        </item>
        <item>
            <title><![CDATA[How to really know if your RAG system is working well.]]></title>
            <link>https://isaac-chung.github.io/blog/rag-eval-and-observability</link>
            <guid>https://isaac-chung.github.io/blog/rag-eval-and-observability</guid>
            <pubDate>Sun, 24 Mar 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[We know that building a Retrieval Augmented Generation (RAG) proof of concept is easy, but making it production-ready can be hard. There are no shortage of tips and tricks out there for us to try, but at the end of the day, it all depends on our data and our application. Transitioning RAG into production follows similar principles to other production systems. Scaling up to handle more data and users, smooth error/exception handling, and getting it to play nice with other systems are some of the main challenges to tackle. How can we really know if our RAG system is working well? and how well? To find out, we should take a look at each component under the hood and be able to evaluate the pipeline with clear metrics.]]></description>
            <content:encoded><![CDATA[<p>We know that building a Retrieval Augmented Generation (RAG) proof of concept is easy, but making it production-ready can be hard. There are no shortage of tips and tricks out there for us to try, but at the end of the day, it all depends on our data and our application. Transitioning RAG into production follows similar principles to other production systems. Scaling up to handle more data and users, smooth error/exception handling, and getting it to play nice with other systems are some of the main challenges to tackle. How can we really know if our RAG system is working well? and how well? To find out, we should take a look at each component under the hood and be able to evaluate the pipeline with clear metrics.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I'll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>How to look under the hood in a RAG system?</li>
<li>How to evaluate RAG systems?</li>
</ul></div></div>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="https://isaac-chung.github.io/assets/images/books-and-robot-1c8bc8b9accc3ef2e643d2d0e5e005da.png" data-pswp-width="0" data-pswp-height="0"><img src="https://isaac-chung.github.io/assets/images/books-and-robot-1c8bc8b9accc3ef2e643d2d0e5e005da.png" alt="A robot sitting on a large pile of books." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em">Image by Stable Diffusion XL.</figcaption></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-look-under-the-hood-in-a-rag-system">How to look under the hood in a RAG system?<a href="https://isaac-chung.github.io/blog/rag-eval-and-observability#how-to-look-under-the-hood-in-a-rag-system" class="hash-link" aria-label="Direct link to How to look under the hood in a RAG system?" title="Direct link to How to look under the hood in a RAG system?">​</a></h2>
<p>Once the components are set up in the RAG system, it is tempting to spot-check it for performance, and try out some <em>advanced techniques</em> with the promise of performance improvements. However, this isn't the most reliable nor structural approach to debugging and improving RAG. The first thing we should do after getting our first end-to-end RAG response is adding observability. This greatly helps us not only during the transition of our RAG system from POC to production but also in its post-launch maintenance phase.</p>
<p>Observability is crucial in RAG production systems for several main reasons:</p>
<ol>
<li><strong>Detecting Issues</strong>: Observability allows for the detection of issues and anomalies within a system. By monitoring various metrics, logs, and traces, we can quickly identify when something goes wrong and take appropriate action to resolve the issue before it impacts users.</li>
<li><strong>Root Cause Analysis</strong>: When problems occur, especially during the development phase, observability enables us to perform root cause analysis efficiently. By examining the data collected from various components, we can trace back the source of the problem and address it effectively. More important, in production this would help reduce downtime and minimizing the impact on users.</li>
<li><strong>Performance Optimization</strong>: Observability provides insights into the performance of the system. By monitoring metrics such as response times, throughput, and resource utilization, we can identify bottlenecks and areas for optimization, leading to better overall performance and user experience.</li>
</ol>
<p>This could be as simple as logging inputs and outputs of each component (e.g. simple setting in <a href="https://docs.llamaindex.ai/en/stable/module_guides/observability/#simple-llm-inputsoutputs" target="_blank" rel="noopener noreferrer">llama-index</a>). There are a variety of LLM observability tools to help trace the timings and outputs at each step of a RAG system. Some of these have minimal config needed, have no pricing page, and are open source, and they are:</p>
<ul>
<li><a href="https://github.com/traceloop/openllmetry" target="_blank" rel="noopener noreferrer">OpenLLMetry</a>: Built on top of OpenTelemetry. If you’re using an LLM framework like Haystack, Langchain or LlamaIndex, there is no need to add any annotations to your code.</li>
<li><a href="https://github.com/Arize-ai/phoenix" target="_blank" rel="noopener noreferrer">Arize Phoenix</a>: Built on top of the OpenInference tracing standard, and uses it to trace, export, and collect critical information about your LLM Application in the form of "spans". It also supports several RAG-related analyses and visualizations.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-evaluate-rag-systems">How to evaluate RAG systems?<a href="https://isaac-chung.github.io/blog/rag-eval-and-observability#how-to-evaluate-rag-systems" class="hash-link" aria-label="Direct link to How to evaluate RAG systems?" title="Direct link to How to evaluate RAG systems?">​</a></h2>
<p>Just like any system, it is important to understand how well the RAG system is performing and how much improvement has been achieved over the baseline. This doesn’t just involve measuring how fast and how much it costs, but also how good the outputs are. We could take a look at RAG-specific evaluation methods. Per-component evaluations, like unit tests, can be done on the retrieval stage and the generation stage separately.</p>
<p>For retrieval, the goal is to find out given the configuration how well can the system retrieve relevant results? Here you would need a golden set of queries and ground truth of relevant documents (or their IDs).  You could use IR metrics like nDCG or Mean Reciprocal Rank (MRR), but for RAG it’s more meaningful to understand 1) the signal to noise ratio of the retrieved context (context precision) and 2) how well it can retrieve all the relevant information required to answer the question (context recall).</p>
<p>For generation, the goal is to find out, given the relevant documents in the context, 1) how factually accurate is the generated answer (faithfulness), and 2) how relevant is the generated answer to the question (answer relevancy). It is also important to evaluate the full pipeline end to end. This might involve some manual efforts to start with or asking an LLM to verify whether the answer is correct. A proxy for gauging how close the generated answer to the ground truth answer could be semantic similarity.</p>
<p>Some open source RAG evaluation tools like <a href="https://github.com/explodinggradients/ragas" target="_blank" rel="noopener noreferrer">Ragas</a> offer readily available guide to evaluate your RAG system with predefined metrics and iterate your RAG system with user feedback in production. Ragas, in particular, offers the ability generate a synthetic test set for “reference-free” evaluation, which means that instead of relying on human-annotated test set, Ragas leverages LLMs under the hood to conduct the evaluations.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-bottom-line">The Bottom Line<a href="https://isaac-chung.github.io/blog/rag-eval-and-observability#the-bottom-line" class="hash-link" aria-label="Direct link to The Bottom Line" title="Direct link to The Bottom Line">​</a></h2>
<p>Fight the urge of treating the RAG system as a black box. Use a structured approach to evaluate your RAG system in terms of performance and other requirements like latency by adding observability and using evaluation tools.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="further-reading">Further Reading<a href="https://isaac-chung.github.io/blog/rag-eval-and-observability#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading">​</a></h2>
<ul>
<li>Catch this talk on <a href="https://pycon.lt/2024/talks/HFXHRV" target="_blank" rel="noopener noreferrer">"Transcend the Knowledge Barriers in RAG"</a> at PyCon Lithuania to understand how each RAG component works under the hood.</li>
<li><a href="https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c" target="_blank" rel="noopener noreferrer">12 RAG Pain points</a></li>
</ul>]]></content:encoded>
            <category>RAG</category>
            <category>LLM</category>
            <category>Evaluation</category>
            <category>Tracing</category>
            <category>Logging</category>
            <category>Observability</category>
        </item>
        <item>
            <title><![CDATA[All about Timing: A quick look at metrics for LLM serving]]></title>
            <link>https://isaac-chung.github.io/blog/llm-serving</link>
            <guid>https://isaac-chung.github.io/blog/llm-serving</guid>
            <pubDate>Sun, 21 Jan 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[How do you measure the performance of LLM serving systems? Production services in engineering are often evaluated using metrics like Requests Per Second (RPS), uptime, and latency. In computer vision systems, Frames Per Second (FPS) is often used as the main model throughput metric for use cases that involve near-real-time detection and tracking. Does serving LLMs have something similar? Certainly. A recent conversation with my team (after they read my Ollama blog) got me thinking about additional metrics that we could be tracking.]]></description>
            <content:encoded><![CDATA[<p>How do you measure the performance of LLM serving systems? Production services in engineering are often evaluated using metrics like Requests Per Second (RPS), uptime, and latency. In computer vision systems, Frames Per Second (FPS) is often used as the main model throughput metric for use cases that involve near-real-time detection and tracking. Does serving LLMs have something similar? Certainly. A recent conversation with my team (after they read my <a href="https://isaac-chung.github.io/blog/what-is-ollama">Ollama blog</a>) got me thinking about additional metrics that we could be tracking.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I'll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>What metrics does Ollama provide?</li>
<li>Why can't I just use tokens per second?</li>
<li>What other LLM serving metrics should I consider?</li>
</ul></div></div>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="https://isaac-chung.github.io/assets/images/coach-19431ad943236168227c2f70e17605bd.jpg" data-pswp-width="0" data-pswp-height="0"><img src="https://isaac-chung.github.io/assets/images/coach-19431ad943236168227c2f70e17605bd.jpg" alt="A baseball coach blowing a whistle while holding a stopwatch." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em">Image by OpenAI DALL-E 3.</figcaption></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-metrics-does-ollama-provide">What metrics does Ollama provide?<a href="https://isaac-chung.github.io/blog/llm-serving#what-metrics-does-ollama-provide" class="hash-link" aria-label="Direct link to What metrics does Ollama provide?" title="Direct link to What metrics does Ollama provide?">​</a></h2>
<p>To see the metrics for each response Ollama provides, add the <code>--verbose</code> flag after the run command. e.g. <code>ollama run llama2 --verbose</code>. Here is an example output with 8 different metrics.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">total duration:       58.502942674s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">load duration:        7.185349ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">prompt eval count:    31 token(s)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">prompt eval duration: 4.044684s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">prompt eval rate:     7.66 tokens/s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">eval count:           266 token(s)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">eval duration:        54.44846s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">eval rate:            4.89 tokens/s</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>I added some indentation and rearranged it a bit so that it's easier to discern which parts are together:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">total duration:       58.502942674s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    1. load duration:        7.185349ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    2. prompt eval duration: 4.044684s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        prompt eval count:    31 token(s)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        prompt eval rate:     7.66 tokens/s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    3. eval duration:        54.44846s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        eval count:           266 token(s)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        eval rate:            4.89 tokens/s</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Total duration can be seen as latency (the overall time from receiving a request to returning a response to the user). This metric often includes all of the overhead in addition to the time a model needs to generate a response, e.g. model load time.</p>
<p>Load duration is the time taken to load the model into memory. Prompt eval is the stage of processing the input prompt. Eval is the stage of generating output.</p>
<p>Tokens per second is a common metric to use for output generation. Looking at the eval rate, this system achieved 4.89 "(output) tokens per second". For comparison, <a href="https://www.reddit.com/r/LocalLLaMA/comments/162pgx9/what_do_yall_consider_acceptable_tokens_per/" target="_blank" rel="noopener noreferrer">7-10 tokens/second is thought to be acceptable for general use</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-cant-i-just-use-tokens-per-second">Why can't I just use tokens per second?<a href="https://isaac-chung.github.io/blog/llm-serving#why-cant-i-just-use-tokens-per-second" class="hash-link" aria-label="Direct link to Why can't I just use tokens per second?" title="Direct link to Why can't I just use tokens per second?">​</a></h2>
<p>A single metric is rarely enough to capture the whole picture. It's important to note that all tokens are not made equal. For example, <a href="https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper" target="_blank" rel="noopener noreferrer">Llama2 tokenization is 19% longer than that of ChatGPT</a>, yet it's still much cheaper. This should be considered when serving LLMs of different families.</p>
<p>Also, the inference system is often used by more than one user at a time (multiple concurrent users). LLMs likely power a bigger system, such as a chat service. You might want to give users a good experience by considering tokens per second <em>per user</em>, or tokens per second at different request rates.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-other-llm-serving-metrics-should-i-consider">What other LLM serving metrics should I consider?<a href="https://isaac-chung.github.io/blog/llm-serving#what-other-llm-serving-metrics-should-i-consider" class="hash-link" aria-label="Direct link to What other LLM serving metrics should I consider?" title="Direct link to What other LLM serving metrics should I consider?">​</a></h2>
<p>A common metric to use is <strong>Time To First Token (TTFT)</strong>, which is how quickly users start seeing model outputs after a request is sent. Low waiting times are important in real-time interactions, but less so in offline workloads. This metric measures the time required to a) process the prompt and then b) generate the first output token. From the example above, the TTFT is just over 4 seconds as the load time was almost negligible.</p>
<p>Another way to represent tokens per second is its inverse: <strong>Time Per Output Token (TPOT)</strong>, the time to generate one output token for each request. This metric can be used to perceive the "speed" of the model. From the example above, the TPOT is 200ms per token.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="final-thought-yes-just-one">Final Thought (yes, just one)<a href="https://isaac-chung.github.io/blog/llm-serving#final-thought-yes-just-one" class="hash-link" aria-label="Direct link to Final Thought (yes, just one)" title="Direct link to Final Thought (yes, just one)">​</a></h2>
<p>Overall, it's important to keep the goal of the system and user requirements in mind and avoid blindly optimizing metrics for optimization's sake.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="further-reading">Further Reading<a href="https://isaac-chung.github.io/blog/llm-serving#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading">​</a></h2>
<p>Here are some extra resources for some background on how LLMs generate text and deeper dives on LLM serving:</p>
<ol>
<li><a href="https://jalammar.github.io/illustrated-gpt2/#part-1-got-and-language-modeling" target="_blank" rel="noopener noreferrer">How LLMs use decoder blocks for generation</a> with illustrations</li>
<li><a href="https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices" target="_blank" rel="noopener noreferrer">Best Practices for LLM Inference</a> from MosaicML</li>
<li><a href="https://www.baseten.co/blog/faster-mixtral-inference-with-tensorrt-llm-and-quantization/" target="_blank" rel="noopener noreferrer">Faster Mixtral inference with TensorRT-LLM</a> by Baseten</li>
</ol>]]></content:encoded>
            <category>metrics</category>
            <category>serving</category>
            <category>timing</category>
            <category>AI</category>
            <category>LLM</category>
        </item>
        <item>
            <title><![CDATA[When Quantized Models Still Don't Fit]]></title>
            <link>https://isaac-chung.github.io/blog/quantized-models-dont-fit</link>
            <guid>https://isaac-chung.github.io/blog/quantized-models-dont-fit</guid>
            <pubDate>Sun, 14 Jan 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That's what happened to me when I was trying to run Mixtral-8x7B with Ollama (check out this previous blog post on what Ollama is). The model requires 26GB of RAM while my laptop only has 16GB. I'll try to walk through the workaround a bit at a time (pun intended).]]></description>
            <content:encoded><![CDATA[<p>A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That's what happened to me when I was trying to run <a href="https://ollama.ai/library/mixtral" target="_blank" rel="noopener noreferrer">Mixtral-8x7B with Ollama</a> (check out this <a href="https://isaac-chung.github.io/blog/what-is-ollama">previous blog post on what Ollama is</a>). The model requires 26GB of RAM while my laptop only has 16GB. I'll try to walk through the workaround a <em>bit</em> at a time (pun intended).</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I'll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>What is quantization?</li>
<li>What is offloading?</li>
<li>How to run Mixtral-8x7B for free?</li>
</ul></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-quantization">What is quantization?<a href="https://isaac-chung.github.io/blog/quantized-models-dont-fit#what-is-quantization" class="hash-link" aria-label="Direct link to What is quantization?" title="Direct link to What is quantization?">​</a></h2>
<p>Quantization generally is a process that converts continuous values to a discrete set of values. A tangible analogy would be how we tell time: time is continuous, and we use hours, minutes, and seconds to "quantize" time. Sometimes "around 10am" is good enough, and sometimes we want to be precise to the millisecond.
In the context of deep learning, it is a technique to reduce the computational and memory costs of running a model by using lower-precision numerical types to represent its weights and activations. In simpler terms, we are trying to be less precise with the numbers that makes up the model weights so that it takes up less memory and can perform operations faster. <a href="https://stackoverflow.blog/2023/08/23/fitting-ai-models-in-your-pocket-with-quantization/" target="_blank" rel="noopener noreferrer">This StackOverflow blog</a> has great visualizations of how quantization works. Without repeating the main content, the key takeaway for me was this image: The fewer bits you use per pixel, the less memory needed, but the image quality also may decrease.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="https://cdn.stackoverflow.co/images/jo7n4k8s/production/5ee6f4e98bf05001b3699344f784adad0177ebe0-688x444.gif?auto=format" data-pswp-width="0" data-pswp-height="0"><img src="https://cdn.stackoverflow.co/images/jo7n4k8s/production/5ee6f4e98bf05001b3699344f784adad0177ebe0-688x444.gif?auto=format" alt="Representing images with varying number of bits." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em">Image from StackOverflow.</figcaption></figure>
<p>Weights normally use 32-bit floating points, and are often quantized to float16 or <a href="https://github.com/TimDettmers/bitsandbytes" target="_blank" rel="noopener noreferrer">int8</a>. Ollama uses 4-bit quantization, which means instead of using 32 "101100..."s for one value, only 4 are used. That means theoretically, you get 8x savings in memory usage. Quantization can be broadly grouped into 2 main methods:</p>
<ol>
<li>Post Training Quantization (PTQ): Done after a model is trained. A "calibration dataset" is used to capture the distribution of activations to calculate quant parameters (scale, zero point) for all inputs. No re-training is needed.</li>
<li>Quantization Aware Training (QAT): Models are quantized during re-training/finetuning where low precision behaviour is simulated in the forward pass (backward pass remains the same). QAT is often able to better preserve accuracy when compared to PTQ, but incurs a high cost from re-training, which may not be suitable for LLMs.</li>
</ol>
<p>When it comes to quantizing LLM weights, methods like <a href="https://arxiv.org/abs/2305.14314" target="_blank" rel="noopener noreferrer">NF4</a>, <a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="noopener noreferrer">GPTQ</a>, <a href="https://arxiv.org/abs/2306.00978" target="_blank" rel="noopener noreferrer">AWQ</a>, and <a href="https://github.com/rustformers/llm/blob/main/crates/ggml/README.md" target="_blank" rel="noopener noreferrer">GGML/GGUF</a> are among the most popular. A key observation is that not all weights are equally important (as pointed out in the AWQ paper). Keeping higher precision for more critical layers / weights proved to be key in the balance of accuracy and resource usage. In particular, the Mixtral model from Ollama seems to be using GGML, which groups blocks of values and rounds them to a lower precision (as opposed to using a global parameter).</p>
<p><a href="https://arxiv.org/pdf/2312.17238.pdf" target="_blank" rel="noopener noreferrer">A recent paper</a> (3 weeks old!) that focuses on running Mixture-of-Experts type models on consumer hardware seems to have the answer to my misfortunes. In addition to quantization, they use one more trick in the book - offloading.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-offloading">What is offloading?<a href="https://isaac-chung.github.io/blog/quantized-models-dont-fit#what-is-offloading" class="hash-link" aria-label="Direct link to What is offloading?" title="Direct link to What is offloading?">​</a></h2>
<p>Offloading is putting some parameters in a separate, cheaper memory, such as system RAM, and only load them "just-in-time" when they are needed for computation. It proves to be very suitable for inferencing and training LLMs with limited GPU memory. In the context of using Mixtral, the MoE architecture contain multiple “experts” (layers) and a “gating function” that selects which experts are used on a given input. That way the MoE block only uses a small portion of all “experts” for any single forward pass. Each expert is offloaded separately and only brought pack to GPU when needed.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="https://isaac-chung.github.io/assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg" data-pswp-width="0" data-pswp-height="0"><img src="https://isaac-chung.github.io/assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg" alt="8 big apples barely fitting into a crate." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em">Image by OpenAI DALL-E 3.</figcaption></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-run-mixtral-8x7b-for-free">How to run Mixtral-8x7B for free?<a href="https://isaac-chung.github.io/blog/quantized-models-dont-fit#how-to-run-mixtral-8x7b-for-free" class="hash-link" aria-label="Direct link to How to run Mixtral-8x7B for free?" title="Direct link to How to run Mixtral-8x7B for free?">​</a></h2>
<p>So here we are. This is by no means to run Mixtral for production use. Here is the <a href="https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb" target="_blank" rel="noopener noreferrer">colab notebook</a> by the authors of the paper mentioned above. Even though they were targeting the hardware specs of the Google free-tier instances, I might just be able to run Mixtral on my laptop after all.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="further-reading">Further Reading<a href="https://isaac-chung.github.io/blog/quantized-models-dont-fit#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading">​</a></h2>
<ol>
<li><a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes" target="_blank" rel="noopener noreferrer">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a></li>
<li><a href="https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172" target="_blank" rel="noopener noreferrer">Quantize Llama models with GGUF and llama.cpp</a></li>
<li><a href="https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34" target="_blank" rel="noopener noreferrer">4-bit Quantization with GPTQ</a></li>
</ol>]]></content:encoded>
            <category>quantization</category>
            <category>mixed</category>
            <category>AI</category>
            <category>LLM</category>
            <category>ML</category>
            <category>chatbot</category>
            <category>mixtral</category>
        </item>
        <item>
            <title><![CDATA[What is Ollama? A shallow dive into running LLMs locally]]></title>
            <link>https://isaac-chung.github.io/blog/what-is-ollama</link>
            <guid>https://isaac-chung.github.io/blog/what-is-ollama</guid>
            <pubDate>Sun, 07 Jan 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Being able to run LLMs locally and easily is truly a game changer. I have heard about Ollama before and decided to take a look at it this past weekend.]]></description>
            <content:encoded><![CDATA[<p>Being able to run LLMs locally and <em>easily</em> is truly a game changer. I have heard about <a href="https://github.com/jmorganca/ollama" target="_blank" rel="noopener noreferrer">Ollama</a> before and decided to take a look at it this past weekend.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I'll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>Why is running LLMs locally becoming a hot thang</li>
<li>What is Ollama?</li>
<li>Should you use Ollama?</li>
</ul></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-is-running-llms-locally-becoming-a-hot-thang">Why is running LLMs locally becoming a hot thang?<a href="https://isaac-chung.github.io/blog/what-is-ollama#why-is-running-llms-locally-becoming-a-hot-thang" class="hash-link" aria-label="Direct link to Why is running LLMs locally becoming a hot thang?" title="Direct link to Why is running LLMs locally becoming a hot thang?">​</a></h2>
<p>There was a time when LLMs were only accessible via cloud APIs from the giant providers like OpenAI and Anthropic. Don't get me wrong, those cloud API providers still dominate the market, and they have nice UIs that makes it easy for many users to get started. However, the price users pay (other than a pro plan or API costs) is that the providers will have full access to your chat data. For those who want run LLMs securely on their own hardware, they either had to train their own LLM (which is super costly), or wait till the release of Llama2 - an open weights model family. After that, a flood of how-to cookbooks and self-deployment launch services came onto the scene to help user deploy their own Llama2 "instance".</p>
<p>At this point, LLMs are still running on the cloud (just happens to be your own cloud), where management of the instances and GPUs could take up quite a lot of resources. It was necessary at the time because of the model sizes. The smallest Llama2 model (16bit floating point precision version of Llama2-7b-chat) comes in at 13GB. This means that most of the LLMs that are bigger that 7B (i.e. in the 10s or even 100s of GB in size) cannot fit into a regular laptop GPU.</p>
<p>Then came the hail mary - quantization (to be covered in a separate blog later). By quantizing the model weights to 4-bits, the <a href="https://ollama.ai/library/llama2:7b" target="_blank" rel="noopener noreferrer">Llama2-7b-chat model</a> now only takes up 3.8GB, which means, it'll finally fit!</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="https://isaac-chung.github.io/assets/images/tiny-llama-eab3b5a919157baa1f102158a2858f98.png" data-pswp-width="0" data-pswp-height="0"><img src="https://isaac-chung.github.io/assets/images/tiny-llama-eab3b5a919157baa1f102158a2858f98.png" alt="A tiny llama alongside a regular sized llama" style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em">Image by OpenAI DALL-E 3.</figcaption></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-ollama">What is Ollama?<a href="https://isaac-chung.github.io/blog/what-is-ollama#what-is-ollama" class="hash-link" aria-label="Direct link to What is Ollama?" title="Direct link to What is Ollama?">​</a></h2>
<p><a href="https://ollama.ai/" target="_blank" rel="noopener noreferrer">Ollama</a> is an open-source app that lets you run, create, and share large language models locally with a command-line interface on MacOS and Linux.
Given the name, Ollama began by supporting Llama2, then expanded its <a href="https://ollama.ai/library" target="_blank" rel="noopener noreferrer">model library</a> to include models like Mistral and Phi-2. Ollama makes it easy to get started with running LLMs on your own hardware in very little setup time.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="should-you-use-ollama">Should you use Ollama?<a href="https://isaac-chung.github.io/blog/what-is-ollama#should-you-use-ollama" class="hash-link" aria-label="Direct link to Should you use Ollama?" title="Direct link to Should you use Ollama?">​</a></h2>
<p>Yes, if you want to be able to run LLMs on your laptop, keep your chat data away from 3rd party services, and can interact with them via command line in a simple way. There are also many community integrations such as UIs and plugins in chat platforms. It might not be for you if you do not want to deal with setting up at all.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-get-started-with-ollama">How to get started with Ollama<a href="https://isaac-chung.github.io/blog/what-is-ollama#how-to-get-started-with-ollama" class="hash-link" aria-label="Direct link to How to get started with Ollama" title="Direct link to How to get started with Ollama">​</a></h2>
<p>It seems super simple.</p>
<ol>
<li>On Mac, simply <a href="https://ollama.ai/download/Ollama-darwin.zip" target="_blank" rel="noopener noreferrer">download the application</a>.</li>
<li>Then run this to start chatting with Llama2:</li>
</ol>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ollama run llama2</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In addition to chatting with text prompts, Ollama also supports:</p>
<ul>
<li><a href="https://github.com/jmorganca/ollama?tab=readme-ov-file#multimodal-models" target="_blank" rel="noopener noreferrer">multi-modal inputs</a>: e.g. asking questions about an image</li>
<li><a href="https://github.com/jmorganca/ollama?tab=readme-ov-file#pass-in-prompt-as-arguments" target="_blank" rel="noopener noreferrer">passing an argument within a prompt</a>: e.g. summarize a README page</li>
<li><a href="https://github.com/jmorganca/ollama?tab=readme-ov-file#rest-api" target="_blank" rel="noopener noreferrer">serving as a REST API</a>: e.g. chat with the model using python scripts</li>
<li><a href="https://ollama.ai/blog/ollama-is-now-available-as-an-official-docker-image" target="_blank" rel="noopener noreferrer">running as a docker image</a>: e.g. Deploy Ollama with Kubernetes</li>
</ul>
<p>The <a href="https://github.com/jmorganca/ollama" target="_blank" rel="noopener noreferrer">official Github repo README page</a> has more examples.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="some-notes">Some notes<a href="https://isaac-chung.github.io/blog/what-is-ollama#some-notes" class="hash-link" aria-label="Direct link to Some notes" title="Direct link to Some notes">​</a></h2>
<p>After using Ollama for a weekend, I have noticed the following that may not be obvious at first glance:</p>
<ol>
<li>By hitting the <code>run</code> command, you start a chat session. This session will live until you exit from it or when you terminate process. Interestingly, chat state was not managed at the beginning, which means that you'd run into issues where <a href="https://github.com/jmorganca/ollama/issues/8" target="_blank" rel="noopener noreferrer">the model immediately forgets about the context right after a response</a>. Now, you can keep chatting and the model remembers what you entered as long as it fits within its context window. Once the context window is exceeded, Ollama will truncate the input from the beginning until it fits the context window again <a href="https://github.com/jmorganca/ollama/pull/306" target="_blank" rel="noopener noreferrer">while keeping the system instructions</a>.</li>
<li>Ollama is based on <a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer">llama.cpp</a>, an implementation of the Llama architecture in plain C/C++ without dependencies using only CPU and RAM.</li>
<li>Ollama is quite docker-like, and for me it feels intuitive. You pull models then run them. The <a href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md" target="_blank" rel="noopener noreferrer">Modelfile</a>, the "blueprint to create and share models with Ollama", is also quite dockerfile-like.</li>
</ol>
<p>Overall I find Ollama quite easy to use and would likely continue to use it for something quick. It would be pretty fun if <a href="https://github.com/jmorganca/ollama/issues/142" target="_blank" rel="noopener noreferrer">conversation history can be persisted</a>!</p>]]></content:encoded>
            <category>ollama</category>
            <category>llama</category>
            <category>chat</category>
            <category>AI</category>
            <category>LLM</category>
            <category>ML</category>
            <category>chatbot</category>
            <category>local</category>
        </item>
    </channel>
</rss>