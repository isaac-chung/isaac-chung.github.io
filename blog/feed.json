{
    "version": "https://jsonfeed.org/version/1",
    "title": "Isaac Chung Blog",
    "home_page_url": "https://isaac-chung.github.io/blog",
    "description": "Isaac Chung Blog",
    "items": [
        {
            "id": "https://isaac-chung.github.io/blog/mmteb",
            "content_html": "<p>Embeddings power many AI applications we interact with — search engines, RAG systems — but how do we know if they’re actually any good? Existing benchmarks tend to focus on a narrow set of tasks, often evaluating models in isolation without considering real-world, multilingual challenges. This can make it tough to figure out which models are truly effective, and where they might fall short. That's why we need a more comprehensive way to evaluate embeddings - one that takes into account the messy, multilingual nature of real-world language use. MMTEB is designed to fill this gap, providing a broad and diverse set of evaluation tasks that can help us better understand what works, and what doesn't, in the world of embeddings.</p>\n<div class=\"theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success\"><div class=\"admonitionHeading_Gvgb\"><span class=\"admonitionIcon_Rf37\"><svg viewBox=\"0 0 12 16\"><path fill-rule=\"evenodd\" d=\"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z\"></path></svg></span>Key questions I'll address are:</div><div class=\"admonitionContent_BuS1\"><ul>\n<li>What is MMTEB?</li>\n<li>What are the key takeaways from MMTEB?</li>\n<li>How can I use MMTEB?</li>\n</ul></div></div>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"what-is-mmteb\">What is MMTEB?<a href=\"https://isaac-chung.github.io/blog/mmteb#what-is-mmteb\" class=\"hash-link\" aria-label=\"Direct link to What is MMTEB?\" title=\"Direct link to What is MMTEB?\">​</a></h2>\n<p>The <a href=\"https://arxiv.org/abs/2502.13595\" target=\"_blank\" rel=\"noopener noreferrer\">Massive Multilingual Text Embedding Benchmark (MMTEB)</a> is an extension to the existing Massive Text Embedding Benchmark <a href=\"https://arxiv.org/abs/2210.07316\" target=\"_blank\" rel=\"noopener noreferrer\">MTEB</a>, a comprehensive evaluation framework for assessing the performance of text embedding models. MTEB was introduced in 2022 as a way to evaluate embeddings models in a single, unified benchmark. The <a href=\"https://huggingface.co/spaces/mteb/leaderboard\" target=\"_blank\" rel=\"noopener noreferrer\">MTEB leaderboard</a> has since been a popular destination for many to gauge embedding model performance using the average score across all tasks. MMTEB takes this a step further by covering over 500 quality-controlled evaluation tasks across 250+ languages, making it the largest multilingual collection of evaluation tasks for embedding models to date.</p>\n<figure style=\"border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right\" id=\"figure-gallery\"><a href=\"https://isaac-chung.github.io/assets/images/leaderboard-2057b60390daa806d0f81bd5555c88c2.png\" data-pswp-width=\"0\" data-pswp-height=\"0\"><img src=\"https://isaac-chung.github.io/assets/images/leaderboard-2057b60390daa806d0f81bd5555c88c2.png\" alt=\"The MMTEB Leaderboard\" style=\"max-width:100%;height:auto\"></a><hr style=\"margin:5px 0;background-color:rgba(0, 0, 0, .2)\"><figcaption style=\"margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em\">The MMTEB Leaderboard. https://huggingface.co/spaces/mteb/leaderboard</figcaption></figure>\n<p>MMTEB introduces a diverse set of challenging, novel tasks that test the capabilities of embedding models in real-world scenarios. These tasks include:</p>\n<ul>\n<li>Instruction following: This task evaluates a model's ability to understand and execute instructions, such as answering questions or generating text based on a given prompt.</li>\n<li>Long-document retrieval: This task assesses a model's ability to retrieve relevant information from long documents, such as articles, reports, or books.</li>\n<li>Code retrieval: This task tests a model's ability to retrieve relevant code snippets or software documentation based on a given query or prompt.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"what-are-the-key-takeaways-from-mmteb\">What are the key takeaways from MMTEB?<a href=\"https://isaac-chung.github.io/blog/mmteb#what-are-the-key-takeaways-from-mmteb\" class=\"hash-link\" aria-label=\"Direct link to What are the key takeaways from MMTEB?\" title=\"Direct link to What are the key takeaways from MMTEB?\">​</a></h2>\n<p><strong>Performance Varies Widely Across Languages</strong> – MMTEB highlights that models like <code>e5-large-v2</code> and <code>all-mpnet-base</code> perform well in English but see sharp performance drops in low-resource languages. Some models even fail completely on certain languages, revealing critical gaps in multilingual generalization.</p>\n<p><strong>Generalization is a Challenge</strong> – MMTEB introduces new task types like long-document retrieval and instruction following, where many leading models struggle. E.g. embedding models trained primarily on short text often fail to retrieve relevant long documents, demonstrating their limited adaptability to new task formats.</p>\n<p><strong>Bigger Isn’t Always Better</strong> – The <code>multilingual-e5-large-instruct</code> model outperforms Mistral-based models in several benchmarks like MTEB(Europe), despite the latter being a larger model. This suggests that <em>smaller, well-optimized multilingual models can generalize better across languages</em> than larger models that are not explicitly fine-tuned for diverse multilingual tasks.</p>\n<p>The bottom line is that: as our world becomes more connected, ML systems should be expected to maintain performance when the language is no longer English. MMTEB aims to provide a way for anyone to evaluate embeddings on more diverse tasks from a wider range of languages, while limiting the compute resources required.</p>\n<p>So what does it mean for you?</p>\n<p><em><strong>For practitioners:</strong></em> This shifts how we should think about embedding models. Instead of defaulting to the latest or most hyped model, it’s now possible to select embeddings based on real, task-specific performance. If you're building a semantic search system, MMTEB provides insights into which models excel at retrieval and reranking. If you’re working with multilingual data, you can see which embeddings actually handle cross-lingual retrieval. This kind of benchmarking isn’t just useful — it’s essential for making informed choices in production AI systems.</p>\n<p>This also has implications for businesses adopting AI-powered solutions. Many companies rely on third-party embedding models for recommendations, knowledge retrieval, or search, but without proper benchmarking, it's easy to pick a model that underperforms in critical areas. MMTEB provides a clear, objective way to evaluate models before committing to them, reducing the risk of investing in suboptimal solutions. Whether you're fine-tuning an open-source model or evaluating commercial offerings, this benchmark helps cut through the noise and focus on what actually works.</p>\n<p><em><strong>For researchers:</strong></em> MMTEB sets a new standard for embedding evaluation. Instead of optimizing for isolated benchmarks, model development can now be guided by a diverse set of real-world tasks, encouraging progress in areas where embeddings still fall short. It also opens up new questions: How can we build embeddings that perform well across languages without sacrificing efficiency? What trade-offs exist between generalization and specialization? As more models are tested against MMTEB, we’ll gain a clearer picture of what’s working, what’s not, and where the field needs to go next.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"how-can-i-use-mmteb\">How can I use MMTEB?<a href=\"https://isaac-chung.github.io/blog/mmteb#how-can-i-use-mmteb\" class=\"hash-link\" aria-label=\"Direct link to How can I use MMTEB?\" title=\"Direct link to How can I use MMTEB?\">​</a></h2>\n<p>MMTEB is available in the <a href=\"https://github.com/embeddings-benchmark/mteb\" target=\"_blank\" rel=\"noopener noreferrer\">MTEB repository on GitHub</a> and provides a structured way to evaluate embedding models across a wide range of multilingual tasks.</p>\n<p>To get started with MMTEB, you can install the <code>mteb</code> library using pip:</p>\n<div class=\"codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#393A34;--prism-background-color:#f6f8fa\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" style=\"color:#393A34;background-color:#f6f8fa\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">pip install mteb</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button type=\"button\" aria-label=\"Copy code to clipboard\" title=\"Copy\" class=\"clean-btn\"><span class=\"copyButtonIcons_eSgA\" aria-hidden=\"true\"><svg viewBox=\"0 0 24 24\" class=\"copyButtonIcon_y97N\"><path fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"></path></svg><svg viewBox=\"0 0 24 24\" class=\"copyButtonSuccessIcon_LjdS\"><path fill=\"currentColor\" d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\"></path></svg></span></button></div></div></div>\n<p>Once installed, you can use either the command line or a Python script to do any of the following:</p>\n<p><strong>Benchmark Your Model</strong>:\nRun your embedding model against MMTEB’s 500+ evaluation tasks (and any predefined benchmarks) to measure its performance across different languages and domains. E.g. to run the new English benchmark (which uses 2% of the original number of documents while maintaining rankings)</p>\n<div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#393A34;--prism-background-color:#f6f8fa\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" style=\"color:#393A34;background-color:#f6f8fa\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token keyword\" style=\"color:#00009f\">import</span><span class=\"token plain\"> mteb</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\"></span><span class=\"token comment\" style=\"color:#999988;font-style:italic\"># Load a pre-trained model. The model weights will be downloaded. </span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">model </span><span class=\"token operator\" style=\"color:#393A34\">=</span><span class=\"token plain\"> mteb</span><span class=\"token punctuation\" style=\"color:#393A34\">.</span><span class=\"token plain\">get_model</span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token string\" style=\"color:#e3116c\">\"intfloat/multilingual-e5-small\"</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\"></span><span class=\"token comment\" style=\"color:#999988;font-style:italic\"># Load a benchmark or tasks.</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">benchmark </span><span class=\"token operator\" style=\"color:#393A34\">=</span><span class=\"token plain\"> mteb</span><span class=\"token punctuation\" style=\"color:#393A34\">.</span><span class=\"token plain\">get_benchmark</span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token string\" style=\"color:#e3116c\">\"MTEB(eng, v2)\"</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\"></span><span class=\"token comment\" style=\"color:#999988;font-style:italic\"># Create an evaluation object</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">evaluation </span><span class=\"token operator\" style=\"color:#393A34\">=</span><span class=\"token plain\"> mteb</span><span class=\"token punctuation\" style=\"color:#393A34\">.</span><span class=\"token plain\">MTEB</span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token plain\">tasks</span><span class=\"token operator\" style=\"color:#393A34\">=</span><span class=\"token plain\">benchmark</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\"></span><span class=\"token comment\" style=\"color:#999988;font-style:italic\"># Run evaluation. The datasets will be downloaded at this step.</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">results </span><span class=\"token operator\" style=\"color:#393A34\">=</span><span class=\"token plain\"> evaluation</span><span class=\"token punctuation\" style=\"color:#393A34\">.</span><span class=\"token plain\">run</span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token plain\">model</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button type=\"button\" aria-label=\"Copy code to clipboard\" title=\"Copy\" class=\"clean-btn\"><span class=\"copyButtonIcons_eSgA\" aria-hidden=\"true\"><svg viewBox=\"0 0 24 24\" class=\"copyButtonIcon_y97N\"><path fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"></path></svg><svg viewBox=\"0 0 24 24\" class=\"copyButtonSuccessIcon_LjdS\"><path fill=\"currentColor\" d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\"></path></svg></span></button></div></div></div>\n<p>You can also task-specific scores to see where your model excels (e.g. retrieval, classification) and where it struggles (e.g., low-resource languages, long-document retrieval). For more details, please refer the repository's <a href=\"https://github.com/embeddings-benchmark/mteb\" target=\"_blank\" rel=\"noopener noreferrer\">README</a> file.</p>\n<p><strong>Compare Against State-of-the-Art</strong>:\nThe MMTEB leaderboard lets you see how your model stacks up against top-performing multilingual embeddings. The MMTEB leaderboard ranks models using the Borda count method, a voting system that assigns points to each model based on its performance across all evaluation tasks. This way, the top-performing model receives the most points, the second-best model receives fewer points, and so on. The model with the highest total score across all tasks is ranked highest on the leaderboard. The Borda count method is used to provide a more nuanced ranking that takes into account a model's performance across multiple tasks, rather than just its average score.</p>\n<p>The \"Zero-Shot\" column on the MMTEB leaderboard indicates the percentage of evaluation datasets that did not have their training split used to train the model. In other words, it measures how well a model performs on tasks it has not seen during training, without any fine-tuning or additional training data. A higher \"Zero-Shot\" score means that the model is more capable of generalizing to new, unseen tasks, which is a key aspect of real-world applicability. This metric is important because it helps evaluate a model's ability to adapt to new scenarios and tasks, rather than just memorizing patterns in the training data.</p>\n<p>So what's next? Try out MMTEB and see how your models stack up. If you hit any roadblocks, open an <a href=\"https://github.com/embeddings-benchmark/mteb/issues\" target=\"_blank\" rel=\"noopener noreferrer\">issue</a>. And if you're able, consider contributing to help us make MMTEB even better.</p>",
            "url": "https://isaac-chung.github.io/blog/mmteb",
            "title": "MMTEB Massive Multilingual Text Embedding Benchmark",
            "summary": "Embeddings power many AI applications we interact with — search engines, RAG systems — but how do we know if they’re actually any good? Existing benchmarks tend to focus on a narrow set of tasks, often evaluating models in isolation without considering real-world, multilingual challenges. This can make it tough to figure out which models are truly effective, and where they might fall short. That's why we need a more comprehensive way to evaluate embeddings - one that takes into account the messy, multilingual nature of real-world language use. MMTEB is designed to fill this gap, providing a broad and diverse set of evaluation tasks that can help us better understand what works, and what doesn't, in the world of embeddings.",
            "date_modified": "2025-03-09T00:00:00.000Z",
            "author": {
                "name": "Isaac Chung",
                "url": "https://isaac-chung.github.io"
            },
            "tags": [
                "mmteb",
                "mteb",
                "embedding",
                "benchmark"
            ]
        },
        {
            "id": "https://isaac-chung.github.io/blog/maintain-focus",
            "content_html": "<p>AI is a fast moving field. This applies for both products and academia. There are new papers on NLP/LLMs/CV/ML coming out almost every day, and there are no shortage of new products and companies popping up on our social feeds. How do we know if our time is well spent focusing on one topic? Would it be better to \"keep up with the Joneses\" and explore every new development as they come?</p>\n<p>This blog isn't technical. Instead, I aim to start a conversation around maintaining focus.</p>\n<div class=\"theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success\"><div class=\"admonitionHeading_Gvgb\"><span class=\"admonitionIcon_Rf37\"><svg viewBox=\"0 0 12 16\"><path fill-rule=\"evenodd\" d=\"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z\"></path></svg></span>Key questions I'll address are:</div><div class=\"admonitionContent_BuS1\"><ul>\n<li>Should we chase every new AI innovation?</li>\n<li>Without focus, what are the chances of our research succeeding in the long term?</li>\n<li>How do we balance research and commercial needs?</li>\n</ul></div></div>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"should-we-chase-every-new-ai-innovation\">Should we chase every new AI innovation?<a href=\"https://isaac-chung.github.io/blog/maintain-focus#should-we-chase-every-new-ai-innovation\" class=\"hash-link\" aria-label=\"Direct link to Should we chase every new AI innovation?\" title=\"Direct link to Should we chase every new AI innovation?\">​</a></h2>\n<p>I can imagine that many of you are also struggling with this question. Keeping up with every new development demands a lot of time and resources, while ignoring them might leave us behind, making our knowledge obsolete.  However, having a dedicated focus makes it easy to 1) dive deeper into the topic and 2) limit the need to stay current on everything. This allows for deeper progress in select areas but might mean we don't stay current with every new trend. By chasing the latest trends, we risk spreading ourselves too thin and losing focus.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"without-focus-what-are-the-chances-of-our-research-succeeding-in-the-long-term\">Without focus, what are the chances of our research succeeding in the long term?<a href=\"https://isaac-chung.github.io/blog/maintain-focus#without-focus-what-are-the-chances-of-our-research-succeeding-in-the-long-term\" class=\"hash-link\" aria-label=\"Direct link to Without focus, what are the chances of our research succeeding in the long term?\" title=\"Direct link to Without focus, what are the chances of our research succeeding in the long term?\">​</a></h2>\n<p>ML research teams usually have fairly long term focuses, which are already aligned with clearly defined commercial needs. But that's not always the case. Teams often are required to manage demands like supporting product releases and handling multiple lines of work.</p>\n<p>But what's the impact of a lack of focus in research?</p>\n<ol>\n<li>\n<p>Loss of continuity: Regularly switching focus means you might not spend enough time on one topic to make substantial progress. You might not complete you work, or have time to develop as deep an understanding as you'd like. Alternatively, you might keep returning to the work and have to suffer through repeated starting phases, which are often less productive than the middle and end stages of research.</p>\n</li>\n<li>\n<p>Resource dilution: Dividing your limited time and cognitive resources across multiple topics prevents deep dives into any one area. Shallow work in multiple areas is less likely to yield significant breakthroughs compared to sustained deep work in one area.</p>\n</li>\n<li>\n<p>Goal fragmentation: Achieving long-term goals requires sustained effort and clear, consistent objectives. Frequent changes can fragment your goals and dilute the clarity of your research path.</p>\n</li>\n<li>\n<p>Building on results: Research often builds incrementally, where each phase relies on the outcomes of the previous phase. Without cumulative progress, your research might lack the depth and evolution necessary for significant discoveries.</p>\n</li>\n<li>\n<p>Team dynamics: Consistent focus areas facilitate better collaboration and synergy within your team, while frequent changes can be confusing and demotivating. This can lead to a loss in productivity and innovation.</p>\n</li>\n</ol>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"how-do-we-balance-research-and-business-needs\">How do we balance research and business needs?<a href=\"https://isaac-chung.github.io/blog/maintain-focus#how-do-we-balance-research-and-business-needs\" class=\"hash-link\" aria-label=\"Direct link to How do we balance research and business needs?\" title=\"Direct link to How do we balance research and business needs?\">​</a></h2>\n<p>Business needs could be a product release or a service delivery, whereas research needs could be to investigate emerging technologies and techniques that may not have immediate commercial applications.</p>\n<p>To balance these two needs we could:</p>\n<ol>\n<li>\n<p>Set clear, multi-purpose goals: Try to align our research objectives with commercial goals where possible. Identify areas where advancing academic knowledge can directly contribute to product improvements or innovations.</p>\n</li>\n<li>\n<p>Leverage external resources: Collaborate with academic institutions to stay on top of research while sharing the burden of exploratory work. Use conferences, publications, and peer reviews to gain feedback on our progress.</p>\n</li>\n<li>\n<p>Secure support and resources: Ensure that stakeholders understand the value of long-term research and support us with appropriate resources and funding. We want to avoid research projects being overshadowed by immediate commercial demands.</p>\n</li>\n<li>\n<p>Understand core competencies: Play to our strengths and understand when to outsource and find ways to compliment the teams' weaknesses.</p>\n</li>\n</ol>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"speed-vs-quality\">Speed vs quality<a href=\"https://isaac-chung.github.io/blog/maintain-focus#speed-vs-quality\" class=\"hash-link\" aria-label=\"Direct link to Speed vs quality\" title=\"Direct link to Speed vs quality\">​</a></h3>\n<p>Balance requires tradeoffs. While some may prefer to work quickly and chase multiple goals simultaneously, it's crucial to balance urgency with the need for quality and thoroughness in both research and product development. Rushed commercialization can lead to:</p>\n<ul>\n<li>Incomplete solutions: Products that fail to fully address the intended problem or meet user needs.</li>\n<li>Technical debt: Issues that accumulate over time, requiring significant resources to resolve later.</li>\n<li>Brand reputation: Potential harm to the company’s reputation if the product performs poorly.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"conclusion\">Conclusion<a href=\"https://isaac-chung.github.io/blog/maintain-focus#conclusion\" class=\"hash-link\" aria-label=\"Direct link to Conclusion\" title=\"Direct link to Conclusion\">​</a></h2>\n<p>The decision to chase every new innovation or maintain focused efforts is a constant dilemma.\nUltimately, finding this equilibrium is crucial to ensure that we can drive meaningful progress while escaping the pitfalls of rushed commercialization. Let me know in the comments what you think and your experience with keeping on top of the latest developments.</p>",
            "url": "https://isaac-chung.github.io/blog/maintain-focus",
            "title": "Keeping up with the Joneses? Maintaining focus in AI",
            "summary": "AI is a fast moving field. This applies for both products and academia. There are new papers on NLP/LLMs/CV/ML coming out almost every day, and there are no shortage of new products and companies popping up on our social feeds. How do we know if our time is well spent focusing on one topic? Would it be better to \"keep up with the Joneses\" and explore every new development as they come?",
            "date_modified": "2024-05-25T00:00:00.000Z",
            "author": {
                "name": "Isaac Chung",
                "url": "https://isaac-chung.github.io"
            },
            "tags": []
        },
        {
            "id": "https://isaac-chung.github.io/blog/concurrent-requests",
            "content_html": "<p>Being able to serve concurrent LLM generation requests are crucial to production LLM applications that have multiple users. <a href=\"https://pycon.lt/2024/talks/DHBLXW\" target=\"_blank\" rel=\"noopener noreferrer\">I recently gave a talk at PyCon Lithuania</a> on serving quantized LLMs with <a href=\"https://github.com/abetlen/llama-cpp-python\" target=\"_blank\" rel=\"noopener noreferrer\">llama-cpp-python</a>, an open source python library that helps serve quantized models in the GGUF format. At the end, a question came from the audience about supporting multiple users and concurrent requests. I decided to take a deeper look into why the library wasn't able to support that at the time.</p>\n<div class=\"theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success\"><div class=\"admonitionHeading_Gvgb\"><span class=\"admonitionIcon_Rf37\"><svg viewBox=\"0 0 12 16\"><path fill-rule=\"evenodd\" d=\"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z\"></path></svg></span>Key questions I'll address are:</div><div class=\"admonitionContent_BuS1\"><ul>\n<li>What are the challenges of serving concurrent requests with LLMs?</li>\n<li>How to serve concurrent requests with quantized LLMs?</li>\n</ul></div></div>\n<figure style=\"border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right\" id=\"figure-gallery\"><a href=\"https://isaac-chung.github.io/assets/images/octopus-69454e9dcdc193dfb7a6f3ba4040312b.png\" data-pswp-width=\"0\" data-pswp-height=\"0\"><img src=\"https://isaac-chung.github.io/assets/images/octopus-69454e9dcdc193dfb7a6f3ba4040312b.png\" alt=\"New yorker style comic depicting a cute, friendly cartoon octopus with a baseball cap holding multiple tennis rackets. White background only.\" style=\"max-width:100%;height:auto\"></a><hr style=\"margin:5px 0;background-color:rgba(0, 0, 0, .2)\"><figcaption style=\"margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em\">Image by Dalle3.</figcaption></figure>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"what-are-the-challenges-of-serving-concurrent-requests-with-llms\">What are the challenges of serving concurrent requests with LLMs?<a href=\"https://isaac-chung.github.io/blog/concurrent-requests#what-are-the-challenges-of-serving-concurrent-requests-with-llms\" class=\"hash-link\" aria-label=\"Direct link to What are the challenges of serving concurrent requests with LLMs?\" title=\"Direct link to What are the challenges of serving concurrent requests with LLMs?\">​</a></h2>\n<p>LLM inference involves generating tokens in an autoregressive manner. To avoid repeating calculations when generating future tokens, a KV cache is used.\nThe KV cache size grows quickly with the number of requests. The <a href=\"https://arxiv.org/abs/2309.06180\" target=\"_blank\" rel=\"noopener noreferrer\">vLLM paper</a> uses the 13B OPT model as an example:</p>\n<blockquote>\n<p>the KV cache of a single token demands 800 KB of space, calculated as 2 (key and value vectors) × 5120 (hidden state size) × 40 (number of layers) × 2 (bytes per FP16). Since OPT can generate sequences up to 2048 tokens, the memory required to store the KV cache of one request can be as much as 1.6 GB</p>\n</blockquote>\n<p>It's clear that this is a memory-bound process. In the context of serving requests, we can batch multiple incoming requests to improve compute utilization. Batching isn't trivial either. The main challenges includes:</p>\n<ol>\n<li>The requests may arrive at different times, and</li>\n<li>the requests may have very different input and output lengths.</li>\n</ol>\n<p>Batching requests naively may lead to huge delays from waiting for earlier requests to finish before starting the next batch, or waiting for the longest generation to finish. It would also lead to computation and memory wastage from padding inputs/outputs due to their difference in lengths.</p>\n<p>Back to the question from the talk. At the time of writing, llama-cpp-python did not support batched requests. Moreover, concurrent requests would <a href=\"https://www.reddit.com/r/LocalLLaMA/comments/15kbbna/how_to_make_multiple_inference_requests_from_a/\" target=\"_blank\" rel=\"noopener noreferrer\">lead to the server crashing</a>. This might be due to the locks being used to control access to shared resources, particularly the <code>llama_proxy</code> variable, which handles the model resources. This means that to serve parallel requests, multiple instances of the models may be needed, and this drastically increases the resources required for model serving.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"how-to-serve-concurrent-requests-with-quantized-llms\">How to serve concurrent requests with quantized LLMs?<a href=\"https://isaac-chung.github.io/blog/concurrent-requests#how-to-serve-concurrent-requests-with-quantized-llms\" class=\"hash-link\" aria-label=\"Direct link to How to serve concurrent requests with quantized LLMs?\" title=\"Direct link to How to serve concurrent requests with quantized LLMs?\">​</a></h2>\n<p>Improvements have been introduced to make serving concurrent requests more efficient. These include and are not limited to:</p>\n<ol>\n<li><strong>Continuous Batching</strong>: It allows new requests to join the current batch in the next decoder cycle instead of waiting for the end of the current batch to finish. This improves throughput and compute utilization. The upstream <a href=\"https://github.com/ggerganov/llama.cpp/tree/master/examples/server\" target=\"_blank\" rel=\"noopener noreferrer\">llama.cpp repo</a> has the capability to serve parallel requests with continuous batching. This, however, does not entirely solve the memory issue. We still need to reserve memory using the longest sequence in the batch.</li>\n<li><strong>Paged Attention</strong>: It divides up the KV cache into blocks that would contain keys and values for a fixed number of tokens. It eliminates external fragmentation (unusable gaps between allocated memory blocks in a GPU's memory) since all blocks have the same size. Also, it eases internal fragmentation by using relatively small blocks. An LLM serving engine that's built on top of Paged Attention is <a href=\"https://github.com/vllm-project/vllm\" target=\"_blank\" rel=\"noopener noreferrer\">vLLM</a>.</li>\n</ol>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"what-is-vllm\">What is vLLM?<a href=\"https://isaac-chung.github.io/blog/concurrent-requests#what-is-vllm\" class=\"hash-link\" aria-label=\"Direct link to What is vLLM?\" title=\"Direct link to What is vLLM?\">​</a></h2>\n<p>vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. vLLM supports serving quantized LLMs with concurrent requests with AWQ (4-bit) models, which boasts a 2.78x speed up on a single GPU according to <a href=\"https://lightning.ai/lightning-ai/studios/optimized-llm-inference-api-for-mistral-7b-using-vllm\" target=\"_blank\" rel=\"noopener noreferrer\">a benchmark done by lightning.ai</a>.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"final-thoughts\">Final Thoughts<a href=\"https://isaac-chung.github.io/blog/concurrent-requests#final-thoughts\" class=\"hash-link\" aria-label=\"Direct link to Final Thoughts\" title=\"Direct link to Final Thoughts\">​</a></h2>\n<p>Tools like llama-cpp-python and ExLlama2 may not have been designed to be <a href=\"https://github.com/turboderp/exllamav2/issues/95#issuecomment-2019991153\" target=\"_blank\" rel=\"noopener noreferrer\">an efficient backend for large deployments</a>, but they are certainly worth a look for anyone who wants to deploy way smaller models on a budget. vLLM on the other hand seem to have that in mind from the beginning. It can even be used with the <a href=\"https://github.com/triton-inference-server/vllm_backend\" target=\"_blank\" rel=\"noopener noreferrer\">triton inference server as a supported backend</a>.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"further-reading\">Further Reading<a href=\"https://isaac-chung.github.io/blog/concurrent-requests#further-reading\" class=\"hash-link\" aria-label=\"Direct link to Further Reading\" title=\"Direct link to Further Reading\">​</a></h2>\n<ul>\n<li>Check out <a href=\"https://www.youtube.com/channel/UCtAcpQcYerN8xxZJYTfWBMw/videos\" target=\"_blank\" rel=\"noopener noreferrer\">this YouTube channel</a> for in-depth explanations about popular transformer/LLM architectures. It helped me get through some dense materials clearly, so thank you, Umar!</li>\n<li>This blog on <a href=\"https://www.run.ai/blog/serving-large-language-models\" target=\"_blank\" rel=\"noopener noreferrer\">Serving LLMs by run.ai</a>.</li>\n</ul>",
            "url": "https://isaac-chung.github.io/blog/concurrent-requests",
            "title": "Serving Concurrent Requests with Quantized LLMs",
            "summary": "Being able to serve concurrent LLM generation requests are crucial to production LLM applications that have multiple users. I recently gave a talk at PyCon Lithuania on serving quantized LLMs with llama-cpp-python, an open source python library that helps serve quantized models in the GGUF format. At the end, a question came from the audience about supporting multiple users and concurrent requests. I decided to take a deeper look into why the library wasn't able to support that at the time.",
            "date_modified": "2024-04-13T00:00:00.000Z",
            "author": {
                "name": "Isaac Chung",
                "url": "https://isaac-chung.github.io"
            },
            "tags": [
                "LLM",
                "concurrent",
                "requests",
                "parallel",
                "multi-user"
            ]
        },
        {
            "id": "https://isaac-chung.github.io/blog/rag-eval-and-observability",
            "content_html": "<p>We know that building a Retrieval Augmented Generation (RAG) proof of concept is easy, but making it production-ready can be hard. There are no shortage of tips and tricks out there for us to try, but at the end of the day, it all depends on our data and our application. Transitioning RAG into production follows similar principles to other production systems. Scaling up to handle more data and users, smooth error/exception handling, and getting it to play nice with other systems are some of the main challenges to tackle. How can we really know if our RAG system is working well? and how well? To find out, we should take a look at each component under the hood and be able to evaluate the pipeline with clear metrics.</p>\n<div class=\"theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success\"><div class=\"admonitionHeading_Gvgb\"><span class=\"admonitionIcon_Rf37\"><svg viewBox=\"0 0 12 16\"><path fill-rule=\"evenodd\" d=\"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z\"></path></svg></span>Key questions I'll address are:</div><div class=\"admonitionContent_BuS1\"><ul>\n<li>How to look under the hood in a RAG system?</li>\n<li>How to evaluate RAG systems?</li>\n</ul></div></div>\n<figure style=\"border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right\" id=\"figure-gallery\"><a href=\"https://isaac-chung.github.io/assets/images/books-and-robot-1c8bc8b9accc3ef2e643d2d0e5e005da.png\" data-pswp-width=\"0\" data-pswp-height=\"0\"><img src=\"https://isaac-chung.github.io/assets/images/books-and-robot-1c8bc8b9accc3ef2e643d2d0e5e005da.png\" alt=\"A robot sitting on a large pile of books.\" style=\"max-width:100%;height:auto\"></a><hr style=\"margin:5px 0;background-color:rgba(0, 0, 0, .2)\"><figcaption style=\"margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em\">Image by Stable Diffusion XL.</figcaption></figure>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"how-to-look-under-the-hood-in-a-rag-system\">How to look under the hood in a RAG system?<a href=\"https://isaac-chung.github.io/blog/rag-eval-and-observability#how-to-look-under-the-hood-in-a-rag-system\" class=\"hash-link\" aria-label=\"Direct link to How to look under the hood in a RAG system?\" title=\"Direct link to How to look under the hood in a RAG system?\">​</a></h2>\n<p>Once the components are set up in the RAG system, it is tempting to spot-check it for performance, and try out some <em>advanced techniques</em> with the promise of performance improvements. However, this isn't the most reliable nor structural approach to debugging and improving RAG. The first thing we should do after getting our first end-to-end RAG response is adding observability. This greatly helps us not only during the transition of our RAG system from POC to production but also in its post-launch maintenance phase.</p>\n<p>Observability is crucial in RAG production systems for several main reasons:</p>\n<ol>\n<li><strong>Detecting Issues</strong>: Observability allows for the detection of issues and anomalies within a system. By monitoring various metrics, logs, and traces, we can quickly identify when something goes wrong and take appropriate action to resolve the issue before it impacts users.</li>\n<li><strong>Root Cause Analysis</strong>: When problems occur, especially during the development phase, observability enables us to perform root cause analysis efficiently. By examining the data collected from various components, we can trace back the source of the problem and address it effectively. More important, in production this would help reduce downtime and minimizing the impact on users.</li>\n<li><strong>Performance Optimization</strong>: Observability provides insights into the performance of the system. By monitoring metrics such as response times, throughput, and resource utilization, we can identify bottlenecks and areas for optimization, leading to better overall performance and user experience.</li>\n</ol>\n<p>This could be as simple as logging inputs and outputs of each component (e.g. simple setting in <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/observability/#simple-llm-inputsoutputs\" target=\"_blank\" rel=\"noopener noreferrer\">llama-index</a>). There are a variety of LLM observability tools to help trace the timings and outputs at each step of a RAG system. Some of these have minimal config needed, have no pricing page, and are open source, and they are:</p>\n<ul>\n<li><a href=\"https://github.com/traceloop/openllmetry\" target=\"_blank\" rel=\"noopener noreferrer\">OpenLLMetry</a>: Built on top of OpenTelemetry. If you’re using an LLM framework like Haystack, Langchain or LlamaIndex, there is no need to add any annotations to your code.</li>\n<li><a href=\"https://github.com/Arize-ai/phoenix\" target=\"_blank\" rel=\"noopener noreferrer\">Arize Phoenix</a>: Built on top of the OpenInference tracing standard, and uses it to trace, export, and collect critical information about your LLM Application in the form of \"spans\". It also supports several RAG-related analyses and visualizations.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"how-to-evaluate-rag-systems\">How to evaluate RAG systems?<a href=\"https://isaac-chung.github.io/blog/rag-eval-and-observability#how-to-evaluate-rag-systems\" class=\"hash-link\" aria-label=\"Direct link to How to evaluate RAG systems?\" title=\"Direct link to How to evaluate RAG systems?\">​</a></h2>\n<p>Just like any system, it is important to understand how well the RAG system is performing and how much improvement has been achieved over the baseline. This doesn’t just involve measuring how fast and how much it costs, but also how good the outputs are. We could take a look at RAG-specific evaluation methods. Per-component evaluations, like unit tests, can be done on the retrieval stage and the generation stage separately.</p>\n<p>For retrieval, the goal is to find out given the configuration how well can the system retrieve relevant results? Here you would need a golden set of queries and ground truth of relevant documents (or their IDs).  You could use IR metrics like nDCG or Mean Reciprocal Rank (MRR), but for RAG it’s more meaningful to understand 1) the signal to noise ratio of the retrieved context (context precision) and 2) how well it can retrieve all the relevant information required to answer the question (context recall).</p>\n<p>For generation, the goal is to find out, given the relevant documents in the context, 1) how factually accurate is the generated answer (faithfulness), and 2) how relevant is the generated answer to the question (answer relevancy). It is also important to evaluate the full pipeline end to end. This might involve some manual efforts to start with or asking an LLM to verify whether the answer is correct. A proxy for gauging how close the generated answer to the ground truth answer could be semantic similarity.</p>\n<p>Some open source RAG evaluation tools like <a href=\"https://github.com/explodinggradients/ragas\" target=\"_blank\" rel=\"noopener noreferrer\">Ragas</a> offer readily available guide to evaluate your RAG system with predefined metrics and iterate your RAG system with user feedback in production. Ragas, in particular, offers the ability generate a synthetic test set for “reference-free” evaluation, which means that instead of relying on human-annotated test set, Ragas leverages LLMs under the hood to conduct the evaluations.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"the-bottom-line\">The Bottom Line<a href=\"https://isaac-chung.github.io/blog/rag-eval-and-observability#the-bottom-line\" class=\"hash-link\" aria-label=\"Direct link to The Bottom Line\" title=\"Direct link to The Bottom Line\">​</a></h2>\n<p>Fight the urge of treating the RAG system as a black box. Use a structured approach to evaluate your RAG system in terms of performance and other requirements like latency by adding observability and using evaluation tools.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"further-reading\">Further Reading<a href=\"https://isaac-chung.github.io/blog/rag-eval-and-observability#further-reading\" class=\"hash-link\" aria-label=\"Direct link to Further Reading\" title=\"Direct link to Further Reading\">​</a></h2>\n<ul>\n<li>Catch this talk on <a href=\"https://pycon.lt/2024/talks/HFXHRV\" target=\"_blank\" rel=\"noopener noreferrer\">\"Transcend the Knowledge Barriers in RAG\"</a> at PyCon Lithuania to understand how each RAG component works under the hood.</li>\n<li><a href=\"https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c\" target=\"_blank\" rel=\"noopener noreferrer\">12 RAG Pain points</a></li>\n</ul>",
            "url": "https://isaac-chung.github.io/blog/rag-eval-and-observability",
            "title": "How to really know if your RAG system is working well.",
            "summary": "We know that building a Retrieval Augmented Generation (RAG) proof of concept is easy, but making it production-ready can be hard. There are no shortage of tips and tricks out there for us to try, but at the end of the day, it all depends on our data and our application. Transitioning RAG into production follows similar principles to other production systems. Scaling up to handle more data and users, smooth error/exception handling, and getting it to play nice with other systems are some of the main challenges to tackle. How can we really know if our RAG system is working well? and how well? To find out, we should take a look at each component under the hood and be able to evaluate the pipeline with clear metrics.",
            "date_modified": "2024-03-24T00:00:00.000Z",
            "author": {
                "name": "Isaac Chung",
                "url": "https://isaac-chung.github.io"
            },
            "tags": [
                "RAG",
                "LLM",
                "Evaluation",
                "Tracing",
                "Logging",
                "Observability"
            ]
        },
        {
            "id": "https://isaac-chung.github.io/blog/llm-serving",
            "content_html": "<p>How do you measure the performance of LLM serving systems? Production services in engineering are often evaluated using metrics like Requests Per Second (RPS), uptime, and latency. In computer vision systems, Frames Per Second (FPS) is often used as the main model throughput metric for use cases that involve near-real-time detection and tracking. Does serving LLMs have something similar? Certainly. A recent conversation with my team (after they read my <a href=\"https://isaac-chung.github.io/blog/what-is-ollama\">Ollama blog</a>) got me thinking about additional metrics that we could be tracking.</p>\n<div class=\"theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success\"><div class=\"admonitionHeading_Gvgb\"><span class=\"admonitionIcon_Rf37\"><svg viewBox=\"0 0 12 16\"><path fill-rule=\"evenodd\" d=\"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z\"></path></svg></span>Key questions I'll address are:</div><div class=\"admonitionContent_BuS1\"><ul>\n<li>What metrics does Ollama provide?</li>\n<li>Why can't I just use tokens per second?</li>\n<li>What other LLM serving metrics should I consider?</li>\n</ul></div></div>\n<figure style=\"border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right\" id=\"figure-gallery\"><a href=\"https://isaac-chung.github.io/assets/images/coach-19431ad943236168227c2f70e17605bd.jpg\" data-pswp-width=\"0\" data-pswp-height=\"0\"><img src=\"https://isaac-chung.github.io/assets/images/coach-19431ad943236168227c2f70e17605bd.jpg\" alt=\"A baseball coach blowing a whistle while holding a stopwatch.\" style=\"max-width:100%;height:auto\"></a><hr style=\"margin:5px 0;background-color:rgba(0, 0, 0, .2)\"><figcaption style=\"margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em\">Image by OpenAI DALL-E 3.</figcaption></figure>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"what-metrics-does-ollama-provide\">What metrics does Ollama provide?<a href=\"https://isaac-chung.github.io/blog/llm-serving#what-metrics-does-ollama-provide\" class=\"hash-link\" aria-label=\"Direct link to What metrics does Ollama provide?\" title=\"Direct link to What metrics does Ollama provide?\">​</a></h2>\n<p>To see the metrics for each response Ollama provides, add the <code>--verbose</code> flag after the run command. e.g. <code>ollama run llama2 --verbose</code>. Here is an example output with 8 different metrics.</p>\n<div class=\"codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#393A34;--prism-background-color:#f6f8fa\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" style=\"color:#393A34;background-color:#f6f8fa\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">total duration:       58.502942674s</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">load duration:        7.185349ms</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">prompt eval count:    31 token(s)</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">prompt eval duration: 4.044684s</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">prompt eval rate:     7.66 tokens/s</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">eval count:           266 token(s)</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">eval duration:        54.44846s</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">eval rate:            4.89 tokens/s</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button type=\"button\" aria-label=\"Copy code to clipboard\" title=\"Copy\" class=\"clean-btn\"><span class=\"copyButtonIcons_eSgA\" aria-hidden=\"true\"><svg viewBox=\"0 0 24 24\" class=\"copyButtonIcon_y97N\"><path fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"></path></svg><svg viewBox=\"0 0 24 24\" class=\"copyButtonSuccessIcon_LjdS\"><path fill=\"currentColor\" d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\"></path></svg></span></button></div></div></div>\n<p>I added some indentation and rearranged it a bit so that it's easier to discern which parts are together:</p>\n<div class=\"codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#393A34;--prism-background-color:#f6f8fa\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" style=\"color:#393A34;background-color:#f6f8fa\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">total duration:       58.502942674s</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">    1. load duration:        7.185349ms</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">    2. prompt eval duration: 4.044684s</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">        prompt eval count:    31 token(s)</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">        prompt eval rate:     7.66 tokens/s</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">    3. eval duration:        54.44846s</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">        eval count:           266 token(s)</span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">        eval rate:            4.89 tokens/s</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button type=\"button\" aria-label=\"Copy code to clipboard\" title=\"Copy\" class=\"clean-btn\"><span class=\"copyButtonIcons_eSgA\" aria-hidden=\"true\"><svg viewBox=\"0 0 24 24\" class=\"copyButtonIcon_y97N\"><path fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"></path></svg><svg viewBox=\"0 0 24 24\" class=\"copyButtonSuccessIcon_LjdS\"><path fill=\"currentColor\" d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\"></path></svg></span></button></div></div></div>\n<p>Total duration can be seen as latency (the overall time from receiving a request to returning a response to the user). This metric often includes all of the overhead in addition to the time a model needs to generate a response, e.g. model load time.</p>\n<p>Load duration is the time taken to load the model into memory. Prompt eval is the stage of processing the input prompt. Eval is the stage of generating output.</p>\n<p>Tokens per second is a common metric to use for output generation. Looking at the eval rate, this system achieved 4.89 \"(output) tokens per second\". For comparison, <a href=\"https://www.reddit.com/r/LocalLLaMA/comments/162pgx9/what_do_yall_consider_acceptable_tokens_per/\" target=\"_blank\" rel=\"noopener noreferrer\">7-10 tokens/second is thought to be acceptable for general use</a>.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"why-cant-i-just-use-tokens-per-second\">Why can't I just use tokens per second?<a href=\"https://isaac-chung.github.io/blog/llm-serving#why-cant-i-just-use-tokens-per-second\" class=\"hash-link\" aria-label=\"Direct link to Why can't I just use tokens per second?\" title=\"Direct link to Why can't I just use tokens per second?\">​</a></h2>\n<p>A single metric is rarely enough to capture the whole picture. It's important to note that all tokens are not made equal. For example, <a href=\"https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper\" target=\"_blank\" rel=\"noopener noreferrer\">Llama2 tokenization is 19% longer than that of ChatGPT</a>, yet it's still much cheaper. This should be considered when serving LLMs of different families.</p>\n<p>Also, the inference system is often used by more than one user at a time (multiple concurrent users). LLMs likely power a bigger system, such as a chat service. You might want to give users a good experience by considering tokens per second <em>per user</em>, or tokens per second at different request rates.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"what-other-llm-serving-metrics-should-i-consider\">What other LLM serving metrics should I consider?<a href=\"https://isaac-chung.github.io/blog/llm-serving#what-other-llm-serving-metrics-should-i-consider\" class=\"hash-link\" aria-label=\"Direct link to What other LLM serving metrics should I consider?\" title=\"Direct link to What other LLM serving metrics should I consider?\">​</a></h2>\n<p>A common metric to use is <strong>Time To First Token (TTFT)</strong>, which is how quickly users start seeing model outputs after a request is sent. Low waiting times are important in real-time interactions, but less so in offline workloads. This metric measures the time required to a) process the prompt and then b) generate the first output token. From the example above, the TTFT is just over 4 seconds as the load time was almost negligible.</p>\n<p>Another way to represent tokens per second is its inverse: <strong>Time Per Output Token (TPOT)</strong>, the time to generate one output token for each request. This metric can be used to perceive the \"speed\" of the model. From the example above, the TPOT is 200ms per token.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"final-thought-yes-just-one\">Final Thought (yes, just one)<a href=\"https://isaac-chung.github.io/blog/llm-serving#final-thought-yes-just-one\" class=\"hash-link\" aria-label=\"Direct link to Final Thought (yes, just one)\" title=\"Direct link to Final Thought (yes, just one)\">​</a></h2>\n<p>Overall, it's important to keep the goal of the system and user requirements in mind and avoid blindly optimizing metrics for optimization's sake.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"further-reading\">Further Reading<a href=\"https://isaac-chung.github.io/blog/llm-serving#further-reading\" class=\"hash-link\" aria-label=\"Direct link to Further Reading\" title=\"Direct link to Further Reading\">​</a></h2>\n<p>Here are some extra resources for some background on how LLMs generate text and deeper dives on LLM serving:</p>\n<ol>\n<li><a href=\"https://jalammar.github.io/illustrated-gpt2/#part-1-got-and-language-modeling\" target=\"_blank\" rel=\"noopener noreferrer\">How LLMs use decoder blocks for generation</a> with illustrations</li>\n<li><a href=\"https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices\" target=\"_blank\" rel=\"noopener noreferrer\">Best Practices for LLM Inference</a> from MosaicML</li>\n<li><a href=\"https://www.baseten.co/blog/faster-mixtral-inference-with-tensorrt-llm-and-quantization/\" target=\"_blank\" rel=\"noopener noreferrer\">Faster Mixtral inference with TensorRT-LLM</a> by Baseten</li>\n</ol>",
            "url": "https://isaac-chung.github.io/blog/llm-serving",
            "title": "All about Timing: A quick look at metrics for LLM serving",
            "summary": "How do you measure the performance of LLM serving systems? Production services in engineering are often evaluated using metrics like Requests Per Second (RPS), uptime, and latency. In computer vision systems, Frames Per Second (FPS) is often used as the main model throughput metric for use cases that involve near-real-time detection and tracking. Does serving LLMs have something similar? Certainly. A recent conversation with my team (after they read my Ollama blog) got me thinking about additional metrics that we could be tracking.",
            "date_modified": "2024-01-21T00:00:00.000Z",
            "author": {
                "name": "Isaac Chung",
                "url": "https://isaac-chung.github.io"
            },
            "tags": [
                "metrics",
                "serving",
                "timing",
                "AI",
                "LLM"
            ]
        },
        {
            "id": "https://isaac-chung.github.io/blog/quantized-models-dont-fit",
            "content_html": "<p>A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That's what happened to me when I was trying to run <a href=\"https://ollama.ai/library/mixtral\" target=\"_blank\" rel=\"noopener noreferrer\">Mixtral-8x7B with Ollama</a> (check out this <a href=\"https://isaac-chung.github.io/blog/what-is-ollama\">previous blog post on what Ollama is</a>). The model requires 26GB of RAM while my laptop only has 16GB. I'll try to walk through the workaround a <em>bit</em> at a time (pun intended).</p>\n<div class=\"theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success\"><div class=\"admonitionHeading_Gvgb\"><span class=\"admonitionIcon_Rf37\"><svg viewBox=\"0 0 12 16\"><path fill-rule=\"evenodd\" d=\"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z\"></path></svg></span>Key questions I'll address are:</div><div class=\"admonitionContent_BuS1\"><ul>\n<li>What is quantization?</li>\n<li>What is offloading?</li>\n<li>How to run Mixtral-8x7B for free?</li>\n</ul></div></div>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"what-is-quantization\">What is quantization?<a href=\"https://isaac-chung.github.io/blog/quantized-models-dont-fit#what-is-quantization\" class=\"hash-link\" aria-label=\"Direct link to What is quantization?\" title=\"Direct link to What is quantization?\">​</a></h2>\n<p>Quantization generally is a process that converts continuous values to a discrete set of values. A tangible analogy would be how we tell time: time is continuous, and we use hours, minutes, and seconds to \"quantize\" time. Sometimes \"around 10am\" is good enough, and sometimes we want to be precise to the millisecond.\nIn the context of deep learning, it is a technique to reduce the computational and memory costs of running a model by using lower-precision numerical types to represent its weights and activations. In simpler terms, we are trying to be less precise with the numbers that makes up the model weights so that it takes up less memory and can perform operations faster. <a href=\"https://stackoverflow.blog/2023/08/23/fitting-ai-models-in-your-pocket-with-quantization/\" target=\"_blank\" rel=\"noopener noreferrer\">This StackOverflow blog</a> has great visualizations of how quantization works. Without repeating the main content, the key takeaway for me was this image: The fewer bits you use per pixel, the less memory needed, but the image quality also may decrease.</p>\n<figure style=\"border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right\" id=\"figure-gallery\"><a href=\"https://cdn.stackoverflow.co/images/jo7n4k8s/production/5ee6f4e98bf05001b3699344f784adad0177ebe0-688x444.gif?auto=format\" data-pswp-width=\"0\" data-pswp-height=\"0\"><img src=\"https://cdn.stackoverflow.co/images/jo7n4k8s/production/5ee6f4e98bf05001b3699344f784adad0177ebe0-688x444.gif?auto=format\" alt=\"Representing images with varying number of bits.\" style=\"max-width:100%;height:auto\"></a><hr style=\"margin:5px 0;background-color:rgba(0, 0, 0, .2)\"><figcaption style=\"margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em\">Image from StackOverflow.</figcaption></figure>\n<p>Weights normally use 32-bit floating points, and are often quantized to float16 or <a href=\"https://github.com/TimDettmers/bitsandbytes\" target=\"_blank\" rel=\"noopener noreferrer\">int8</a>. Ollama uses 4-bit quantization, which means instead of using 32 \"101100...\"s for one value, only 4 are used. That means theoretically, you get 8x savings in memory usage. Quantization can be broadly grouped into 2 main methods:</p>\n<ol>\n<li>Post Training Quantization (PTQ): Done after a model is trained. A \"calibration dataset\" is used to capture the distribution of activations to calculate quant parameters (scale, zero point) for all inputs. No re-training is needed.</li>\n<li>Quantization Aware Training (QAT): Models are quantized during re-training/finetuning where low precision behaviour is simulated in the forward pass (backward pass remains the same). QAT is often able to better preserve accuracy when compared to PTQ, but incurs a high cost from re-training, which may not be suitable for LLMs.</li>\n</ol>\n<p>When it comes to quantizing LLM weights, methods like <a href=\"https://arxiv.org/abs/2305.14314\" target=\"_blank\" rel=\"noopener noreferrer\">NF4</a>, <a href=\"https://arxiv.org/abs/2210.17323\" target=\"_blank\" rel=\"noopener noreferrer\">GPTQ</a>, <a href=\"https://arxiv.org/abs/2306.00978\" target=\"_blank\" rel=\"noopener noreferrer\">AWQ</a>, and <a href=\"https://github.com/rustformers/llm/blob/main/crates/ggml/README.md\" target=\"_blank\" rel=\"noopener noreferrer\">GGML/GGUF</a> are among the most popular. A key observation is that not all weights are equally important (as pointed out in the AWQ paper). Keeping higher precision for more critical layers / weights proved to be key in the balance of accuracy and resource usage. In particular, the Mixtral model from Ollama seems to be using GGML, which groups blocks of values and rounds them to a lower precision (as opposed to using a global parameter).</p>\n<p><a href=\"https://arxiv.org/pdf/2312.17238.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">A recent paper</a> (3 weeks old!) that focuses on running Mixture-of-Experts type models on consumer hardware seems to have the answer to my misfortunes. In addition to quantization, they use one more trick in the book - offloading.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"what-is-offloading\">What is offloading?<a href=\"https://isaac-chung.github.io/blog/quantized-models-dont-fit#what-is-offloading\" class=\"hash-link\" aria-label=\"Direct link to What is offloading?\" title=\"Direct link to What is offloading?\">​</a></h2>\n<p>Offloading is putting some parameters in a separate, cheaper memory, such as system RAM, and only load them \"just-in-time\" when they are needed for computation. It proves to be very suitable for inferencing and training LLMs with limited GPU memory. In the context of using Mixtral, the MoE architecture contain multiple “experts” (layers) and a “gating function” that selects which experts are used on a given input. That way the MoE block only uses a small portion of all “experts” for any single forward pass. Each expert is offloaded separately and only brought pack to GPU when needed.</p>\n<figure style=\"border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right\" id=\"figure-gallery\"><a href=\"https://isaac-chung.github.io/assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg\" data-pswp-width=\"0\" data-pswp-height=\"0\"><img src=\"https://isaac-chung.github.io/assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg\" alt=\"8 big apples barely fitting into a crate.\" style=\"max-width:100%;height:auto\"></a><hr style=\"margin:5px 0;background-color:rgba(0, 0, 0, .2)\"><figcaption style=\"margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em\">Image by OpenAI DALL-E 3.</figcaption></figure>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"how-to-run-mixtral-8x7b-for-free\">How to run Mixtral-8x7B for free?<a href=\"https://isaac-chung.github.io/blog/quantized-models-dont-fit#how-to-run-mixtral-8x7b-for-free\" class=\"hash-link\" aria-label=\"Direct link to How to run Mixtral-8x7B for free?\" title=\"Direct link to How to run Mixtral-8x7B for free?\">​</a></h2>\n<p>So here we are. This is by no means to run Mixtral for production use. Here is the <a href=\"https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">colab notebook</a> by the authors of the paper mentioned above. Even though they were targeting the hardware specs of the Google free-tier instances, I might just be able to run Mixtral on my laptop after all.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"further-reading\">Further Reading<a href=\"https://isaac-chung.github.io/blog/quantized-models-dont-fit#further-reading\" class=\"hash-link\" aria-label=\"Direct link to Further Reading\" title=\"Direct link to Further Reading\">​</a></h2>\n<ol>\n<li><a href=\"https://huggingface.co/blog/4bit-transformers-bitsandbytes\" target=\"_blank\" rel=\"noopener noreferrer\">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a></li>\n<li><a href=\"https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172\" target=\"_blank\" rel=\"noopener noreferrer\">Quantize Llama models with GGUF and llama.cpp</a></li>\n<li><a href=\"https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34\" target=\"_blank\" rel=\"noopener noreferrer\">4-bit Quantization with GPTQ</a></li>\n</ol>",
            "url": "https://isaac-chung.github.io/blog/quantized-models-dont-fit",
            "title": "When Quantized Models Still Don't Fit",
            "summary": "A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That's what happened to me when I was trying to run Mixtral-8x7B with Ollama (check out this previous blog post on what Ollama is). The model requires 26GB of RAM while my laptop only has 16GB. I'll try to walk through the workaround a bit at a time (pun intended).",
            "date_modified": "2024-01-14T00:00:00.000Z",
            "author": {
                "name": "Isaac Chung",
                "url": "https://isaac-chung.github.io"
            },
            "tags": [
                "quantization",
                "mixed",
                "AI",
                "LLM",
                "ML",
                "chatbot",
                "mixtral"
            ]
        },
        {
            "id": "https://isaac-chung.github.io/blog/what-is-ollama",
            "content_html": "<p>Being able to run LLMs locally and <em>easily</em> is truly a game changer. I have heard about <a href=\"https://github.com/jmorganca/ollama\" target=\"_blank\" rel=\"noopener noreferrer\">Ollama</a> before and decided to take a look at it this past weekend.</p>\n<div class=\"theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success\"><div class=\"admonitionHeading_Gvgb\"><span class=\"admonitionIcon_Rf37\"><svg viewBox=\"0 0 12 16\"><path fill-rule=\"evenodd\" d=\"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z\"></path></svg></span>Key questions I'll address are:</div><div class=\"admonitionContent_BuS1\"><ul>\n<li>Why is running LLMs locally becoming a hot thang</li>\n<li>What is Ollama?</li>\n<li>Should you use Ollama?</li>\n</ul></div></div>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"why-is-running-llms-locally-becoming-a-hot-thang\">Why is running LLMs locally becoming a hot thang?<a href=\"https://isaac-chung.github.io/blog/what-is-ollama#why-is-running-llms-locally-becoming-a-hot-thang\" class=\"hash-link\" aria-label=\"Direct link to Why is running LLMs locally becoming a hot thang?\" title=\"Direct link to Why is running LLMs locally becoming a hot thang?\">​</a></h2>\n<p>There was a time when LLMs were only accessible via cloud APIs from the giant providers like OpenAI and Anthropic. Don't get me wrong, those cloud API providers still dominate the market, and they have nice UIs that makes it easy for many users to get started. However, the price users pay (other than a pro plan or API costs) is that the providers will have full access to your chat data. For those who want run LLMs securely on their own hardware, they either had to train their own LLM (which is super costly), or wait till the release of Llama2 - an open weights model family. After that, a flood of how-to cookbooks and self-deployment launch services came onto the scene to help user deploy their own Llama2 \"instance\".</p>\n<p>At this point, LLMs are still running on the cloud (just happens to be your own cloud), where management of the instances and GPUs could take up quite a lot of resources. It was necessary at the time because of the model sizes. The smallest Llama2 model (16bit floating point precision version of Llama2-7b-chat) comes in at 13GB. This means that most of the LLMs that are bigger that 7B (i.e. in the 10s or even 100s of GB in size) cannot fit into a regular laptop GPU.</p>\n<p>Then came the hail mary - quantization (to be covered in a separate blog later). By quantizing the model weights to 4-bits, the <a href=\"https://ollama.ai/library/llama2:7b\" target=\"_blank\" rel=\"noopener noreferrer\">Llama2-7b-chat model</a> now only takes up 3.8GB, which means, it'll finally fit!</p>\n<figure style=\"border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right\" id=\"figure-gallery\"><a href=\"https://isaac-chung.github.io/assets/images/tiny-llama-eab3b5a919157baa1f102158a2858f98.png\" data-pswp-width=\"0\" data-pswp-height=\"0\"><img src=\"https://isaac-chung.github.io/assets/images/tiny-llama-eab3b5a919157baa1f102158a2858f98.png\" alt=\"A tiny llama alongside a regular sized llama\" style=\"max-width:100%;height:auto\"></a><hr style=\"margin:5px 0;background-color:rgba(0, 0, 0, .2)\"><figcaption style=\"margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em\">Image by OpenAI DALL-E 3.</figcaption></figure>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"what-is-ollama\">What is Ollama?<a href=\"https://isaac-chung.github.io/blog/what-is-ollama#what-is-ollama\" class=\"hash-link\" aria-label=\"Direct link to What is Ollama?\" title=\"Direct link to What is Ollama?\">​</a></h2>\n<p><a href=\"https://ollama.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Ollama</a> is an open-source app that lets you run, create, and share large language models locally with a command-line interface on MacOS and Linux.\nGiven the name, Ollama began by supporting Llama2, then expanded its <a href=\"https://ollama.ai/library\" target=\"_blank\" rel=\"noopener noreferrer\">model library</a> to include models like Mistral and Phi-2. Ollama makes it easy to get started with running LLMs on your own hardware in very little setup time.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"should-you-use-ollama\">Should you use Ollama?<a href=\"https://isaac-chung.github.io/blog/what-is-ollama#should-you-use-ollama\" class=\"hash-link\" aria-label=\"Direct link to Should you use Ollama?\" title=\"Direct link to Should you use Ollama?\">​</a></h2>\n<p>Yes, if you want to be able to run LLMs on your laptop, keep your chat data away from 3rd party services, and can interact with them via command line in a simple way. There are also many community integrations such as UIs and plugins in chat platforms. It might not be for you if you do not want to deal with setting up at all.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"how-to-get-started-with-ollama\">How to get started with Ollama<a href=\"https://isaac-chung.github.io/blog/what-is-ollama#how-to-get-started-with-ollama\" class=\"hash-link\" aria-label=\"Direct link to How to get started with Ollama\" title=\"Direct link to How to get started with Ollama\">​</a></h2>\n<p>It seems super simple.</p>\n<ol>\n<li>On Mac, simply <a href=\"https://ollama.ai/download/Ollama-darwin.zip\" target=\"_blank\" rel=\"noopener noreferrer\">download the application</a>.</li>\n<li>Then run this to start chatting with Llama2:</li>\n</ol>\n<div class=\"codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#393A34;--prism-background-color:#f6f8fa\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" style=\"color:#393A34;background-color:#f6f8fa\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">ollama run llama2</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button type=\"button\" aria-label=\"Copy code to clipboard\" title=\"Copy\" class=\"clean-btn\"><span class=\"copyButtonIcons_eSgA\" aria-hidden=\"true\"><svg viewBox=\"0 0 24 24\" class=\"copyButtonIcon_y97N\"><path fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"></path></svg><svg viewBox=\"0 0 24 24\" class=\"copyButtonSuccessIcon_LjdS\"><path fill=\"currentColor\" d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\"></path></svg></span></button></div></div></div>\n<p>In addition to chatting with text prompts, Ollama also supports:</p>\n<ul>\n<li><a href=\"https://github.com/jmorganca/ollama?tab=readme-ov-file#multimodal-models\" target=\"_blank\" rel=\"noopener noreferrer\">multi-modal inputs</a>: e.g. asking questions about an image</li>\n<li><a href=\"https://github.com/jmorganca/ollama?tab=readme-ov-file#pass-in-prompt-as-arguments\" target=\"_blank\" rel=\"noopener noreferrer\">passing an argument within a prompt</a>: e.g. summarize a README page</li>\n<li><a href=\"https://github.com/jmorganca/ollama?tab=readme-ov-file#rest-api\" target=\"_blank\" rel=\"noopener noreferrer\">serving as a REST API</a>: e.g. chat with the model using python scripts</li>\n<li><a href=\"https://ollama.ai/blog/ollama-is-now-available-as-an-official-docker-image\" target=\"_blank\" rel=\"noopener noreferrer\">running as a docker image</a>: e.g. Deploy Ollama with Kubernetes</li>\n</ul>\n<p>The <a href=\"https://github.com/jmorganca/ollama\" target=\"_blank\" rel=\"noopener noreferrer\">official Github repo README page</a> has more examples.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"some-notes\">Some notes<a href=\"https://isaac-chung.github.io/blog/what-is-ollama#some-notes\" class=\"hash-link\" aria-label=\"Direct link to Some notes\" title=\"Direct link to Some notes\">​</a></h2>\n<p>After using Ollama for a weekend, I have noticed the following that may not be obvious at first glance:</p>\n<ol>\n<li>By hitting the <code>run</code> command, you start a chat session. This session will live until you exit from it or when you terminate process. Interestingly, chat state was not managed at the beginning, which means that you'd run into issues where <a href=\"https://github.com/jmorganca/ollama/issues/8\" target=\"_blank\" rel=\"noopener noreferrer\">the model immediately forgets about the context right after a response</a>. Now, you can keep chatting and the model remembers what you entered as long as it fits within its context window. Once the context window is exceeded, Ollama will truncate the input from the beginning until it fits the context window again <a href=\"https://github.com/jmorganca/ollama/pull/306\" target=\"_blank\" rel=\"noopener noreferrer\">while keeping the system instructions</a>.</li>\n<li>Ollama is based on <a href=\"https://github.com/ggerganov/llama.cpp\" target=\"_blank\" rel=\"noopener noreferrer\">llama.cpp</a>, an implementation of the Llama architecture in plain C/C++ without dependencies using only CPU and RAM.</li>\n<li>Ollama is quite docker-like, and for me it feels intuitive. You pull models then run them. The <a href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md\" target=\"_blank\" rel=\"noopener noreferrer\">Modelfile</a>, the \"blueprint to create and share models with Ollama\", is also quite dockerfile-like.</li>\n</ol>\n<p>Overall I find Ollama quite easy to use and would likely continue to use it for something quick. It would be pretty fun if <a href=\"https://github.com/jmorganca/ollama/issues/142\" target=\"_blank\" rel=\"noopener noreferrer\">conversation history can be persisted</a>!</p>",
            "url": "https://isaac-chung.github.io/blog/what-is-ollama",
            "title": "What is Ollama? A shallow dive into running LLMs locally",
            "summary": "Being able to run LLMs locally and easily is truly a game changer. I have heard about Ollama before and decided to take a look at it this past weekend.",
            "date_modified": "2024-01-07T00:00:00.000Z",
            "author": {
                "name": "Isaac Chung",
                "url": "https://isaac-chung.github.io"
            },
            "tags": [
                "ollama",
                "llama",
                "chat",
                "AI",
                "LLM",
                "ML",
                "chatbot",
                "local"
            ]
        }
    ]
}