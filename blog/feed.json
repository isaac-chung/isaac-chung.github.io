{
    "version": "https://jsonfeed.org/version/1",
    "title": "Isaac Chung Blog",
    "home_page_url": "https://isaac-chung.github.io/blog",
    "description": "Isaac Chung Blog",
    "items": [
        {
            "id": "https://isaac-chung.github.io/blog/what-is-ollama",
            "content_html": "<p>Being able to run LLMs locally and <em>easily</em> is truly a game changer. I have heard about <a href=\"https://github.com/jmorganca/ollama\" target=\"_blank\" rel=\"noopener noreferrer\">Ollama</a> before and decided to take a look at it this past weekend.</p>\n<div class=\"theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success\"><div class=\"admonitionHeading_Gvgb\"><span class=\"admonitionIcon_Rf37\"><svg viewBox=\"0 0 12 16\"><path fill-rule=\"evenodd\" d=\"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z\"></path></svg></span>Key questions I'll address are:</div><div class=\"admonitionContent_BuS1\"><ul>\n<li>Why is running LLMs locally becoming a hot thang</li>\n<li>What is Ollama?</li>\n<li>Should you use Ollama?</li>\n</ul></div></div>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"why-is-running-llms-locally-becoming-a-hot-thang\">Why is running LLMs locally becoming a hot thang?<a href=\"https://isaac-chung.github.io/blog/what-is-ollama#why-is-running-llms-locally-becoming-a-hot-thang\" class=\"hash-link\" aria-label=\"Direct link to Why is running LLMs locally becoming a hot thang?\" title=\"Direct link to Why is running LLMs locally becoming a hot thang?\">​</a></h2>\n<p>There was a time when LLMs were only accessible via cloud APIs from the giant providers like OpenAI and Anthropic. Don't get me wrong, those cloud API providers still dominate the market, and they have nice UIs that makes it easy for many users to get started. The price users pay (other than a pro plan or API costs) is that the providers will have full access to your chat data. For those who want run LLMs securely on their own hardware, they either had to train their own LLM (which is super costly), or wait till the release of Llama2 - an open weights model family. After that, a flood of how-to cookbooks and self-deployment launch services came onto the scene to help user deploy their own Llama2 \"instance\".</p>\n<p>At this point, LLMs are still running on the cloud (just happens to be your own cloud), where management of the instances and GPUs could take up quite a lot of resources. It was necessary at the time because of the model sizes. The smallest Llama2 model (16bit floating point precision version of Llama2-7b-chat) comes in at 13GB. This means that most of the LLMs that are bigger that 7B (i.e. in the 10s or even 100s of GB in size) cannot fit into a regular laptop GPU.</p>\n<p>Then came the hail mary - quantization (to be covered in a separate blog later). By quantizing the model weights to 4-bits, the <a href=\"https://ollama.ai/library/llama2:7b\" target=\"_blank\" rel=\"noopener noreferrer\">Llama2-7b-chat model</a> now only takes up 3.8GB, which means, it'll finally fit!</p>\n<figure style=\"border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right\"><img src=\"https://isaac-chung.github.io/assets/images/tiny-llama-eab3b5a919157baa1f102158a2858f98.png\" alt=\"A tiny llama alongside a regular sized llama\" style=\"max-width:100%;height:auto\"><hr style=\"margin:5px 0;background-color:rgba(0, 0, 0, .2)\"><figcaption style=\"margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em\">Image by OpenAI DALL-E 3.</figcaption></figure>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"what-is-ollama\">What is Ollama?<a href=\"https://isaac-chung.github.io/blog/what-is-ollama#what-is-ollama\" class=\"hash-link\" aria-label=\"Direct link to What is Ollama?\" title=\"Direct link to What is Ollama?\">​</a></h2>\n<p><a href=\"https://ollama.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Ollama</a> is an open-source app that lets you run, create, and share large language models locally with a command-line interface on MacOS and Linux.\nGiven the name, Ollama began by supporting Llama2, then expanded its <a href=\"https://ollama.ai/library\" target=\"_blank\" rel=\"noopener noreferrer\">model library</a> to include models like Mistral and Phi-2. Ollama makes it easy to get started with running LLMs on your own hardware in very little setup time.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"should-you-use-ollama\">Should you use Ollama?<a href=\"https://isaac-chung.github.io/blog/what-is-ollama#should-you-use-ollama\" class=\"hash-link\" aria-label=\"Direct link to Should you use Ollama?\" title=\"Direct link to Should you use Ollama?\">​</a></h2>\n<p>Yes, if you want to be able to run LLMs on your laptop, keep your chat data away from 3rd party services, and can interact with them via command line in a simple way. There are also many community integrations such as UIs and plugins in chat platforms. It might not be for you if you do not want to deal with setting up at all.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"how-to-get-started-with-ollama\">How to get started with Ollama<a href=\"https://isaac-chung.github.io/blog/what-is-ollama#how-to-get-started-with-ollama\" class=\"hash-link\" aria-label=\"Direct link to How to get started with Ollama\" title=\"Direct link to How to get started with Ollama\">​</a></h2>\n<p>It seems super simple.</p>\n<ol>\n<li>On Mac, simply <a href=\"https://ollama.ai/download/Ollama-darwin.zip\" target=\"_blank\" rel=\"noopener noreferrer\">download the application</a>.</li>\n<li>Then run this to start chatting with Llama2:</li>\n</ol>\n<div class=\"codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#393A34;--prism-background-color:#f6f8fa\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" style=\"color:#393A34;background-color:#f6f8fa\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">ollama run llama2</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button type=\"button\" aria-label=\"Copy code to clipboard\" title=\"Copy\" class=\"clean-btn\"><span class=\"copyButtonIcons_eSgA\" aria-hidden=\"true\"><svg viewBox=\"0 0 24 24\" class=\"copyButtonIcon_y97N\"><path fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"></path></svg><svg viewBox=\"0 0 24 24\" class=\"copyButtonSuccessIcon_LjdS\"><path fill=\"currentColor\" d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\"></path></svg></span></button></div></div></div>\n<p>In addition to chatting with text prompts, Ollama also supports:</p>\n<ul>\n<li><a href=\"https://github.com/jmorganca/ollama?tab=readme-ov-file#multimodal-models\" target=\"_blank\" rel=\"noopener noreferrer\">multimodal inputs</a>: e.g. asking questions about an iamge</li>\n<li><a href=\"https://github.com/jmorganca/ollama?tab=readme-ov-file#pass-in-prompt-as-arguments\" target=\"_blank\" rel=\"noopener noreferrer\">passing an argument within a prompt</a>: e.g. summarize a README page</li>\n<li><a href=\"https://github.com/jmorganca/ollama?tab=readme-ov-file#rest-api\" target=\"_blank\" rel=\"noopener noreferrer\">serving as a REST API</a>: e.g. chat with the model using python scripts</li>\n<li><a href=\"https://ollama.ai/blog/ollama-is-now-available-as-an-official-docker-image\" target=\"_blank\" rel=\"noopener noreferrer\">running as a docker image</a>: e.g. Deploy Ollama with Kubernetes</li>\n</ul>\n<p>The <a href=\"https://github.com/jmorganca/ollama\" target=\"_blank\" rel=\"noopener noreferrer\">official Github repo README page</a> has more examples.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"some-notes\">Some notes<a href=\"https://isaac-chung.github.io/blog/what-is-ollama#some-notes\" class=\"hash-link\" aria-label=\"Direct link to Some notes\" title=\"Direct link to Some notes\">​</a></h2>\n<p>After using Ollama for a weekend, I have noticed the following that may not be obvious at first glance:</p>\n<ol>\n<li>By hitting the <code>run</code> command, you start a chat session. This session will live until you exit from it or when you terminate process. Interestingly, chat state was not managed at the beginning, which means that you'd run into issues where <a href=\"https://github.com/jmorganca/ollama/issues/8\" target=\"_blank\" rel=\"noopener noreferrer\">the model immediately forgets about the context right after a response</a>. Now, you can keep chatting and the model remembers what you entered as long as it fits within its context window. Once the context window is exceeded, Ollama will truncate the input from the beginngin until it fits the context window again <a href=\"https://github.com/jmorganca/ollama/pull/306\" target=\"_blank\" rel=\"noopener noreferrer\">while keeping the system instructions</a>.</li>\n<li>Ollama is based on <a href=\"https://github.com/ggerganov/llama.cpp\" target=\"_blank\" rel=\"noopener noreferrer\">llama.cpp</a>, an implementation of the Llama architecture in plain C/C++ without dependencies using only CPU and RAM.</li>\n<li>Ollama is quite docker-like, and for me it feels intuitive. You pull models then run them. The <a href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md\" target=\"_blank\" rel=\"noopener noreferrer\">Modelfile</a>, the \"blueprint to create and share models with Ollama\", is also quite dockerfile-like.</li>\n</ol>\n<p>Overall I find Ollama quite easy to use and would likely continue to use it for something quick. It would be pretty fun if <a href=\"https://github.com/jmorganca/ollama/issues/142\" target=\"_blank\" rel=\"noopener noreferrer\">conversation history can be persisted</a>!</p>",
            "url": "https://isaac-chung.github.io/blog/what-is-ollama",
            "title": "What is Ollama? A shallow dive into running LLMs locally",
            "summary": "Being able to run LLMs locally and easily is truly a game changer. I have heard about Ollama before and decided to take a look at it this past weekend.",
            "date_modified": "2024-01-07T00:00:00.000Z",
            "author": {
                "name": "Isaac Chung",
                "url": "https://isaac-chung.github.io"
            },
            "tags": [
                "ollama",
                "llama",
                "chat",
                "AI",
                "LLM",
                "ML",
                "chatbot",
                "local"
            ]
        }
    ]
}