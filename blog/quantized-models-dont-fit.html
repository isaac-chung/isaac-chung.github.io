<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">When Quantized Models Still Don&#x27;t Fit | Isaac Chung</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://isaac-chung.github.io/blog/quantized-models-dont-fit"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="When Quantized Models Still Don&#x27;t Fit | Isaac Chung"><meta data-rh="true" name="description" content="A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That&#x27;s what happened to me when I was trying to run Mixtral-8x7B with Ollama (check out this previous blog post on what Ollama is). The model requires 26GB of RAM while my laptop only has 16GB. I&#x27;ll try to walk through the workaround a bit at a time (pun intended)."><meta data-rh="true" property="og:description" content="A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That&#x27;s what happened to me when I was trying to run Mixtral-8x7B with Ollama (check out this previous blog post on what Ollama is). The model requires 26GB of RAM while my laptop only has 16GB. I&#x27;ll try to walk through the workaround a bit at a time (pun intended)."><meta data-rh="true" property="og:image" content="https://isaac-chung.github.io/assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg"><meta data-rh="true" name="twitter:image" content="https://isaac-chung.github.io/assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-01-14T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://isaac-chung.github.io"><meta data-rh="true" property="article:tag" content="quantization,mixed,AI,LLM,ML,chatbot,mixtral"><link data-rh="true" rel="icon" href="/img/1311.png"><link data-rh="true" rel="canonical" href="https://isaac-chung.github.io/blog/quantized-models-dont-fit"><link data-rh="true" rel="alternate" href="https://isaac-chung.github.io/blog/quantized-models-dont-fit" hreflang="en"><link data-rh="true" rel="alternate" href="https://isaac-chung.github.io/blog/quantized-models-dont-fit" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Isaac Chung RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Isaac Chung Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Isaac Chung JSON Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-D4GPBWLTG6"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-D4GPBWLTG6",{anonymize_ip:!0})</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.c9f2c36d.css">
<script src="/assets/js/runtime~main.23991363.js" defer="defer"></script>
<script src="/assets/js/main.2c022a8e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/1311.png" alt="icon" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/1311.png" alt="icon" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Isaac Chung</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/concurrent-requests">Serving Concurrent Requests with Quantized LLMs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/rag-eval-and-observability">How to really know if your RAG system is working well.</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/llm-serving">All about Timing: A quick look at metrics for LLM serving</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/quantized-models-dont-fit">When Quantized Models Still Don&#x27;t Fit</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/what-is-ollama">What is Ollama? A shallow dive into running LLMs locally</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="https://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That&#x27;s what happened to me when I was trying to run Mixtral-8x7B with Ollama (check out this previous blog post on what Ollama is). The model requires 26GB of RAM while my laptop only has 16GB. I&#x27;ll try to walk through the workaround a bit at a time (pun intended)."><link itemprop="image" href="https://isaac-chung.github.io/assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg"><header><h1 class="title_f1Hy" itemprop="headline">When Quantized Models Still Don&#x27;t Fit</h1><div class="container_mt6G margin-vert--md"><time datetime="2024-01-14T00:00:00.000Z" itemprop="datePublished">January 14, 2024</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/isaac-chung.png" alt="Isaac Chung" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Isaac Chung</span></a></div><small class="avatar__subtitle" itemprop="description">Senior Research Engineer @ Clarifai</small></div></div></div></div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><p>A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That&#x27;s what happened to me when I was trying to run <a href="https://ollama.ai/library/mixtral" target="_blank" rel="noopener noreferrer">Mixtral-8x7B with Ollama</a> (check out this <a href="/blog/what-is-ollama">previous blog post on what Ollama is</a>). The model requires 26GB of RAM while my laptop only has 16GB. I&#x27;ll try to walk through the workaround a <em>bit</em> at a time (pun intended).</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I&#x27;ll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>What is quantization?</li>
<li>What is offloading?</li>
<li>How to run Mixtral-8x7B for free?</li>
</ul></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-quantization">What is quantization?<a href="#what-is-quantization" class="hash-link" aria-label="Direct link to What is quantization?" title="Direct link to What is quantization?">​</a></h2>
<p>Quantization generally is a process that converts continuous values to a discrete set of values. A tangible analogy would be how we tell time: time is continuous, and we use hours, minutes, and seconds to &quot;quantize&quot; time. Sometimes &quot;around 10am&quot; is good enough, and sometimes we want to be precise to the millisecond.
In the context of deep learning, it is a technique to reduce the computational and memory costs of running a model by using lower-precision numerical types to represent its weights and activations. In simpler terms, we are trying to be less precise with the numbers that makes up the model weights so that it takes up less memory and can perform operations faster. <a href="https://stackoverflow.blog/2023/08/23/fitting-ai-models-in-your-pocket-with-quantization/" target="_blank" rel="noopener noreferrer">This StackOverflow blog</a> has great visualizations of how quantization works. Without repeating the main content, the key takeaway for me was this image: The fewer bits you use per pixel, the less memory needed, but the image quality also may decrease.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="https://cdn.stackoverflow.co/images/jo7n4k8s/production/5ee6f4e98bf05001b3699344f784adad0177ebe0-688x444.gif?auto=format" data-pswp-width="0" data-pswp-height="0"><img src="https://cdn.stackoverflow.co/images/jo7n4k8s/production/5ee6f4e98bf05001b3699344f784adad0177ebe0-688x444.gif?auto=format" alt="Representing images with varying number of bits." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em">Image from StackOverflow.</figcaption></figure>
<p>Weights normally use 32-bit floating points, and are often quantized to float16 or <a href="https://github.com/TimDettmers/bitsandbytes" target="_blank" rel="noopener noreferrer">int8</a>. Ollama uses 4-bit quantization, which means instead of using 32 &quot;101100...&quot;s for one value, only 4 are used. That means theoretically, you get 8x savings in memory usage. Quantization can be broadly grouped into 2 main methods:</p>
<ol>
<li>Post Training Quantization (PTQ): Done after a model is trained. A &quot;calibration dataset&quot; is used to capture the distribution of activations to calculate quant parameters (scale, zero point) for all inputs. No re-training is needed.</li>
<li>Quantization Aware Training (QAT): Models are quantized during re-training/finetuning where low precision behaviour is simulated in the forward pass (backward pass remains the same). QAT is often able to better preserve accuracy when compared to PTQ, but incurs a high cost from re-training, which may not be suitable for LLMs.</li>
</ol>
<p>When it comes to quantizing LLM weights, methods like <a href="https://arxiv.org/abs/2305.14314" target="_blank" rel="noopener noreferrer">NF4</a>, <a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="noopener noreferrer">GPTQ</a>, <a href="https://arxiv.org/abs/2306.00978" target="_blank" rel="noopener noreferrer">AWQ</a>, and <a href="https://github.com/rustformers/llm/blob/main/crates/ggml/README.md" target="_blank" rel="noopener noreferrer">GGML/GGUF</a> are among the most popular. A key observation is that not all weights are equally important (as pointed out in the AWQ paper). Keeping higher precision for more critical layers / weights proved to be key in the balance of accuracy and resource usage. In particular, the Mixtral model from Ollama seems to be using GGML, which groups blocks of values and rounds them to a lower precision (as opposed to using a global parameter).</p>
<p><a href="https://arxiv.org/pdf/2312.17238.pdf" target="_blank" rel="noopener noreferrer">A recent paper</a> (3 weeks old!) that focuses on running Mixture-of-Experts type models on consumer hardware seems to have the answer to my misfortunes. In addition to quantization, they use one more trick in the book - offloading.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-offloading">What is offloading?<a href="#what-is-offloading" class="hash-link" aria-label="Direct link to What is offloading?" title="Direct link to What is offloading?">​</a></h2>
<p>Offloading is putting some parameters in a separate, cheaper memory, such as system RAM, and only load them &quot;just-in-time&quot; when they are needed for computation. It proves to be very suitable for inferencing and training LLMs with limited GPU memory. In the context of using Mixtral, the MoE architecture contain multiple “experts” (layers) and a “gating function” that selects which experts are used on a given input. That way the MoE block only uses a small portion of all “experts” for any single forward pass. Each expert is offloaded separately and only brought pack to GPU when needed.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg" alt="8 big apples barely fitting into a crate." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em">Image by OpenAI DALL-E 3.</figcaption></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-run-mixtral-8x7b-for-free">How to run Mixtral-8x7B for free?<a href="#how-to-run-mixtral-8x7b-for-free" class="hash-link" aria-label="Direct link to How to run Mixtral-8x7B for free?" title="Direct link to How to run Mixtral-8x7B for free?">​</a></h2>
<p>So here we are. This is by no means to run Mixtral for production use. Here is the <a href="https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb" target="_blank" rel="noopener noreferrer">colab notebook</a> by the authors of the paper mentioned above. Even though they were targeting the hardware specs of the Google free-tier instances, I might just be able to run Mixtral on my laptop after all.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="further-reading">Further Reading<a href="#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading">​</a></h2>
<ol>
<li><a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes" target="_blank" rel="noopener noreferrer">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a></li>
<li><a href="https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172" target="_blank" rel="noopener noreferrer">Quantize Llama models with GGUF and llama.cpp</a></li>
<li><a href="https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34" target="_blank" rel="noopener noreferrer">4-bit Quantization with GPTQ</a></li>
</ol></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/quantization">quantization</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mixed">mixed</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">AI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ml">ML</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/chatbot">chatbot</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mixtral">mixtral</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/llm-serving"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">All about Timing: A quick look at metrics for LLM serving</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/what-is-ollama"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">What is Ollama? A shallow dive into running LLMs locally</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-quantization" class="table-of-contents__link toc-highlight">What is quantization?</a></li><li><a href="#what-is-offloading" class="table-of-contents__link toc-highlight">What is offloading?</a></li><li><a href="#how-to-run-mixtral-8x7b-for-free" class="table-of-contents__link toc-highlight">How to run Mixtral-8x7B for free?</a></li><li><a href="#further-reading" class="table-of-contents__link toc-highlight">Further Reading</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Social</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://twitter.com/isaacchung1217" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/isaac-chung/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/isaac-chung" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Blog Feeds</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://isaac-chung.github.io/blog/rss.xml" target="_blank" rel="noopener noreferrer" class="footer__link-item">RSS<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://isaac-chung.github.io/blog/atom.xml" target="_blank" rel="noopener noreferrer" class="footer__link-item">Atom<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://isaac-chung.github.io/blog/feed.json" target="_blank" rel="noopener noreferrer" class="footer__link-item">JSON<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Isaac Chung</div></div></div></footer></div>
</body>
</html>