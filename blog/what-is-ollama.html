<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">What is Ollama? A shallow dive into running LLMs locally | Isaac Chung</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://isaac-chung.github.io/blog/what-is-ollama"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="What is Ollama? A shallow dive into running LLMs locally | Isaac Chung"><meta data-rh="true" name="description" content="Being able to run LLMs locally and easily is truly a game changer. I have heard about Ollama before and decided to take a look at it this past weekend."><meta data-rh="true" property="og:description" content="Being able to run LLMs locally and easily is truly a game changer. I have heard about Ollama before and decided to take a look at it this past weekend."><meta data-rh="true" property="og:image" content="https://isaac-chung.github.io/assets/images/tiny-llama-eab3b5a919157baa1f102158a2858f98.png"><meta data-rh="true" name="twitter:image" content="https://isaac-chung.github.io/assets/images/tiny-llama-eab3b5a919157baa1f102158a2858f98.png"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-01-07T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://isaac-chung.github.io"><meta data-rh="true" property="article:tag" content="ollama,llama,chat,AI,LLM,ML,chatbot,local"><link data-rh="true" rel="icon" href="/img/1311.png"><link data-rh="true" rel="canonical" href="https://isaac-chung.github.io/blog/what-is-ollama"><link data-rh="true" rel="alternate" href="https://isaac-chung.github.io/blog/what-is-ollama" hreflang="en"><link data-rh="true" rel="alternate" href="https://isaac-chung.github.io/blog/what-is-ollama" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Isaac Chung RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Isaac Chung Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Isaac Chung JSON Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PNHB98RZ0D"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PNHB98RZ0D",{anonymize_ip:!0})</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.3a700447.css">
<script src="/assets/js/runtime~main.779375cc.js" defer="defer"></script>
<script src="/assets/js/main.16f562d1.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/1311.png" alt="icon" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/1311.png" alt="icon" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Isaac Chung</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/isaac-chung" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/quantized-models-dont-fit">When Quantized Models Still Don&#x27;t Fit</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/what-is-ollama">What is Ollama? A shallow dive into running LLMs locally</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="https://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="Being able to run LLMs locally and easily is truly a game changer. I have heard about Ollama before and decided to take a look at it this past weekend."><link itemprop="image" href="https://isaac-chung.github.io/assets/images/tiny-llama-eab3b5a919157baa1f102158a2858f98.png"><header><h1 class="title_f1Hy" itemprop="headline">What is Ollama? A shallow dive into running LLMs locally</h1><div class="container_mt6G margin-vert--md"><time datetime="2024-01-07T00:00:00.000Z" itemprop="datePublished">January 7, 2024</time> Â· <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/isaac-chung.png" alt="Isaac Chung" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Isaac Chung</span></a></div><small class="avatar__subtitle" itemprop="description">Senior Research Engineer @ Clarifai</small></div></div></div></div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><p>Being able to run LLMs locally and <em>easily</em> is truly a game changer. I have heard about <a href="https://github.com/jmorganca/ollama" target="_blank" rel="noopener noreferrer">Ollama</a> before and decided to take a look at it this past weekend.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I&#x27;ll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>Why is running LLMs locally becoming a hot thang</li>
<li>What is Ollama?</li>
<li>Should you use Ollama?</li>
</ul></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-is-running-llms-locally-becoming-a-hot-thang">Why is running LLMs locally becoming a hot thang?<a href="#why-is-running-llms-locally-becoming-a-hot-thang" class="hash-link" aria-label="Direct link to Why is running LLMs locally becoming a hot thang?" title="Direct link to Why is running LLMs locally becoming a hot thang?">â</a></h2>
<p>There was a time when LLMs were only accessible via cloud APIs from the giant providers like OpenAI and Anthropic. Don&#x27;t get me wrong, those cloud API providers still dominate the market, and they have nice UIs that makes it easy for many users to get started. However, the price users pay (other than a pro plan or API costs) is that the providers will have full access to your chat data. For those who want run LLMs securely on their own hardware, they either had to train their own LLM (which is super costly), or wait till the release of Llama2 - an open weights model family. After that, a flood of how-to cookbooks and self-deployment launch services came onto the scene to help user deploy their own Llama2 &quot;instance&quot;.</p>
<p>At this point, LLMs are still running on the cloud (just happens to be your own cloud), where management of the instances and GPUs could take up quite a lot of resources. It was necessary at the time because of the model sizes. The smallest Llama2 model (16bit floating point precision version of Llama2-7b-chat) comes in at 13GB. This means that most of the LLMs that are bigger that 7B (i.e. in the 10s or even 100s of GB in size) cannot fit into a regular laptop GPU.</p>
<p>Then came the hail mary - quantization (to be covered in a separate blog later). By quantizing the model weights to 4-bits, the <a href="https://ollama.ai/library/llama2:7b" target="_blank" rel="noopener noreferrer">Llama2-7b-chat model</a> now only takes up 3.8GB, which means, it&#x27;ll finally fit!</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right"><img src="/assets/images/tiny-llama-eab3b5a919157baa1f102158a2858f98.png" alt="A tiny llama alongside a regular sized llama" style="max-width:100%;height:auto"><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em">Image by OpenAI DALL-E 3.</figcaption></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-ollama">What is Ollama?<a href="#what-is-ollama" class="hash-link" aria-label="Direct link to What is Ollama?" title="Direct link to What is Ollama?">â</a></h2>
<p><a href="https://ollama.ai/" target="_blank" rel="noopener noreferrer">Ollama</a> is an open-source app that lets you run, create, and share large language models locally with a command-line interface on MacOS and Linux.
Given the name, Ollama began by supporting Llama2, then expanded its <a href="https://ollama.ai/library" target="_blank" rel="noopener noreferrer">model library</a> to include models like Mistral and Phi-2. Ollama makes it easy to get started with running LLMs on your own hardware in very little setup time.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="should-you-use-ollama">Should you use Ollama?<a href="#should-you-use-ollama" class="hash-link" aria-label="Direct link to Should you use Ollama?" title="Direct link to Should you use Ollama?">â</a></h2>
<p>Yes, if you want to be able to run LLMs on your laptop, keep your chat data away from 3rd party services, and can interact with them via command line in a simple way. There are also many community integrations such as UIs and plugins in chat platforms. It might not be for you if you do not want to deal with setting up at all.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-get-started-with-ollama">How to get started with Ollama<a href="#how-to-get-started-with-ollama" class="hash-link" aria-label="Direct link to How to get started with Ollama" title="Direct link to How to get started with Ollama">â</a></h2>
<p>It seems super simple.</p>
<ol>
<li>On Mac, simply <a href="https://ollama.ai/download/Ollama-darwin.zip" target="_blank" rel="noopener noreferrer">download the application</a>.</li>
<li>Then run this to start chatting with Llama2:</li>
</ol>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ollama run llama2</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In addition to chatting with text prompts, Ollama also supports:</p>
<ul>
<li><a href="https://github.com/jmorganca/ollama?tab=readme-ov-file#multimodal-models" target="_blank" rel="noopener noreferrer">multi-modal inputs</a>: e.g. asking questions about an image</li>
<li><a href="https://github.com/jmorganca/ollama?tab=readme-ov-file#pass-in-prompt-as-arguments" target="_blank" rel="noopener noreferrer">passing an argument within a prompt</a>: e.g. summarize a README page</li>
<li><a href="https://github.com/jmorganca/ollama?tab=readme-ov-file#rest-api" target="_blank" rel="noopener noreferrer">serving as a REST API</a>: e.g. chat with the model using python scripts</li>
<li><a href="https://ollama.ai/blog/ollama-is-now-available-as-an-official-docker-image" target="_blank" rel="noopener noreferrer">running as a docker image</a>: e.g. Deploy Ollama with Kubernetes</li>
</ul>
<p>The <a href="https://github.com/jmorganca/ollama" target="_blank" rel="noopener noreferrer">official Github repo README page</a> has more examples.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="some-notes">Some notes<a href="#some-notes" class="hash-link" aria-label="Direct link to Some notes" title="Direct link to Some notes">â</a></h2>
<p>After using Ollama for a weekend, I have noticed the following that may not be obvious at first glance:</p>
<ol>
<li>By hitting the <code>run</code> command, you start a chat session. This session will live until you exit from it or when you terminate process. Interestingly, chat state was not managed at the beginning, which means that you&#x27;d run into issues where <a href="https://github.com/jmorganca/ollama/issues/8" target="_blank" rel="noopener noreferrer">the model immediately forgets about the context right after a response</a>. Now, you can keep chatting and the model remembers what you entered as long as it fits within its context window. Once the context window is exceeded, Ollama will truncate the input from the beginning until it fits the context window again <a href="https://github.com/jmorganca/ollama/pull/306" target="_blank" rel="noopener noreferrer">while keeping the system instructions</a>.</li>
<li>Ollama is based on <a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer">llama.cpp</a>, an implementation of the Llama architecture in plain C/C++ without dependencies using only CPU and RAM.</li>
<li>Ollama is quite docker-like, and for me it feels intuitive. You pull models then run them. The <a href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md" target="_blank" rel="noopener noreferrer">Modelfile</a>, the &quot;blueprint to create and share models with Ollama&quot;, is also quite dockerfile-like.</li>
</ol>
<p>Overall I find Ollama quite easy to use and would likely continue to use it for something quick. It would be pretty fun if <a href="https://github.com/jmorganca/ollama/issues/142" target="_blank" rel="noopener noreferrer">conversation history can be persisted</a>!</p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ollama">ollama</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llama">llama</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/chat">chat</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">AI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ml">ML</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/chatbot">chatbot</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/local">local</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/quantized-models-dont-fit"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">When Quantized Models Still Don&#x27;t Fit</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#why-is-running-llms-locally-becoming-a-hot-thang" class="table-of-contents__link toc-highlight">Why is running LLMs locally becoming a hot thang?</a></li><li><a href="#what-is-ollama" class="table-of-contents__link toc-highlight">What is Ollama?</a></li><li><a href="#should-you-use-ollama" class="table-of-contents__link toc-highlight">Should you use Ollama?</a></li><li><a href="#how-to-get-started-with-ollama" class="table-of-contents__link toc-highlight">How to get started with Ollama</a></li><li><a href="#some-notes" class="table-of-contents__link toc-highlight">Some notes</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Social</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://twitter.com/isaacchung1217" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/isaac-chung" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2024 Isaac Chung</div></div></div></footer></div>
</body>
</html>