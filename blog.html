<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">Blog | Isaac Chung</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://isaac-chung.github.io/img/1311.jpg"><meta data-rh="true" name="twitter:image" content="https://isaac-chung.github.io/img/1311.jpg"><meta data-rh="true" property="og:url" content="https://isaac-chung.github.io/blog"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="Blog | Isaac Chung"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/1311.png"><link data-rh="true" rel="canonical" href="https://isaac-chung.github.io/blog"><link data-rh="true" rel="alternate" href="https://isaac-chung.github.io/blog" hreflang="en"><link data-rh="true" rel="alternate" href="https://isaac-chung.github.io/blog" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Isaac Chung RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Isaac Chung Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Isaac Chung JSON Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-D4GPBWLTG6"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-D4GPBWLTG6",{anonymize_ip:!0})</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.c9f2c36d.css">
<script src="/assets/js/runtime~main.e6853be3.js" defer="defer"></script>
<script src="/assets/js/main.70ba2358.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/1311.png" alt="icon" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/1311.png" alt="icon" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Isaac Chung</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/mmteb">MMTEB Massive Multilingual Text Embedding Benchmark</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/maintain-focus">Keeping up with the Joneses? Maintaining focus in AI</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/concurrent-requests">Serving Concurrent Requests with Quantized LLMs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/rag-eval-and-observability">How to really know if your RAG system is working well.</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/llm-serving">All about Timing: A quick look at metrics for LLM serving</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="https://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="Embeddings power many AI applications we interact with — search engines, RAG systems — but how do we know if they’re actually any good? Existing benchmarks tend to focus on a narrow set of tasks, often evaluating models in isolation without considering real-world, multilingual challenges. This can make it tough to figure out which models are truly effective, and where they might fall short. That&#x27;s why we need a more comprehensive way to evaluate embeddings - one that takes into account the messy, multilingual nature of real-world language use. MMTEB is designed to fill this gap, providing a broad and diverse set of evaluation tasks that can help us better understand what works, and what doesn&#x27;t, in the world of embeddings."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/mmteb">MMTEB Massive Multilingual Text Embedding Benchmark</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-03-09T00:00:00.000Z" itemprop="datePublished">March 9, 2025</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/isaac-chung.png" alt="Isaac Chung" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Isaac Chung</span></a></div><small class="avatar__subtitle" itemprop="description">Staff Machine Learning Scientist @ Zendesk QA</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Embeddings power many AI applications we interact with — search engines, RAG systems — but how do we know if they’re actually any good? Existing benchmarks tend to focus on a narrow set of tasks, often evaluating models in isolation without considering real-world, multilingual challenges. This can make it tough to figure out which models are truly effective, and where they might fall short. That&#x27;s why we need a more comprehensive way to evaluate embeddings - one that takes into account the messy, multilingual nature of real-world language use. MMTEB is designed to fill this gap, providing a broad and diverse set of evaluation tasks that can help us better understand what works, and what doesn&#x27;t, in the world of embeddings.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I&#x27;ll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>What is MMTEB?</li>
<li>What are the key takeaways from MMTEB?</li>
<li>How can I use MMTEB?</li>
</ul></div></div></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mmteb">mmteb</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mteb">mteb</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/embedding">embedding</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/benchmark">benchmark</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about MMTEB Massive Multilingual Text Embedding Benchmark" href="/blog/mmteb"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="AI is a fast moving field. This applies for both products and academia. There are new papers on NLP/LLMs/CV/ML coming out almost every day, and there are no shortage of new products and companies popping up on our social feeds. How do we know if our time is well spent focusing on one topic? Would it be better to &quot;keep up with the Joneses&quot; and explore every new development as they come?"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/maintain-focus">Keeping up with the Joneses? Maintaining focus in AI</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-05-25T00:00:00.000Z" itemprop="datePublished">May 25, 2024</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/isaac-chung.png" alt="Isaac Chung" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Isaac Chung</span></a></div><small class="avatar__subtitle" itemprop="description">Staff Machine Learning Scientist @ Zendesk QA</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>AI is a fast moving field. This applies for both products and academia. There are new papers on NLP/LLMs/CV/ML coming out almost every day, and there are no shortage of new products and companies popping up on our social feeds. How do we know if our time is well spent focusing on one topic? Would it be better to &quot;keep up with the Joneses&quot; and explore every new development as they come?</p>
<p>This blog isn&#x27;t technical. Instead, I aim to start a conversation around maintaining focus.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I&#x27;ll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>Should we chase every new AI innovation?</li>
<li>Without focus, what are the chances of our research succeeding in the long term?</li>
<li>How do we balance research and commercial needs?</li>
</ul></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="should-we-chase-every-new-ai-innovation">Should we chase every new AI innovation?<a href="#should-we-chase-every-new-ai-innovation" class="hash-link" aria-label="Direct link to Should we chase every new AI innovation?" title="Direct link to Should we chase every new AI innovation?">​</a></h2>
<p>I can imagine that many of you are also struggling with this question. Keeping up with every new development demands a lot of time and resources, while ignoring them might leave us behind, making our knowledge obsolete.  However, having a dedicated focus makes it easy to 1) dive deeper into the topic and 2) limit the need to stay current on everything. This allows for deeper progress in select areas but might mean we don&#x27;t stay current with every new trend. By chasing the latest trends, we risk spreading ourselves too thin and losing focus.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="without-focus-what-are-the-chances-of-our-research-succeeding-in-the-long-term">Without focus, what are the chances of our research succeeding in the long term?<a href="#without-focus-what-are-the-chances-of-our-research-succeeding-in-the-long-term" class="hash-link" aria-label="Direct link to Without focus, what are the chances of our research succeeding in the long term?" title="Direct link to Without focus, what are the chances of our research succeeding in the long term?">​</a></h2>
<p>ML research teams usually have fairly long term focuses, which are already aligned with clearly defined commercial needs. But that&#x27;s not always the case. Teams often are required to manage demands like supporting product releases and handling multiple lines of work.</p>
<p>But what&#x27;s the impact of a lack of focus in research?</p>
<ol>
<li>
<p>Loss of continuity: Regularly switching focus means you might not spend enough time on one topic to make substantial progress. You might not complete you work, or have time to develop as deep an understanding as you&#x27;d like. Alternatively, you might keep returning to the work and have to suffer through repeated starting phases, which are often less productive than the middle and end stages of research.</p>
</li>
<li>
<p>Resource dilution: Dividing your limited time and cognitive resources across multiple topics prevents deep dives into any one area. Shallow work in multiple areas is less likely to yield significant breakthroughs compared to sustained deep work in one area.</p>
</li>
<li>
<p>Goal fragmentation: Achieving long-term goals requires sustained effort and clear, consistent objectives. Frequent changes can fragment your goals and dilute the clarity of your research path.</p>
</li>
<li>
<p>Building on results: Research often builds incrementally, where each phase relies on the outcomes of the previous phase. Without cumulative progress, your research might lack the depth and evolution necessary for significant discoveries.</p>
</li>
<li>
<p>Team dynamics: Consistent focus areas facilitate better collaboration and synergy within your team, while frequent changes can be confusing and demotivating. This can lead to a loss in productivity and innovation.</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-we-balance-research-and-business-needs">How do we balance research and business needs?<a href="#how-do-we-balance-research-and-business-needs" class="hash-link" aria-label="Direct link to How do we balance research and business needs?" title="Direct link to How do we balance research and business needs?">​</a></h2>
<p>Business needs could be a product release or a service delivery, whereas research needs could be to investigate emerging technologies and techniques that may not have immediate commercial applications.</p>
<p>To balance these two needs we could:</p>
<ol>
<li>
<p>Set clear, multi-purpose goals: Try to align our research objectives with commercial goals where possible. Identify areas where advancing academic knowledge can directly contribute to product improvements or innovations.</p>
</li>
<li>
<p>Leverage external resources: Collaborate with academic institutions to stay on top of research while sharing the burden of exploratory work. Use conferences, publications, and peer reviews to gain feedback on our progress.</p>
</li>
<li>
<p>Secure support and resources: Ensure that stakeholders understand the value of long-term research and support us with appropriate resources and funding. We want to avoid research projects being overshadowed by immediate commercial demands.</p>
</li>
<li>
<p>Understand core competencies: Play to our strengths and understand when to outsource and find ways to compliment the teams&#x27; weaknesses.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="speed-vs-quality">Speed vs quality<a href="#speed-vs-quality" class="hash-link" aria-label="Direct link to Speed vs quality" title="Direct link to Speed vs quality">​</a></h3>
<p>Balance requires tradeoffs. While some may prefer to work quickly and chase multiple goals simultaneously, it&#x27;s crucial to balance urgency with the need for quality and thoroughness in both research and product development. Rushed commercialization can lead to:</p>
<ul>
<li>Incomplete solutions: Products that fail to fully address the intended problem or meet user needs.</li>
<li>Technical debt: Issues that accumulate over time, requiring significant resources to resolve later.</li>
<li>Brand reputation: Potential harm to the company’s reputation if the product performs poorly.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>The decision to chase every new innovation or maintain focused efforts is a constant dilemma.
Ultimately, finding this equilibrium is crucial to ensure that we can drive meaningful progress while escaping the pitfalls of rushed commercialization. Let me know in the comments what you think and your experience with keeping on top of the latest developments.</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="Being able to serve concurrent LLM generation requests are crucial to production LLM applications that have multiple users. I recently gave a talk at PyCon Lithuania on serving quantized LLMs with llama-cpp-python, an open source python library that helps serve quantized models in the GGUF format. At the end, a question came from the audience about supporting multiple users and concurrent requests. I decided to take a deeper look into why the library wasn&#x27;t able to support that at the time."><link itemprop="image" href="https://isaac-chung.github.io/assets/images/octopus-69454e9dcdc193dfb7a6f3ba4040312b.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/concurrent-requests">Serving Concurrent Requests with Quantized LLMs</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-04-13T00:00:00.000Z" itemprop="datePublished">April 13, 2024</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/isaac-chung.png" alt="Isaac Chung" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Isaac Chung</span></a></div><small class="avatar__subtitle" itemprop="description">Staff Machine Learning Scientist @ Zendesk QA</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Being able to serve concurrent LLM generation requests are crucial to production LLM applications that have multiple users. <a href="https://pycon.lt/2024/talks/DHBLXW" target="_blank" rel="noopener noreferrer">I recently gave a talk at PyCon Lithuania</a> on serving quantized LLMs with <a href="https://github.com/abetlen/llama-cpp-python" target="_blank" rel="noopener noreferrer">llama-cpp-python</a>, an open source python library that helps serve quantized models in the GGUF format. At the end, a question came from the audience about supporting multiple users and concurrent requests. I decided to take a deeper look into why the library wasn&#x27;t able to support that at the time.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I&#x27;ll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>What are the challenges of serving concurrent requests with LLMs?</li>
<li>How to serve concurrent requests with quantized LLMs?</li>
</ul></div></div></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/concurrent">concurrent</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/requests">requests</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/parallel">parallel</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/multi-user">multi-user</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Serving Concurrent Requests with Quantized LLMs" href="/blog/concurrent-requests"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="We know that building a Retrieval Augmented Generation (RAG) proof of concept is easy, but making it production-ready can be hard. There are no shortage of tips and tricks out there for us to try, but at the end of the day, it all depends on our data and our application. Transitioning RAG into production follows similar principles to other production systems. Scaling up to handle more data and users, smooth error/exception handling, and getting it to play nice with other systems are some of the main challenges to tackle. How can we really know if our RAG system is working well? and how well? To find out, we should take a look at each component under the hood and be able to evaluate the pipeline with clear metrics."><link itemprop="image" href="https://isaac-chung.github.io/assets/images/books-and-robot-1c8bc8b9accc3ef2e643d2d0e5e005da.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/rag-eval-and-observability">How to really know if your RAG system is working well.</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-03-24T00:00:00.000Z" itemprop="datePublished">March 24, 2024</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/isaac-chung.png" alt="Isaac Chung" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Isaac Chung</span></a></div><small class="avatar__subtitle" itemprop="description">Staff Machine Learning Scientist @ Zendesk QA</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>We know that building a Retrieval Augmented Generation (RAG) proof of concept is easy, but making it production-ready can be hard. There are no shortage of tips and tricks out there for us to try, but at the end of the day, it all depends on our data and our application. Transitioning RAG into production follows similar principles to other production systems. Scaling up to handle more data and users, smooth error/exception handling, and getting it to play nice with other systems are some of the main challenges to tackle. How can we really know if our RAG system is working well? and how well? To find out, we should take a look at each component under the hood and be able to evaluate the pipeline with clear metrics.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I&#x27;ll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>How to look under the hood in a RAG system?</li>
<li>How to evaluate RAG systems?</li>
</ul></div></div></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/rag">RAG</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/evaluation">Evaluation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/tracing">Tracing</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/logging">Logging</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/observability">Observability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about How to really know if your RAG system is working well." href="/blog/rag-eval-and-observability"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="How do you measure the performance of LLM serving systems? Production services in engineering are often evaluated using metrics like Requests Per Second (RPS), uptime, and latency. In computer vision systems, Frames Per Second (FPS) is often used as the main model throughput metric for use cases that involve near-real-time detection and tracking. Does serving LLMs have something similar? Certainly. A recent conversation with my team (after they read my Ollama blog) got me thinking about additional metrics that we could be tracking."><link itemprop="image" href="https://isaac-chung.github.io/assets/images/coach-19431ad943236168227c2f70e17605bd.jpg"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/llm-serving">All about Timing: A quick look at metrics for LLM serving</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-01-21T00:00:00.000Z" itemprop="datePublished">January 21, 2024</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/isaac-chung.png" alt="Isaac Chung" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Isaac Chung</span></a></div><small class="avatar__subtitle" itemprop="description">Staff Machine Learning Scientist @ Zendesk QA</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>How do you measure the performance of LLM serving systems? Production services in engineering are often evaluated using metrics like Requests Per Second (RPS), uptime, and latency. In computer vision systems, Frames Per Second (FPS) is often used as the main model throughput metric for use cases that involve near-real-time detection and tracking. Does serving LLMs have something similar? Certainly. A recent conversation with my team (after they read my <a href="/blog/what-is-ollama">Ollama blog</a>) got me thinking about additional metrics that we could be tracking.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I&#x27;ll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>What metrics does Ollama provide?</li>
<li>Why can&#x27;t I just use tokens per second?</li>
<li>What other LLM serving metrics should I consider?</li>
</ul></div></div></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/metrics">metrics</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/serving">serving</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/timing">timing</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">AI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about All about Timing: A quick look at metrics for LLM serving" href="/blog/llm-serving"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That&#x27;s what happened to me when I was trying to run Mixtral-8x7B with Ollama (check out this previous blog post on what Ollama is). The model requires 26GB of RAM while my laptop only has 16GB. I&#x27;ll try to walk through the workaround a bit at a time (pun intended)."><link itemprop="image" href="https://isaac-chung.github.io/assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/quantized-models-dont-fit">When Quantized Models Still Don&#x27;t Fit</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-01-14T00:00:00.000Z" itemprop="datePublished">January 14, 2024</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/isaac-chung.png" alt="Isaac Chung" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Isaac Chung</span></a></div><small class="avatar__subtitle" itemprop="description">Staff Machine Learning Scientist @ Zendesk QA</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That&#x27;s what happened to me when I was trying to run <a href="https://ollama.ai/library/mixtral" target="_blank" rel="noopener noreferrer">Mixtral-8x7B with Ollama</a> (check out this <a href="/blog/what-is-ollama">previous blog post on what Ollama is</a>). The model requires 26GB of RAM while my laptop only has 16GB. I&#x27;ll try to walk through the workaround a <em>bit</em> at a time (pun intended).</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I&#x27;ll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>What is quantization?</li>
<li>What is offloading?</li>
<li>How to run Mixtral-8x7B for free?</li>
</ul></div></div></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/quantization">quantization</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mixed">mixed</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">AI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ml">ML</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/chatbot">chatbot</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mixtral">mixtral</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about When Quantized Models Still Don&#x27;t Fit" href="/blog/quantized-models-dont-fit"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="Being able to run LLMs locally and easily is truly a game changer. I have heard about Ollama before and decided to take a look at it this past weekend."><link itemprop="image" href="https://isaac-chung.github.io/assets/images/tiny-llama-eab3b5a919157baa1f102158a2858f98.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/what-is-ollama">What is Ollama? A shallow dive into running LLMs locally</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-01-07T00:00:00.000Z" itemprop="datePublished">January 7, 2024</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/isaac-chung.png" alt="Isaac Chung" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://isaac-chung.github.io" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Isaac Chung</span></a></div><small class="avatar__subtitle" itemprop="description">Staff Machine Learning Scientist @ Zendesk QA</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Being able to run LLMs locally and <em>easily</em> is truly a game changer. I have heard about <a href="https://github.com/jmorganca/ollama" target="_blank" rel="noopener noreferrer">Ollama</a> before and decided to take a look at it this past weekend.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Key questions I&#x27;ll address are:</div><div class="admonitionContent_BuS1"><ul>
<li>Why is running LLMs locally becoming a hot thang</li>
<li>What is Ollama?</li>
<li>Should you use Ollama?</li>
</ul></div></div></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ollama">ollama</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llama">llama</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/chat">chat</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">AI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ml">ML</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/chatbot">chatbot</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/local">local</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about What is Ollama? A shallow dive into running LLMs locally" href="/blog/what-is-ollama"><b>Read More</b></a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Social</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://twitter.com/isaacchung1217" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/isaac-chung/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/isaac-chung" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Blog Feeds</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://isaac-chung.github.io/blog/rss.xml" target="_blank" rel="noopener noreferrer" class="footer__link-item">RSS<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://isaac-chung.github.io/blog/atom.xml" target="_blank" rel="noopener noreferrer" class="footer__link-item">Atom<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://isaac-chung.github.io/blog/feed.json" target="_blank" rel="noopener noreferrer" class="footer__link-item">JSON<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Isaac Chung</div></div></div></footer></div>
</body>
</html>