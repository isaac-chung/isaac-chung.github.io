"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[9237],{6604:(t,i,a)=>{a.r(i),a.d(i,{assets:()=>r,contentTitle:()=>o,default:()=>g,frontMatter:()=>n,metadata:()=>s,toc:()=>h});var e=a(5893),l=a(1151);a(5124),a(4720);const n={slug:"quantized-models-dont-fit",title:"When Quantized Models Still Don't Fit",authors:["ichung"],tags:["quantization","mixed","AI","LLM","ML","chatbot","mixtral"],image:"./big-apples.jpg"},o=void 0,s={permalink:"/blog/quantized-models-dont-fit",source:"@site/blog/2024-01-14-quant-models-dont-fit/index.md",title:"When Quantized Models Still Don't Fit",description:"A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That's what happened to me when I was trying to run Mixtral-8x7B with Ollama (check out this previous blog post on what Ollama is). The model requires 26GB of RAM while my laptop only has 16GB. I'll try to walk through the workaround a bit at a time (pun intended).",date:"2024-01-14T00:00:00.000Z",formattedDate:"January 14, 2024",tags:[{label:"quantization",permalink:"/blog/tags/quantization"},{label:"mixed",permalink:"/blog/tags/mixed"},{label:"AI",permalink:"/blog/tags/ai"},{label:"LLM",permalink:"/blog/tags/llm"},{label:"ML",permalink:"/blog/tags/ml"},{label:"chatbot",permalink:"/blog/tags/chatbot"},{label:"mixtral",permalink:"/blog/tags/mixtral"}],readingTime:2.533333333333333,hasTruncateMarker:!0,authors:[{name:"Isaac Chung",title:"Senior Research Engineer @ Clarifai",url:"https://isaac-chung.github.io",imageURL:"https://github.com/isaac-chung.png",key:"ichung"}],frontMatter:{slug:"quantized-models-dont-fit",title:"When Quantized Models Still Don't Fit",authors:["ichung"],tags:["quantization","mixed","AI","LLM","ML","chatbot","mixtral"],image:"./big-apples.jpg"},unlisted:!1,prevItem:{title:"All about Timing: A quick look at metrics for LLM serving",permalink:"/blog/llm-serving"},nextItem:{title:"What is Ollama? A shallow dive into running LLMs locally",permalink:"/blog/what-is-ollama"}},r={image:a(5731).Z,authorsImageUrls:[void 0]},h=[];function d(t){const i={a:"a",admonition:"admonition",em:"em",li:"li",p:"p",ul:"ul",...(0,l.a)(),...t.components};return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsxs)(i.p,{children:["A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That's what happened to me when I was trying to run ",(0,e.jsx)(i.a,{href:"https://ollama.ai/library/mixtral",children:"Mixtral-8x7B with Ollama"})," (check out this ",(0,e.jsx)(i.a,{href:"/blog/what-is-ollama",children:"previous blog post on what Ollama is"}),"). The model requires 26GB of RAM while my laptop only has 16GB. I'll try to walk through the workaround a ",(0,e.jsx)(i.em,{children:"bit"})," at a time (pun intended)."]}),"\n",(0,e.jsx)(i.admonition,{title:"Key questions I'll address are:",type:"tip",children:(0,e.jsxs)(i.ul,{children:["\n",(0,e.jsx)(i.li,{children:"What is quantization?"}),"\n",(0,e.jsx)(i.li,{children:"What is offloading?"}),"\n",(0,e.jsx)(i.li,{children:"How to run Mixtral-8x7B for free?"}),"\n"]})})]})}function g(t={}){const{wrapper:i}={...(0,l.a)(),...t.components};return i?(0,e.jsx)(i,{...t,children:(0,e.jsx)(d,{...t})}):d(t)}},5124:(t,i,a)=>{a.d(i,{Z:()=>o});var e=a(7294),l=a(9276),n=a(5893);function o(t){let{image:i,alt:o,caption:s}=t;const[r,h]=(0,e.useState)({width:0,height:0}),d=s.split("\\n").map(((t,i,a)=>(0,n.jsxs)(e.Fragment,{children:[t,i<a.length-1&&(0,n.jsx)("br",{})]},i)));return(0,e.useEffect)((()=>{const t=new Image;t.onload=()=>{h({width:t.naturalWidth,height:t.naturalHeight})},t.src=i;const e=new l.Z({gallery:"#figure-gallery",children:"a",pswpModule:()=>a.e(5826).then(a.bind(a,7766))});return e.init(),()=>{e.destroy()}}),[i]),(0,n.jsxs)("figure",{style:{border:"1px dashed rgba(0, 0, 0, .1)",padding:0,margin:0,marginBottom:20,borderRadius:"15px",textAlign:"right"},id:"figure-gallery",children:[(0,n.jsx)("a",{href:i,"data-pswp-width":r.width,"data-pswp-height":r.height,children:(0,n.jsx)("img",{src:i,alt:o,style:{maxWidth:"100%",height:"auto"}})}),(0,n.jsx)("hr",{style:{margin:"5px 0",backgroundColor:"rgba(0, 0, 0, .2)"}}),(0,n.jsx)("figcaption",{style:{marginTop:"0.5em",marginBottom:"0.5em",marginRight:"1em",textAlign:"right",fontSize:"0.8em"},children:d})]})}},5731:(t,i,a)=>{a.d(i,{Z:()=>e});const e=a.p+"assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg"},4720:(t,i,a)=>{a.d(i,{Z:()=>e});const e=a.p+"assets/images/big-apples-08d16b4bca289d87d39789f9c2d86aa5.jpg"}}]);