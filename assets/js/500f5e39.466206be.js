"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[2301],{8425:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>s,metadata:()=>o,toc:()=>g});var n=i(5893),a=i(1151);i(5124),i(1550);const s={slug:"llm-serving",title:"All about Timing: A quick look at metrics for LLM serving",authors:["ichung"],tags:["metrics","serving","timing","AI","LLM"],image:"./coach.jpg",enableComments:!0},r=void 0,o={permalink:"/blog/llm-serving",source:"@site/blog/2024-01-21-llm-serving/index.md",title:"All about Timing: A quick look at metrics for LLM serving",description:"How do you measure the performance of LLM serving systems? Production services in engineering are often evaluated using metrics like Requests Per Second (RPS), uptime, and latency. In computer vision systems, Frames Per Second (FPS) is often used as the main model throughput metric for use cases that involve near-real-time detection and tracking. Does serving LLMs have something similar? Certainly. A recent conversation with my team (after they read my Ollama blog) got me thinking about additional metrics that we could be tracking.",date:"2024-01-21T00:00:00.000Z",formattedDate:"January 21, 2024",tags:[{label:"metrics",permalink:"/blog/tags/metrics"},{label:"serving",permalink:"/blog/tags/serving"},{label:"timing",permalink:"/blog/tags/timing"},{label:"AI",permalink:"/blog/tags/ai"},{label:"LLM",permalink:"/blog/tags/llm"}],readingTime:2.4133333333333336,hasTruncateMarker:!0,authors:[{name:"Isaac Chung",title:"Senior Research Engineer @ Clarifai",url:"https://isaac-chung.github.io",imageURL:"https://github.com/isaac-chung.png",key:"ichung"}],frontMatter:{slug:"llm-serving",title:"All about Timing: A quick look at metrics for LLM serving",authors:["ichung"],tags:["metrics","serving","timing","AI","LLM"],image:"./coach.jpg",enableComments:!0},unlisted:!1,prevItem:{title:"How to really know if your RAG system is working well.",permalink:"/blog/rag-eval-and-observability"},nextItem:{title:"When Quantized Models Still Don't Fit",permalink:"/blog/quantized-models-dont-fit"}},l={image:i(1457).Z,authorsImageUrls:[void 0]},g=[];function m(e){const t={a:"a",admonition:"admonition",li:"li",p:"p",ul:"ul",...(0,a.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(t.p,{children:["How do you measure the performance of LLM serving systems? Production services in engineering are often evaluated using metrics like Requests Per Second (RPS), uptime, and latency. In computer vision systems, Frames Per Second (FPS) is often used as the main model throughput metric for use cases that involve near-real-time detection and tracking. Does serving LLMs have something similar? Certainly. A recent conversation with my team (after they read my ",(0,n.jsx)(t.a,{href:"/blog/what-is-ollama",children:"Ollama blog"}),") got me thinking about additional metrics that we could be tracking."]}),"\n",(0,n.jsx)(t.admonition,{title:"Key questions I'll address are:",type:"tip",children:(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"What metrics does Ollama provide?"}),"\n",(0,n.jsx)(t.li,{children:"Why can't I just use tokens per second?"}),"\n",(0,n.jsx)(t.li,{children:"What other LLM serving metrics should I consider?"}),"\n"]})})]})}function c(e={}){const{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(m,{...e})}):m(e)}},5124:(e,t,i)=>{i.d(t,{Z:()=>r});var n=i(7294),a=i(9276),s=i(5893);function r(e){let{image:t,alt:r,caption:o}=e;const[l,g]=(0,n.useState)({width:0,height:0}),m=o.split("\\n").map(((e,t,i)=>(0,s.jsxs)(n.Fragment,{children:[e,t<i.length-1&&(0,s.jsx)("br",{})]},t)));return(0,n.useEffect)((()=>{const e=new Image;e.onload=()=>{g({width:e.naturalWidth,height:e.naturalHeight})},e.src=t;const n=new a.Z({gallery:"#figure-gallery",children:"a",pswpModule:()=>i.e(5826).then(i.bind(i,7766))});return n.init(),()=>{n.destroy()}}),[t]),(0,s.jsxs)("figure",{style:{border:"1px dashed rgba(0, 0, 0, .1)",padding:0,margin:0,marginBottom:20,borderRadius:"15px",textAlign:"right"},id:"figure-gallery",children:[(0,s.jsx)("a",{href:t,"data-pswp-width":l.width,"data-pswp-height":l.height,children:(0,s.jsx)("img",{src:t,alt:r,style:{maxWidth:"100%",height:"auto"}})}),(0,s.jsx)("hr",{style:{margin:"5px 0",backgroundColor:"rgba(0, 0, 0, .2)"}}),(0,s.jsx)("figcaption",{style:{marginTop:"0.5em",marginBottom:"0.5em",marginRight:"1em",textAlign:"right",fontSize:"0.8em"},children:m})]})}},1457:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/coach-19431ad943236168227c2f70e17605bd.jpg"},1550:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/coach-19431ad943236168227c2f70e17605bd.jpg"}}]);