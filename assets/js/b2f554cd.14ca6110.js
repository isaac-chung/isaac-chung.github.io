"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"maintain-focus","metadata":{"permalink":"/blog/maintain-focus","source":"@site/blog/2024-05-25-focus/index.md","title":"Keeping up with the Joneses? Maintaining focus in AI","description":"AI is a fast moving field. This applies for both products and academia. There are new papers on NLP/LLMs/CV/ML coming out almost every day, and there are no shortage of new products and companies popping up on our social feeds. How do we know if our time is well spent focusing on one topic? Would it be better to \\"keep up with the Joneses\\" and explore every new development as they come?","date":"2024-05-25T00:00:00.000Z","formattedDate":"May 25, 2024","tags":[],"readingTime":2.6733333333333333,"hasTruncateMarker":false,"authors":[{"name":"Isaac Chung","title":"Senior Research Engineer @ Clarifai","url":"https://isaac-chung.github.io","imageURL":"https://github.com/isaac-chung.png","key":"ichung"}],"frontMatter":{"slug":"maintain-focus","title":"Keeping up with the Joneses? Maintaining focus in AI","authors":["ichung"],"tags":[],"enableComments":true},"unlisted":false,"nextItem":{"title":"Serving Concurrent Requests with Quantized LLMs","permalink":"/blog/concurrent-requests"}},"content":"AI is a fast moving field. This applies for both products and academia. There are new papers on NLP/LLMs/CV/ML coming out almost every day, and there are no shortage of new products and companies popping up on our social feeds. How do we know if our time is well spent focusing on one topic? Would it be better to \\"keep up with the Joneses\\" and explore every new development as they come?\\n\\nThis blog isn\'t technical. Instead, I aim to start a conversation around maintaining focus.\\n\\n:::tip[Key questions I\'ll address are:]\\n\\n- Should we chase every new AI innovation?\\n- Without focus, what are the chances of our research succeeding in the long term?\\n- How do we balance research and commercial needs?\\n\\n:::\\n\\n## Should we chase every new AI innovation?\\n\\nI can imagine that many of you are also struggling with this question. Keeping up with every new development demands a lot of time and resources, while ignoring them might leave us behind, making our knowledge obsolete.  However, having a dedicated focus makes it easy to 1) dive deeper into the topic and 2) limit the need to stay current on everything. This allows for deeper progress in select areas but might mean we don\'t stay current with every new trend. By chasing the latest trends, we risk spreading ourselves too thin and losing focus. \\n\\n## Without focus, what are the chances of our research succeeding in the long term?\\n\\nML research teams usually have fairly long term focuses, which are already aligned with clearly defined commercial needs. But that\'s not always the case. Teams often are required to manage demands like supporting product releases and handling multiple lines of work. \\n\\nBut what\'s the impact of a lack of focus in research?\\n\\n1. Loss of continuity: Regularly switching focus means you might not spend enough time on one topic to make substantial progress. You might not complete you work, or have time to develop as deep an understanding as you\'d like. Alternatively, you might keep returning to the work and have to suffer through repeated starting phases, which are often less productive than the middle and end stages of research.\\n\\n2. Resource dilution: Dividing your limited time and cognitive resources across multiple topics prevents deep dives into any one area. Shallow work in multiple areas is less likely to yield significant breakthroughs compared to sustained deep work in one area.\\n\\n3. Goal fragmentation: Achieving long-term goals requires sustained effort and clear, consistent objectives. Frequent changes can fragment your goals and dilute the clarity of your research path.\\n\\n4. Building on results: Research often builds incrementally, where each phase relies on the outcomes of the previous phase. Without cumulative progress, your research might lack the depth and evolution necessary for significant discoveries.\\n\\n5. Team dynamics: Consistent focus areas facilitate better collaboration and synergy within your team, while frequent changes can be confusing and demotivating. This can lead to a loss in productivity and innovation. \\n\\n## How do we balance research and business needs?\\nBusiness needs could be a product release or a service delivery, whereas research needs could be to investigate emerging technologies and techniques that may not have immediate commercial applications.\\n\\nTo balance these two needs we could:\\n\\n1. Set clear, multi-purpose goals: Try to align our research objectives with commercial goals where possible. Identify areas where advancing academic knowledge can directly contribute to product improvements or innovations. \\n\\n2. Leverage external resources: Collaborate with academic institutions to stay on top of research while sharing the burden of exploratory work. Use conferences, publications, and peer reviews to gain feedback on our progress. \\n\\n3. Secure support and resources: Ensure that stakeholders understand the value of long-term research and support us with appropriate resources and funding. We want to avoid research projects being overshadowed by immediate commercial demands.\\n\\n4. Understand core competencies: Play to our strengths and understand when to outsource and find ways to compliment the teams\' weaknesses. \\n\\n### Speed vs quality\\nBalance requires tradeoffs. While some may prefer to work quickly and chase multiple goals simultaneously, it\'s crucial to balance urgency with the need for quality and thoroughness in both research and product development. Rushed commercialization can lead to:\\n\\n- Incomplete solutions: Products that fail to fully address the intended problem or meet user needs.\\n- Technical debt: Issues that accumulate over time, requiring significant resources to resolve later.\\n- Brand reputation: Potential harm to the company\u2019s reputation if the product performs poorly.\\n\\n## Conclusion\\nThe decision to chase every new innovation or maintain focused efforts is a constant dilemma.\\nUltimately, finding this equilibrium is crucial to ensure that we can drive meaningful progress while escaping the pitfalls of rushed commercialization. Let me know in the comments what you think and your experience with keeping on top of the latest developments."},{"id":"concurrent-requests","metadata":{"permalink":"/blog/concurrent-requests","source":"@site/blog/2024-04-13-concurrent-requests/index.md","title":"Serving Concurrent Requests with Quantized LLMs","description":"Being able to serve concurrent LLM generation requests are crucial to production LLM applications that have multiple users. I recently gave a talk at PyCon Lithuania on serving quantized LLMs with llama-cpp-python, an open source python library that helps serve quantized models in the GGUF format. At the end, a question came from the audience about supporting multiple users and concurrent requests. I decided to take a deeper look into why the library wasn\'t able to support that at the time.","date":"2024-04-13T00:00:00.000Z","formattedDate":"April 13, 2024","tags":[{"label":"LLM","permalink":"/blog/tags/llm"},{"label":"concurrent","permalink":"/blog/tags/concurrent"},{"label":"requests","permalink":"/blog/tags/requests"},{"label":"parallel","permalink":"/blog/tags/parallel"},{"label":"multi-user","permalink":"/blog/tags/multi-user"}],"readingTime":2.5033333333333334,"hasTruncateMarker":true,"authors":[{"name":"Isaac Chung","title":"Senior Research Engineer @ Clarifai","url":"https://isaac-chung.github.io","imageURL":"https://github.com/isaac-chung.png","key":"ichung"}],"frontMatter":{"slug":"concurrent-requests","title":"Serving Concurrent Requests with Quantized LLMs","authors":["ichung"],"tags":["LLM","concurrent","requests","parallel","multi-user"],"image":"./octopus.png","enableComments":true},"unlisted":false,"prevItem":{"title":"Keeping up with the Joneses? Maintaining focus in AI","permalink":"/blog/maintain-focus"},"nextItem":{"title":"How to really know if your RAG system is working well.","permalink":"/blog/rag-eval-and-observability"}},"content":"import Figure from \'@site/src/components/figure\';\\nimport octopus from \'./octopus.png\';\\n\\nBeing able to serve concurrent LLM generation requests are crucial to production LLM applications that have multiple users. [I recently gave a talk at PyCon Lithuania](https://pycon.lt/2024/talks/DHBLXW) on serving quantized LLMs with [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), an open source python library that helps serve quantized models in the GGUF format. At the end, a question came from the audience about supporting multiple users and concurrent requests. I decided to take a deeper look into why the library wasn\'t able to support that at the time.\\n\\n:::tip[Key questions I\'ll address are:]\\n\\n- What are the challenges of serving concurrent requests with LLMs?\\n- How to serve concurrent requests with quantized LLMs?\\n\\n:::\\n\\n\x3c!-- truncate --\x3e\\n\\n<Figure\\n  image={octopus}\\n  alt=\\"New yorker style comic depicting a cute, friendly cartoon octopus with a baseball cap holding multiple tennis rackets. White background only.\\"\\n  caption=\\"Image by Dalle3.\\"\\n/>\\n\\n## What are the challenges of serving concurrent requests with LLMs?\\nLLM inference involves generating tokens in an autoregressive manner. To avoid repeating calculations when generating future tokens, a KV cache is used. \\nThe KV cache size grows quickly with the number of requests. The [vLLM paper](https://arxiv.org/abs/2309.06180) uses the 13B OPT model as an example:\\n> the KV cache of a single token demands 800 KB of space, calculated as 2 (key and value vectors) \xd7 5120 (hidden state size) \xd7 40 (number of layers) \xd7 2 (bytes per FP16). Since OPT can generate sequences up to 2048 tokens, the memory required to store the KV cache of one request can be as much as 1.6 GB\\n\\nIt\'s clear that this is a memory-bound process. In the context of serving requests, we can batch multiple incoming requests to improve compute utilization. Batching isn\'t trivial either. The main challenges includes:\\n1. The requests may arrive at different times, and\\n2. the requests may have very different input and output lengths.\\n\\nBatching requests naively may lead to huge delays from waiting for earlier requests to finish before starting the next batch, or waiting for the longest generation to finish. It would also lead to computation and memory wastage from padding inputs/outputs due to their difference in lengths. \\n\\nBack to the question from the talk. At the time of writing, llama-cpp-python did not support batched requests. Moreover, concurrent requests would [lead to the server crashing](https://www.reddit.com/r/LocalLLaMA/comments/15kbbna/how_to_make_multiple_inference_requests_from_a/). This might be due to the locks being used to control access to shared resources, particularly the `llama_proxy` variable, which handles the model resources. This means that to serve parallel requests, multiple instances of the models may be needed, and this drastically increases the resources required for model serving. \\n\\n## How to serve concurrent requests with quantized LLMs?\\nImprovements have been introduced to make serving concurrent requests more efficient. These include and are not limited to:\\n1. **Continuous Batching**: It allows new requests to join the current batch in the next decoder cycle instead of waiting for the end of the current batch to finish. This improves throughput and compute utilization. The upstream [llama.cpp repo](https://github.com/ggerganov/llama.cpp/tree/master/examples/server) has the capability to serve parallel requests with continuous batching. This, however, does not entirely solve the memory issue. We still need to reserve memory using the longest sequence in the batch. \\n2. **Paged Attention**: It divides up the KV cache into blocks that would contain keys and values for a fixed number of tokens. It eliminates external fragmentation (unusable gaps between allocated memory blocks in a GPU\'s memory) since all blocks have the same size. Also, it eases internal fragmentation by using relatively small blocks. An LLM serving engine that\'s built on top of Paged Attention is [vLLM](https://github.com/vllm-project/vllm). \\n\\n## What is vLLM?\\nvLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. vLLM supports serving quantized LLMs with concurrent requests with AWQ (4-bit) models, which boasts a 2.78x speed up on a single GPU according to [a benchmark done by lightning.ai](https://lightning.ai/lightning-ai/studios/optimized-llm-inference-api-for-mistral-7b-using-vllm). \\n\\n## Final Thoughts\\nTools like llama-cpp-python and ExLlama2 may not have been designed to be [an efficient backend for large deployments](https://github.com/turboderp/exllamav2/issues/95#issuecomment-2019991153), but they are certainly worth a look for anyone who wants to deploy way smaller models on a budget. vLLM on the other hand seem to have that in mind from the beginning. It can even be used with the [triton inference server as a supported backend](https://github.com/triton-inference-server/vllm_backend). \\n\\n## Further Reading\\n- Check out [this YouTube channel](https://www.youtube.com/channel/UCtAcpQcYerN8xxZJYTfWBMw/videos) for in-depth explanations about popular transformer/LLM architectures. It helped me get through some dense materials clearly, so thank you, Umar!\\n- This blog on [Serving LLMs by run.ai](https://www.run.ai/blog/serving-large-language-models)."},{"id":"rag-eval-and-observability","metadata":{"permalink":"/blog/rag-eval-and-observability","source":"@site/blog/2024-03-24-rag-eval-and-observability/index.md","title":"How to really know if your RAG system is working well.","description":"We know that building a Retrieval Augmented Generation (RAG) proof of concept is easy, but making it production-ready can be hard. There are no shortage of tips and tricks out there for us to try, but at the end of the day, it all depends on our data and our application. Transitioning RAG into production follows similar principles to other production systems. Scaling up to handle more data and users, smooth error/exception handling, and getting it to play nice with other systems are some of the main challenges to tackle. How can we really know if our RAG system is working well? and how well? To find out, we should take a look at each component under the hood and be able to evaluate the pipeline with clear metrics.","date":"2024-03-24T00:00:00.000Z","formattedDate":"March 24, 2024","tags":[{"label":"RAG","permalink":"/blog/tags/rag"},{"label":"LLM","permalink":"/blog/tags/llm"},{"label":"Evaluation","permalink":"/blog/tags/evaluation"},{"label":"Tracing","permalink":"/blog/tags/tracing"},{"label":"Logging","permalink":"/blog/tags/logging"},{"label":"Observability","permalink":"/blog/tags/observability"}],"readingTime":3.1233333333333335,"hasTruncateMarker":true,"authors":[{"name":"Isaac Chung","title":"Senior Research Engineer @ Clarifai","url":"https://isaac-chung.github.io","imageURL":"https://github.com/isaac-chung.png","key":"ichung"}],"frontMatter":{"slug":"rag-eval-and-observability","title":"How to really know if your RAG system is working well.","authors":["ichung"],"tags":["RAG","LLM","Evaluation","Tracing","Logging","Observability"],"image":"./books-and-robot.png","enableComments":true},"unlisted":false,"prevItem":{"title":"Serving Concurrent Requests with Quantized LLMs","permalink":"/blog/concurrent-requests"},"nextItem":{"title":"All about Timing: A quick look at metrics for LLM serving","permalink":"/blog/llm-serving"}},"content":"import Figure from \'@site/src/components/figure\';\\nimport books from \'./books-and-robot.png\';\\n\\nWe know that building a Retrieval Augmented Generation (RAG) proof of concept is easy, but making it production-ready can be hard. There are no shortage of tips and tricks out there for us to try, but at the end of the day, it all depends on our data and our application. Transitioning RAG into production follows similar principles to other production systems. Scaling up to handle more data and users, smooth error/exception handling, and getting it to play nice with other systems are some of the main challenges to tackle. How can we really know if our RAG system is working well? and how well? To find out, we should take a look at each component under the hood and be able to evaluate the pipeline with clear metrics. \\n\\n:::tip[Key questions I\'ll address are:]\\n\\n- How to look under the hood in a RAG system?\\n- How to evaluate RAG systems?\\n\\n:::\\n\\n\x3c!-- truncate --\x3e\\n\\n<Figure\\n  image={books}\\n  alt=\\"A robot sitting on a large pile of books.\\"\\n  caption=\\"Image by Stable Diffusion XL.\\"\\n/>\\n\\n## How to look under the hood in a RAG system?\\nOnce the components are set up in the RAG system, it is tempting to spot-check it for performance, and try out some _advanced techniques_ with the promise of performance improvements. However, this isn\'t the most reliable nor structural approach to debugging and improving RAG. The first thing we should do after getting our first end-to-end RAG response is adding observability. This greatly helps us not only during the transition of our RAG system from POC to production but also in its post-launch maintenance phase.\\n\\nObservability is crucial in RAG production systems for several main reasons:\\n1. **Detecting Issues**: Observability allows for the detection of issues and anomalies within a system. By monitoring various metrics, logs, and traces, we can quickly identify when something goes wrong and take appropriate action to resolve the issue before it impacts users.\\n2. **Root Cause Analysis**: When problems occur, especially during the development phase, observability enables us to perform root cause analysis efficiently. By examining the data collected from various components, we can trace back the source of the problem and address it effectively. More important, in production this would help reduce downtime and minimizing the impact on users.\\n3. **Performance Optimization**: Observability provides insights into the performance of the system. By monitoring metrics such as response times, throughput, and resource utilization, we can identify bottlenecks and areas for optimization, leading to better overall performance and user experience.\\n\\nThis could be as simple as logging inputs and outputs of each component (e.g. simple setting in [llama-index](https://docs.llamaindex.ai/en/stable/module_guides/observability/#simple-llm-inputsoutputs)). There are a variety of LLM observability tools to help trace the timings and outputs at each step of a RAG system. Some of these have minimal config needed, have no pricing page, and are open source, and they are:\\n- [OpenLLMetry](https://github.com/traceloop/openllmetry): Built on top of OpenTelemetry. If you\u2019re using an LLM framework like Haystack, Langchain or LlamaIndex, there is no need to add any annotations to your code.\\n- [Arize Phoenix](https://github.com/Arize-ai/phoenix): Built on top of the OpenInference tracing standard, and uses it to trace, export, and collect critical information about your LLM Application in the form of \\"spans\\". It also supports several RAG-related analyses and visualizations.\\n\\n## How to evaluate RAG systems?\\nJust like any system, it is important to understand how well the RAG system is performing and how much improvement has been achieved over the baseline. This doesn\u2019t just involve measuring how fast and how much it costs, but also how good the outputs are. We could take a look at RAG-specific evaluation methods. Per-component evaluations, like unit tests, can be done on the retrieval stage and the generation stage separately. \\n\\nFor retrieval, the goal is to find out given the configuration how well can the system retrieve relevant results? Here you would need a golden set of queries and ground truth of relevant documents (or their IDs).  You could use IR metrics like nDCG or Mean Reciprocal Rank (MRR), but for RAG it\u2019s more meaningful to understand 1) the signal to noise ratio of the retrieved context (context precision) and 2) how well it can retrieve all the relevant information required to answer the question (context recall). \\n\\nFor generation, the goal is to find out, given the relevant documents in the context, 1) how factually accurate is the generated answer (faithfulness), and 2) how relevant is the generated answer to the question (answer relevancy). It is also important to evaluate the full pipeline end to end. This might involve some manual efforts to start with or asking an LLM to verify whether the answer is correct. A proxy for gauging how close the generated answer to the ground truth answer could be semantic similarity. \\n\\nSome open source RAG evaluation tools like [Ragas](https://github.com/explodinggradients/ragas) offer readily available guide to evaluate your RAG system with predefined metrics and iterate your RAG system with user feedback in production. Ragas, in particular, offers the ability generate a synthetic test set for \u201creference-free\u201d evaluation, which means that instead of relying on human-annotated test set, Ragas leverages LLMs under the hood to conduct the evaluations.\\n\\n\\n## The Bottom Line\\nFight the urge of treating the RAG system as a black box. Use a structured approach to evaluate your RAG system in terms of performance and other requirements like latency by adding observability and using evaluation tools. \\n\\n## Further Reading\\n- Catch this talk on [\\"Transcend the Knowledge Barriers in RAG\\"](https://pycon.lt/2024/talks/HFXHRV) at PyCon Lithuania to understand how each RAG component works under the hood.\\n- [12 RAG Pain points](https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c)"},{"id":"llm-serving","metadata":{"permalink":"/blog/llm-serving","source":"@site/blog/2024-01-21-llm-serving/index.md","title":"All about Timing: A quick look at metrics for LLM serving","description":"How do you measure the performance of LLM serving systems? Production services in engineering are often evaluated using metrics like Requests Per Second (RPS), uptime, and latency. In computer vision systems, Frames Per Second (FPS) is often used as the main model throughput metric for use cases that involve near-real-time detection and tracking. Does serving LLMs have something similar? Certainly. A recent conversation with my team (after they read my Ollama blog) got me thinking about additional metrics that we could be tracking.","date":"2024-01-21T00:00:00.000Z","formattedDate":"January 21, 2024","tags":[{"label":"metrics","permalink":"/blog/tags/metrics"},{"label":"serving","permalink":"/blog/tags/serving"},{"label":"timing","permalink":"/blog/tags/timing"},{"label":"AI","permalink":"/blog/tags/ai"},{"label":"LLM","permalink":"/blog/tags/llm"}],"readingTime":2.4133333333333336,"hasTruncateMarker":true,"authors":[{"name":"Isaac Chung","title":"Senior Research Engineer @ Clarifai","url":"https://isaac-chung.github.io","imageURL":"https://github.com/isaac-chung.png","key":"ichung"}],"frontMatter":{"slug":"llm-serving","title":"All about Timing: A quick look at metrics for LLM serving","authors":["ichung"],"tags":["metrics","serving","timing","AI","LLM"],"image":"./coach.jpg","enableComments":true},"unlisted":false,"prevItem":{"title":"How to really know if your RAG system is working well.","permalink":"/blog/rag-eval-and-observability"},"nextItem":{"title":"When Quantized Models Still Don\'t Fit","permalink":"/blog/quantized-models-dont-fit"}},"content":"import Figure from \'@site/src/components/figure\';\\nimport coach from \'./coach.jpg\';\\n\\n\\nHow do you measure the performance of LLM serving systems? Production services in engineering are often evaluated using metrics like Requests Per Second (RPS), uptime, and latency. In computer vision systems, Frames Per Second (FPS) is often used as the main model throughput metric for use cases that involve near-real-time detection and tracking. Does serving LLMs have something similar? Certainly. A recent conversation with my team (after they read my [Ollama blog](/blog/what-is-ollama)) got me thinking about additional metrics that we could be tracking. \\n\\n:::tip[Key questions I\'ll address are:]\\n\\n- What metrics does Ollama provide?\\n- Why can\'t I just use tokens per second?\\n- What other LLM serving metrics should I consider?\\n\\n:::\\n\\n\x3c!-- truncate --\x3e\\n\\n<Figure\\n  image={coach}\\n  alt=\\"A baseball coach blowing a whistle while holding a stopwatch.\\"\\n  caption=\\"Image by OpenAI DALL-E 3.\\"\\n/>\\n\\n## What metrics does Ollama provide?\\nTo see the metrics for each response Ollama provides, add the `--verbose` flag after the run command. e.g. `ollama run llama2 --verbose`. Here is an example output with 8 different metrics. \\n```\\ntotal duration:       58.502942674s\\nload duration:        7.185349ms\\nprompt eval count:    31 token(s)\\nprompt eval duration: 4.044684s\\nprompt eval rate:     7.66 tokens/s\\neval count:           266 token(s)\\neval duration:        54.44846s\\neval rate:            4.89 tokens/s\\n```\\n\\nI added some indentation and rearranged it a bit so that it\'s easier to discern which parts are together:\\n```\\ntotal duration:       58.502942674s\\n    1. load duration:        7.185349ms\\n    2. prompt eval duration: 4.044684s\\n        prompt eval count:    31 token(s)\\n        prompt eval rate:     7.66 tokens/s\\n    3. eval duration:        54.44846s\\n        eval count:           266 token(s)\\n        eval rate:            4.89 tokens/s\\n```\\n\\nTotal duration can be seen as latency (the overall time from receiving a request to returning a response to the user). This metric often includes all of the overhead in addition to the time a model needs to generate a response, e.g. model load time. \\n\\nLoad duration is the time taken to load the model into memory. Prompt eval is the stage of processing the input prompt. Eval is the stage of generating output.\\n\\nTokens per second is a common metric to use for output generation. Looking at the eval rate, this system achieved 4.89 \\"(output) tokens per second\\". For comparison, [7-10 tokens/second is thought to be acceptable for general use](https://www.reddit.com/r/LocalLLaMA/comments/162pgx9/what_do_yall_consider_acceptable_tokens_per/).\\n\\n\\n## Why can\'t I just use tokens per second?\\n\\nA single metric is rarely enough to capture the whole picture. It\'s important to note that all tokens are not made equal. For example, [Llama2 tokenization is 19% longer than that of ChatGPT](https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper), yet it\'s still much cheaper. This should be considered when serving LLMs of different families.\\n\x3c!-- There are other factors that may play a role, e.g. while producing more output tokens leads to significantly higher latency than adding more input tokens  --\x3e\\n\\nAlso, the inference system is often used by more than one user at a time (multiple concurrent users). LLMs likely power a bigger system, such as a chat service. You might want to give users a good experience by considering tokens per second _per user_, or tokens per second at different request rates.\\n\\n## What other LLM serving metrics should I consider?\\nA common metric to use is **Time To First Token (TTFT)**, which is how quickly users start seeing model outputs after a request is sent. Low waiting times are important in real-time interactions, but less so in offline workloads. This metric measures the time required to a) process the prompt and then b) generate the first output token. From the example above, the TTFT is just over 4 seconds as the load time was almost negligible.\\n\\nAnother way to represent tokens per second is its inverse: **Time Per Output Token (TPOT)**, the time to generate one output token for each request. This metric can be used to perceive the \\"speed\\" of the model. From the example above, the TPOT is 200ms per token.\\n\\n## Final Thought (yes, just one)\\nOverall, it\'s important to keep the goal of the system and user requirements in mind and avoid blindly optimizing metrics for optimization\'s sake. \\n\\n## Further Reading\\nHere are some extra resources for some background on how LLMs generate text and deeper dives on LLM serving:\\n1. [How LLMs use decoder blocks for generation](https://jalammar.github.io/illustrated-gpt2/#part-1-got-and-language-modeling) with illustrations\\n2. [Best Practices for LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) from MosaicML\\n3. [Faster Mixtral inference with TensorRT-LLM](https://www.baseten.co/blog/faster-mixtral-inference-with-tensorrt-llm-and-quantization/) by Baseten"},{"id":"quantized-models-dont-fit","metadata":{"permalink":"/blog/quantized-models-dont-fit","source":"@site/blog/2024-01-14-quant-models-dont-fit/index.md","title":"When Quantized Models Still Don\'t Fit","description":"A key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That\'s what happened to me when I was trying to run Mixtral-8x7B with Ollama (check out this previous blog post on what Ollama is). The model requires 26GB of RAM while my laptop only has 16GB. I\'ll try to walk through the workaround a bit at a time (pun intended).","date":"2024-01-14T00:00:00.000Z","formattedDate":"January 14, 2024","tags":[{"label":"quantization","permalink":"/blog/tags/quantization"},{"label":"mixed","permalink":"/blog/tags/mixed"},{"label":"AI","permalink":"/blog/tags/ai"},{"label":"LLM","permalink":"/blog/tags/llm"},{"label":"ML","permalink":"/blog/tags/ml"},{"label":"chatbot","permalink":"/blog/tags/chatbot"},{"label":"mixtral","permalink":"/blog/tags/mixtral"}],"readingTime":2.533333333333333,"hasTruncateMarker":true,"authors":[{"name":"Isaac Chung","title":"Senior Research Engineer @ Clarifai","url":"https://isaac-chung.github.io","imageURL":"https://github.com/isaac-chung.png","key":"ichung"}],"frontMatter":{"slug":"quantized-models-dont-fit","title":"When Quantized Models Still Don\'t Fit","authors":["ichung"],"tags":["quantization","mixed","AI","LLM","ML","chatbot","mixtral"],"image":"./big-apples.jpg","enableComments":true},"unlisted":false,"prevItem":{"title":"All about Timing: A quick look at metrics for LLM serving","permalink":"/blog/llm-serving"},"nextItem":{"title":"What is Ollama? A shallow dive into running LLMs locally","permalink":"/blog/what-is-ollama"}},"content":"import Figure from \'@site/src/components/figure\';\\nimport bigApples from \'./big-apples.jpg\';\\n\\n\\nA key ingredient to running LLMs locally (read: without high-end GPUs, as in multiple) is quantization. What do you do when the 4-bit quantized model is still too big for your machine? That\'s what happened to me when I was trying to run [Mixtral-8x7B with Ollama](https://ollama.ai/library/mixtral) (check out this [previous blog post on what Ollama is](/blog/what-is-ollama)). The model requires 26GB of RAM while my laptop only has 16GB. I\'ll try to walk through the workaround a _bit_ at a time (pun intended).\\n\\n:::tip[Key questions I\'ll address are:]\\n\\n- What is quantization?\\n- What is offloading?\\n- How to run Mixtral-8x7B for free?\\n\\n:::\\n\\n\x3c!-- truncate --\x3e\\n\\n## What is quantization?\\nQuantization generally is a process that converts continuous values to a discrete set of values. A tangible analogy would be how we tell time: time is continuous, and we use hours, minutes, and seconds to \\"quantize\\" time. Sometimes \\"around 10am\\" is good enough, and sometimes we want to be precise to the millisecond. \\nIn the context of deep learning, it is a technique to reduce the computational and memory costs of running a model by using lower-precision numerical types to represent its weights and activations. In simpler terms, we are trying to be less precise with the numbers that makes up the model weights so that it takes up less memory and can perform operations faster. [This StackOverflow blog](https://stackoverflow.blog/2023/08/23/fitting-ai-models-in-your-pocket-with-quantization/) has great visualizations of how quantization works. Without repeating the main content, the key takeaway for me was this image: The fewer bits you use per pixel, the less memory needed, but the image quality also may decrease.\\n<Figure\\n  image=\\"https://cdn.stackoverflow.co/images/jo7n4k8s/production/5ee6f4e98bf05001b3699344f784adad0177ebe0-688x444.gif?auto=format\\"\\n  alt=\\"Representing images with varying number of bits.\\"\\n  caption=\\"Image from StackOverflow.\\"\\n/>\\n\\nWeights normally use 32-bit floating points, and are often quantized to float16 or [int8](https://github.com/TimDettmers/bitsandbytes). Ollama uses 4-bit quantization, which means instead of using 32 \\"101100...\\"s for one value, only 4 are used. That means theoretically, you get 8x savings in memory usage. Quantization can be broadly grouped into 2 main methods: \\n1. Post Training Quantization (PTQ): Done after a model is trained. A \\"calibration dataset\\" is used to capture the distribution of activations to calculate quant parameters (scale, zero point) for all inputs. No re-training is needed.\\n2. Quantization Aware Training (QAT): Models are quantized during re-training/finetuning where low precision behaviour is simulated in the forward pass (backward pass remains the same). QAT is often able to better preserve accuracy when compared to PTQ, but incurs a high cost from re-training, which may not be suitable for LLMs.\\n\\n\\nWhen it comes to quantizing LLM weights, methods like [NF4](https://arxiv.org/abs/2305.14314), [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), and [GGML/GGUF](https://github.com/rustformers/llm/blob/main/crates/ggml/README.md) are among the most popular. A key observation is that not all weights are equally important (as pointed out in the AWQ paper). Keeping higher precision for more critical layers / weights proved to be key in the balance of accuracy and resource usage. In particular, the Mixtral model from Ollama seems to be using GGML, which groups blocks of values and rounds them to a lower precision (as opposed to using a global parameter). \\n\\n[A recent paper](https://arxiv.org/pdf/2312.17238.pdf) (3 weeks old!) that focuses on running Mixture-of-Experts type models on consumer hardware seems to have the answer to my misfortunes. In addition to quantization, they use one more trick in the book - offloading.\\n\\n## What is offloading?\\nOffloading is putting some parameters in a separate, cheaper memory, such as system RAM, and only load them \\"just-in-time\\" when they are needed for computation. It proves to be very suitable for inferencing and training LLMs with limited GPU memory. In the context of using Mixtral, the MoE architecture contain multiple \u201cexperts\u201d (layers) and a \u201cgating function\u201d that selects which experts are used on a given input. That way the MoE block only uses a small portion of all \u201cexperts\u201d for any single forward pass. Each expert is offloaded separately and only brought pack to GPU when needed. \\n\\n<Figure\\n  image={bigApples}\\n  alt=\\"8 big apples barely fitting into a crate.\\"\\n  caption=\\"Image by OpenAI DALL-E 3.\\"\\n/>\\n\\n## How to run Mixtral-8x7B for free?\\nSo here we are. This is by no means to run Mixtral for production use. Here is the [colab notebook](https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb) by the authors of the paper mentioned above. Even though they were targeting the hardware specs of the Google free-tier instances, I might just be able to run Mixtral on my laptop after all.\\n\\n\\n## Further Reading\\n1. [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\\n2. [Quantize Llama models with GGUF and llama.cpp](https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172)\\n3. [4-bit Quantization with GPTQ](https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34)"},{"id":"what-is-ollama","metadata":{"permalink":"/blog/what-is-ollama","source":"@site/blog/2024-01-07-ollama/index.md","title":"What is Ollama? A shallow dive into running LLMs locally","description":"Being able to run LLMs locally and easily is truly a game changer. I have heard about Ollama before and decided to take a look at it this past weekend.","date":"2024-01-07T00:00:00.000Z","formattedDate":"January 7, 2024","tags":[{"label":"ollama","permalink":"/blog/tags/ollama"},{"label":"llama","permalink":"/blog/tags/llama"},{"label":"chat","permalink":"/blog/tags/chat"},{"label":"AI","permalink":"/blog/tags/ai"},{"label":"LLM","permalink":"/blog/tags/llm"},{"label":"ML","permalink":"/blog/tags/ml"},{"label":"chatbot","permalink":"/blog/tags/chatbot"},{"label":"local","permalink":"/blog/tags/local"}],"readingTime":2.62,"hasTruncateMarker":true,"authors":[{"name":"Isaac Chung","title":"Senior Research Engineer @ Clarifai","url":"https://isaac-chung.github.io","imageURL":"https://github.com/isaac-chung.png","key":"ichung"}],"frontMatter":{"slug":"what-is-ollama","title":"What is Ollama? A shallow dive into running LLMs locally","authors":["ichung"],"tags":["ollama","llama","chat","AI","LLM","ML","chatbot","local"],"image":"./tiny-llama.png","enableComments":true},"unlisted":false,"prevItem":{"title":"When Quantized Models Still Don\'t Fit","permalink":"/blog/quantized-models-dont-fit"}},"content":"import Figure from \'@site/src/components/figure\';\\nimport tinyLlama from \'./tiny-llama.png\';\\n\\n\\nBeing able to run LLMs locally and _easily_ is truly a game changer. I have heard about [Ollama](https://github.com/jmorganca/ollama) before and decided to take a look at it this past weekend. \\n\\n:::tip[Key questions I\'ll address are:]\\n\\n- Why is running LLMs locally becoming a hot thang\\n- What is Ollama?\\n- Should you use Ollama?\\n\\n:::\\n\\n\x3c!-- truncate --\x3e\\n\\n## Why is running LLMs locally becoming a hot thang?\\nThere was a time when LLMs were only accessible via cloud APIs from the giant providers like OpenAI and Anthropic. Don\'t get me wrong, those cloud API providers still dominate the market, and they have nice UIs that makes it easy for many users to get started. However, the price users pay (other than a pro plan or API costs) is that the providers will have full access to your chat data. For those who want run LLMs securely on their own hardware, they either had to train their own LLM (which is super costly), or wait till the release of Llama2 - an open weights model family. After that, a flood of how-to cookbooks and self-deployment launch services came onto the scene to help user deploy their own Llama2 \\"instance\\".\\n\\nAt this point, LLMs are still running on the cloud (just happens to be your own cloud), where management of the instances and GPUs could take up quite a lot of resources. It was necessary at the time because of the model sizes. The smallest Llama2 model (16bit floating point precision version of Llama2-7b-chat) comes in at 13GB. This means that most of the LLMs that are bigger that 7B (i.e. in the 10s or even 100s of GB in size) cannot fit into a regular laptop GPU. \\n\\nThen came the hail mary - quantization (to be covered in a separate blog later). By quantizing the model weights to 4-bits, the [Llama2-7b-chat model](https://ollama.ai/library/llama2:7b) now only takes up 3.8GB, which means, it\'ll finally fit!\\n\\n<Figure\\n  image={tinyLlama}\\n  alt=\\"A tiny llama alongside a regular sized llama\\"\\n  caption=\\"Image by OpenAI DALL-E 3.\\"\\n/>\\n\\n## What is Ollama?\\n[Ollama](https://ollama.ai/) is an open-source app that lets you run, create, and share large language models locally with a command-line interface on MacOS and Linux. \\nGiven the name, Ollama began by supporting Llama2, then expanded its [model library](https://ollama.ai/library) to include models like Mistral and Phi-2. Ollama makes it easy to get started with running LLMs on your own hardware in very little setup time.\\n\\n## Should you use Ollama?\\nYes, if you want to be able to run LLMs on your laptop, keep your chat data away from 3rd party services, and can interact with them via command line in a simple way. There are also many community integrations such as UIs and plugins in chat platforms. It might not be for you if you do not want to deal with setting up at all.\\n\\n\\n## How to get started with Ollama\\nIt seems super simple. \\n\\n1. On Mac, simply [download the application](https://ollama.ai/download/Ollama-darwin.zip). \\n2. Then run this to start chatting with Llama2:\\n```\\nollama run llama2\\n```\\n\\nIn addition to chatting with text prompts, Ollama also supports:\\n- [multi-modal inputs](https://github.com/jmorganca/ollama?tab=readme-ov-file#multimodal-models): e.g. asking questions about an image\\n- [passing an argument within a prompt](https://github.com/jmorganca/ollama?tab=readme-ov-file#pass-in-prompt-as-arguments): e.g. summarize a README page\\n- [serving as a REST API](https://github.com/jmorganca/ollama?tab=readme-ov-file#rest-api): e.g. chat with the model using python scripts\\n- [running as a docker image](https://ollama.ai/blog/ollama-is-now-available-as-an-official-docker-image): e.g. Deploy Ollama with Kubernetes \\n\\nThe [official Github repo README page](https://github.com/jmorganca/ollama) has more examples.\\n\\n## Some notes\\nAfter using Ollama for a weekend, I have noticed the following that may not be obvious at first glance:\\n1. By hitting the `run` command, you start a chat session. This session will live until you exit from it or when you terminate process. Interestingly, chat state was not managed at the beginning, which means that you\'d run into issues where [the model immediately forgets about the context right after a response](https://github.com/jmorganca/ollama/issues/8). Now, you can keep chatting and the model remembers what you entered as long as it fits within its context window. Once the context window is exceeded, Ollama will truncate the input from the beginning until it fits the context window again [while keeping the system instructions](https://github.com/jmorganca/ollama/pull/306). \\n2. Ollama is based on [llama.cpp](https://github.com/ggerganov/llama.cpp), an implementation of the Llama architecture in plain C/C++ without dependencies using only CPU and RAM. \\n3. Ollama is quite docker-like, and for me it feels intuitive. You pull models then run them. The [Modelfile](https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md), the \\"blueprint to create and share models with Ollama\\", is also quite dockerfile-like.\\n\\nOverall I find Ollama quite easy to use and would likely continue to use it for something quick. It would be pretty fun if [conversation history can be persisted](https://github.com/jmorganca/ollama/issues/142)!"}]}')}}]);