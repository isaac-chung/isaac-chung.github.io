"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[1936],{2265:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>l,toc:()=>u});var a=n(5893),r=n(1151);n(5124),n(9769);const i={slug:"concurrent-requests",title:"Serving Concurrent Requests with Quantized LLMs",authors:["ichung"],tags:["LLM","concurrent","requests","parallel","multi-user"],image:"./octopus.png",enableComments:!0},s=void 0,l={permalink:"/blog/concurrent-requests",source:"@site/blog/2024-04-13-concurrent-requests/index.md",title:"Serving Concurrent Requests with Quantized LLMs",description:"Being able to serve concurrent LLM generation requests are crucial to production LLM applications that have multiple users. I recently gave a talk at PyCon Lithuania on serving quantized LLMs with llama-cpp-python, an open source python library that helps serve quantized models in the GGUF format. At the end, a question came from the audience about supporting multiple users and concurrent requests. I decided to take a deeper look into why the library wasn't able to support that at the time.",date:"2024-04-13T00:00:00.000Z",formattedDate:"April 13, 2024",tags:[{label:"LLM",permalink:"/blog/tags/llm"},{label:"concurrent",permalink:"/blog/tags/concurrent"},{label:"requests",permalink:"/blog/tags/requests"},{label:"parallel",permalink:"/blog/tags/parallel"},{label:"multi-user",permalink:"/blog/tags/multi-user"}],readingTime:2.5033333333333334,hasTruncateMarker:!0,authors:[{name:"Isaac Chung",title:"Staff Machine Learning Scientist @ Zendesk QA",url:"https://isaac-chung.github.io",imageURL:"https://github.com/isaac-chung.png",key:"ichung"}],frontMatter:{slug:"concurrent-requests",title:"Serving Concurrent Requests with Quantized LLMs",authors:["ichung"],tags:["LLM","concurrent","requests","parallel","multi-user"],image:"./octopus.png",enableComments:!0},unlisted:!1,prevItem:{title:"Keeping up with the Joneses? Maintaining focus in AI",permalink:"/blog/maintain-focus"},nextItem:{title:"How to really know if your RAG system is working well.",permalink:"/blog/rag-eval-and-observability"}},o={image:n(9949).Z,authorsImageUrls:[void 0]},u=[];function c(e){const t={a:"a",admonition:"admonition",li:"li",p:"p",ul:"ul",...(0,r.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(t.p,{children:["Being able to serve concurrent LLM generation requests are crucial to production LLM applications that have multiple users. ",(0,a.jsx)(t.a,{href:"https://pycon.lt/2024/talks/DHBLXW",children:"I recently gave a talk at PyCon Lithuania"})," on serving quantized LLMs with ",(0,a.jsx)(t.a,{href:"https://github.com/abetlen/llama-cpp-python",children:"llama-cpp-python"}),", an open source python library that helps serve quantized models in the GGUF format. At the end, a question came from the audience about supporting multiple users and concurrent requests. I decided to take a deeper look into why the library wasn't able to support that at the time."]}),"\n",(0,a.jsx)(t.admonition,{title:"Key questions I'll address are:",type:"tip",children:(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"What are the challenges of serving concurrent requests with LLMs?"}),"\n",(0,a.jsx)(t.li,{children:"How to serve concurrent requests with quantized LLMs?"}),"\n"]})})]})}function h(e={}){const{wrapper:t}={...(0,r.a)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},5124:(e,t,n)=>{n.d(t,{Z:()=>s});var a=n(7294),r=n(9276),i=n(5893);function s(e){let{image:t,alt:s,caption:l}=e;const[o,u]=(0,a.useState)({width:0,height:0}),c=l.split("\\n").map(((e,t,n)=>(0,i.jsxs)(a.Fragment,{children:[e,t<n.length-1&&(0,i.jsx)("br",{})]},t)));return(0,a.useEffect)((()=>{const e=new Image;e.onload=()=>{u({width:e.naturalWidth,height:e.naturalHeight})},e.src=t;const a=new r.Z({gallery:"#figure-gallery",children:"a",pswpModule:()=>n.e(5826).then(n.bind(n,7766))});return a.init(),()=>{a.destroy()}}),[t]),(0,i.jsxs)("figure",{style:{border:"1px dashed rgba(0, 0, 0, .1)",padding:0,margin:0,marginBottom:20,borderRadius:"15px",textAlign:"right"},id:"figure-gallery",children:[(0,i.jsx)("a",{href:t,"data-pswp-width":o.width,"data-pswp-height":o.height,children:(0,i.jsx)("img",{src:t,alt:s,style:{maxWidth:"100%",height:"auto"}})}),(0,i.jsx)("hr",{style:{margin:"5px 0",backgroundColor:"rgba(0, 0, 0, .2)"}}),(0,i.jsx)("figcaption",{style:{marginTop:"0.5em",marginBottom:"0.5em",marginRight:"1em",textAlign:"right",fontSize:"0.8em"},children:c})]})}},9949:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/octopus-69454e9dcdc193dfb7a6f3ba4040312b.png"},9769:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/octopus-69454e9dcdc193dfb7a6f3ba4040312b.png"}}]);