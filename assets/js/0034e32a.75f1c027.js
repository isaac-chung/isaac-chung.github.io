"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[1936],{2265:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>o,contentTitle:()=>i,default:()=>h,frontMatter:()=>s,metadata:()=>l,toc:()=>u});var n=a(5893),r=a(1151);a(5124),a(9769);const s={slug:"concurrent-requests",title:"Serving Concurrent Requests with Quantized LLMs",authors:["ichung"],tags:["LLM","concurrent","requests","parallel","multi-user"],image:"./octopus.png",enableComments:!0},i=void 0,l={permalink:"/blog/concurrent-requests",source:"@site/blog/2024-04-13-concurrent-requests/index.md",title:"Serving Concurrent Requests with Quantized LLMs",description:"Being able to serve concurrent LLM generation requests are crucial to production LLM applications that have multiple users. I recently gave a talk at PyCon Lithuania on serving quantized LLMs with llama-cpp-python, an open source python library that helps serve quantized models in the GGUF format. At the end, a question came from the audience about supporting multiple users and concurrent requests. I decided to take a deeper look into why the library wasn't able to support that at the time.",date:"2024-04-13T00:00:00.000Z",formattedDate:"April 13, 2024",tags:[{label:"LLM",permalink:"/blog/tags/llm"},{label:"concurrent",permalink:"/blog/tags/concurrent"},{label:"requests",permalink:"/blog/tags/requests"},{label:"parallel",permalink:"/blog/tags/parallel"},{label:"multi-user",permalink:"/blog/tags/multi-user"}],readingTime:2.5033333333333334,hasTruncateMarker:!0,authors:[{name:"Isaac Chung",title:"Senior Research Engineer @ Clarifai",url:"https://isaac-chung.github.io",imageURL:"https://github.com/isaac-chung.png",key:"ichung"}],frontMatter:{slug:"concurrent-requests",title:"Serving Concurrent Requests with Quantized LLMs",authors:["ichung"],tags:["LLM","concurrent","requests","parallel","multi-user"],image:"./octopus.png",enableComments:!0},unlisted:!1,nextItem:{title:"How to really know if your RAG system is working well.",permalink:"/blog/rag-eval-and-observability"}},o={image:a(9949).Z,authorsImageUrls:[void 0]},u=[];function c(e){const t={a:"a",admonition:"admonition",li:"li",p:"p",ul:"ul",...(0,r.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(t.p,{children:["Being able to serve concurrent LLM generation requests are crucial to production LLM applications that have multiple users. ",(0,n.jsx)(t.a,{href:"https://pycon.lt/2024/talks/DHBLXW",children:"I recently gave a talk at PyCon Lithuania"})," on serving quantized LLMs with ",(0,n.jsx)(t.a,{href:"https://github.com/abetlen/llama-cpp-python",children:"llama-cpp-python"}),", an open source python library that helps serve quantized models in the GGUF format. At the end, a question came from the audience about supporting multiple users and concurrent requests. I decided to take a deeper look into why the library wasn't able to support that at the time."]}),"\n",(0,n.jsx)(t.admonition,{title:"Key questions I'll address are:",type:"tip",children:(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"What are the challenges of serving concurrent requests with LLMs?"}),"\n",(0,n.jsx)(t.li,{children:"How to serve concurrent requests with quantized LLMs?"}),"\n"]})})]})}function h(e={}){const{wrapper:t}={...(0,r.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},5124:(e,t,a)=>{a.d(t,{Z:()=>i});var n=a(7294),r=a(9276),s=a(5893);function i(e){let{image:t,alt:i,caption:l}=e;const[o,u]=(0,n.useState)({width:0,height:0}),c=l.split("\\n").map(((e,t,a)=>(0,s.jsxs)(n.Fragment,{children:[e,t<a.length-1&&(0,s.jsx)("br",{})]},t)));return(0,n.useEffect)((()=>{const e=new Image;e.onload=()=>{u({width:e.naturalWidth,height:e.naturalHeight})},e.src=t;const n=new r.Z({gallery:"#figure-gallery",children:"a",pswpModule:()=>a.e(5826).then(a.bind(a,7766))});return n.init(),()=>{n.destroy()}}),[t]),(0,s.jsxs)("figure",{style:{border:"1px dashed rgba(0, 0, 0, .1)",padding:0,margin:0,marginBottom:20,borderRadius:"15px",textAlign:"right"},id:"figure-gallery",children:[(0,s.jsx)("a",{href:t,"data-pswp-width":o.width,"data-pswp-height":o.height,children:(0,s.jsx)("img",{src:t,alt:i,style:{maxWidth:"100%",height:"auto"}})}),(0,s.jsx)("hr",{style:{margin:"5px 0",backgroundColor:"rgba(0, 0, 0, .2)"}}),(0,s.jsx)("figcaption",{style:{marginTop:"0.5em",marginBottom:"0.5em",marginRight:"1em",textAlign:"right",fontSize:"0.8em"},children:c})]})}},9949:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/octopus-69454e9dcdc193dfb7a6f3ba4040312b.png"},9769:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/octopus-69454e9dcdc193dfb7a6f3ba4040312b.png"}}]);