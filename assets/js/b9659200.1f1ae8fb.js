"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[4964],{5110:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>r,contentTitle:()=>l,default:()=>g,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var n=a(5893),i=a(1151);a(5124),a(8662);const s={slug:"mmteb",title:"MMTEB Massive Multilingual Text Embedding Benchmark",authors:["ichung"],tags:["mmteb","mteb","embedding","benchmark"],enableComments:!0},l=void 0,o={permalink:"/blog/mmteb",source:"@site/blog/2025-03-09-mmteb/index.md",title:"MMTEB Massive Multilingual Text Embedding Benchmark",description:"Embeddings power many AI applications we interact with \u2014 search engines, RAG systems \u2014 but how do we know if they\u2019re actually any good? Existing benchmarks tend to focus on a narrow set of tasks, often evaluating models in isolation without considering real-world, multilingual challenges. This can make it tough to figure out which models are truly effective, and where they might fall short. That's why we need a more comprehensive way to evaluate embeddings - one that takes into account the messy, multilingual nature of real-world language use. MMTEB is designed to fill this gap, providing a broad and diverse set of evaluation tasks that can help us better understand what works, and what doesn't, in the world of embeddings.",date:"2025-03-09T00:00:00.000Z",formattedDate:"March 9, 2025",tags:[{label:"mmteb",permalink:"/blog/tags/mmteb"},{label:"mteb",permalink:"/blog/tags/mteb"},{label:"embedding",permalink:"/blog/tags/embedding"},{label:"benchmark",permalink:"/blog/tags/benchmark"}],readingTime:4.176666666666667,hasTruncateMarker:!0,authors:[{name:"Isaac Chung",title:"Staff Machine Learning Scientist @ Zendesk QA",url:"https://isaac-chung.github.io",imageURL:"https://github.com/isaac-chung.png",key:"ichung"}],frontMatter:{slug:"mmteb",title:"MMTEB Massive Multilingual Text Embedding Benchmark",authors:["ichung"],tags:["mmteb","mteb","embedding","benchmark"],enableComments:!0},unlisted:!1,nextItem:{title:"Keeping up with the Joneses? Maintaining focus in AI",permalink:"/blog/maintain-focus"}},r={authorsImageUrls:[void 0]},d=[];function h(e){const t={admonition:"admonition",li:"li",p:"p",ul:"ul",...(0,i.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.p,{children:"Embeddings power many AI applications we interact with \u2014 search engines, RAG systems \u2014 but how do we know if they\u2019re actually any good? Existing benchmarks tend to focus on a narrow set of tasks, often evaluating models in isolation without considering real-world, multilingual challenges. This can make it tough to figure out which models are truly effective, and where they might fall short. That's why we need a more comprehensive way to evaluate embeddings - one that takes into account the messy, multilingual nature of real-world language use. MMTEB is designed to fill this gap, providing a broad and diverse set of evaluation tasks that can help us better understand what works, and what doesn't, in the world of embeddings."}),"\n",(0,n.jsx)(t.admonition,{title:"Key questions I'll address are:",type:"tip",children:(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"What is MMTEB?"}),"\n",(0,n.jsx)(t.li,{children:"What are the key takeaways from MMTEB?"}),"\n",(0,n.jsx)(t.li,{children:"How can I use MMTEB?"}),"\n"]})})]})}function g(e={}){const{wrapper:t}={...(0,i.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},5124:(e,t,a)=>{a.d(t,{Z:()=>l});var n=a(7294),i=a(9276),s=a(5893);function l(e){let{image:t,alt:l,caption:o}=e;const[r,d]=(0,n.useState)({width:0,height:0}),h=o.split("\\n").map(((e,t,a)=>(0,s.jsxs)(n.Fragment,{children:[e,t<a.length-1&&(0,s.jsx)("br",{})]},t)));return(0,n.useEffect)((()=>{const e=new Image;e.onload=()=>{d({width:e.naturalWidth,height:e.naturalHeight})},e.src=t;const n=new i.Z({gallery:"#figure-gallery",children:"a",pswpModule:()=>a.e(5826).then(a.bind(a,7766))});return n.init(),()=>{n.destroy()}}),[t]),(0,s.jsxs)("figure",{style:{border:"1px dashed rgba(0, 0, 0, .1)",padding:0,margin:0,marginBottom:20,borderRadius:"15px",textAlign:"right"},id:"figure-gallery",children:[(0,s.jsx)("a",{href:t,"data-pswp-width":r.width,"data-pswp-height":r.height,children:(0,s.jsx)("img",{src:t,alt:l,style:{maxWidth:"100%",height:"auto"}})}),(0,s.jsx)("hr",{style:{margin:"5px 0",backgroundColor:"rgba(0, 0, 0, .2)"}}),(0,s.jsx)("figcaption",{style:{marginTop:"0.5em",marginBottom:"0.5em",marginRight:"1em",textAlign:"right",fontSize:"0.8em"},children:h})]})}},8662:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/leaderboard-2057b60390daa806d0f81bd5555c88c2.png"}}]);