"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[2535],{5641:l=>{l.exports=JSON.parse('{"title":"Recent posts","items":[{"title":"Serving Concurrent Requests with Quantized LLMs","permalink":"/blog/concurrent-requests","unlisted":false},{"title":"How to really know if your RAG system is working well.","permalink":"/blog/rag-eval-and-observability","unlisted":false},{"title":"All about Timing: A quick look at metrics for LLM serving","permalink":"/blog/llm-serving","unlisted":false},{"title":"When Quantized Models Still Don\'t Fit","permalink":"/blog/quantized-models-dont-fit","unlisted":false},{"title":"What is Ollama? A shallow dive into running LLMs locally","permalink":"/blog/what-is-ollama","unlisted":false}]}')}}]);